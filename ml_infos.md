---
layout: default
title: ML Infos
permalink: /ml_infos/
---
            
<div align="center">
    <h1>Machine Learning Infos in AI4(M)S Papers</h1> 
    <h3>Update Time: 2025-10-05 15:42:47</h3>
    </div>

---

## 🌳 Machine Learning Taxonomy


<div align="center">

<img src="{{ site.baseurl }}/assets/ml_solution.png"  width="800">

</div>


### 🎯 Table 1: Tasks (What to Solve) [16 Categories → 91 Specifics]

| Category | Items |
|----------|-------|
| **Prediction Tasks** | Regression, Classification, Binary Classification, Multi-class Classification, Multi-label Classification, Ordinal Regression, Time Series Forecasting, Survival Analysis |
| **Ranking and Retrieval** | Ranking, Information Retrieval, Recommendation, Collaborative Filtering, Content-Based Filtering |
| **Clustering and Grouping** | Clustering, Community Detection, Grouping |
| **Dimensionality Reduction** | Dimensionality Reduction, Feature Selection, Feature Extraction |
| **Anomaly and Outlier** | Anomaly Detection, Outlier Detection, Novelty Detection, Fraud Detection |
| **Density and Distribution** | Density Estimation, Distribution Estimation |
| **Structured Prediction** | Structured Prediction, Sequence Labeling, Named Entity Recognition, Part-of-Speech Tagging, Sequence-to-Sequence |
| **Computer Vision Tasks** | Image Classification, Object Detection, Object Localization, Semantic Segmentation, Instance Segmentation, Panoptic Segmentation, Pose Estimation, Action Recognition, Video Classification, Optical Flow Estimation, Depth Estimation, Image Super-Resolution, Image Denoising, Image Inpainting, Style Transfer, Image-to-Image Translation, Image Generation, Video Generation |
| **Natural Language Processing Tasks** | Language Modeling, Text Classification, Sentiment Analysis, Machine Translation, Text Summarization, Question Answering, Reading Comprehension, Dialog Generation, Text Generation, Paraphrase Generation, Text-to-Speech, Speech Recognition, Speech Synthesis |
| **Graph Tasks** | Node Classification, Link Prediction, Graph Classification, Graph Generation, Graph Matching, Influence Maximization |
| **Decision Making** | Decision Making, Policy Learning, Control, Planning, Optimization, Resource Allocation |
| **Design Tasks** | Experimental Design, Hyperparameter Optimization, Architecture Search, AutoML, Neural Architecture Search |
| **Association and Pattern** | Association Rule Mining, Pattern Recognition, Motif Discovery |
| **Matching and Alignment** | Entity Matching, Entity Alignment, Record Linkage, Image Matching |
| **Generative Tasks** | Data Generation, Data Augmentation, Synthetic Data Generation |
| **Causal Tasks** | Causal Inference, Treatment Effect Estimation, Counterfactual Reasoning |

---


### 📊 Table 2: Models (What to Use) [18 Categories → 102 Specifics]

| Category | Items |
|----------|-------|
| **Linear Models** | Linear Model, Polynomial Model, Generalized Linear Model |
| **Tree-based Models** | Decision Tree, Random Forest, Gradient Boosting Tree, XGBoost, LightGBM, CatBoost |
| **Kernel-based Models** | Support Vector Machine, Gaussian Process, Radial Basis Function Network |
| **Probabilistic Models** | Naive Bayes, Bayesian Network, Hidden Markov Model, Markov Random Field, Conditional Random Field, Gaussian Mixture Model, Latent Dirichlet Allocation |
| **Basic Neural Networks** | Perceptron, Multi-Layer Perceptron, Feedforward Neural Network, Radial Basis Function Network |
| **Convolutional Neural Networks** | Convolutional Neural Network, LeNet, AlexNet, VGG, ResNet, Inception, DenseNet, MobileNet, EfficientNet, SqueezeNet, ResNeXt, SENet, NASNet, U-Net |
| **Recurrent Neural Networks** | Recurrent Neural Network, Long Short-Term Memory, Gated Recurrent Unit, Bidirectional RNN, Bidirectional LSTM |
| **Transformer Architectures** | Transformer, BERT, GPT, T5, Vision Transformer, CLIP, DALL-E, Swin Transformer |
| **Attention Mechanisms** | Attention Mechanism, Self-Attention Network, Multi-Head Attention, Cross-Attention |
| **Graph Neural Networks** | Graph Neural Network, Graph Convolutional Network, Graph Attention Network, GraphSAGE, Message Passing Neural Network, Graph Isomorphism Network, Temporal Graph Network |
| **Generative Models** | Autoencoder, Variational Autoencoder, Generative Adversarial Network, Conditional GAN, Deep Convolutional GAN, StyleGAN, CycleGAN, Diffusion Model, Denoising Diffusion Probabilistic Model, Normalizing Flow |
| **Energy-based Models** | Boltzmann Machine, Restricted Boltzmann Machine, Hopfield Network |
| **Memory Networks** | Neural Turing Machine, Memory Network, Differentiable Neural Computer |
| **Specialized Architectures** | Capsule Network, Siamese Network, Triplet Network, Attention Network, Pointer Network, WaveNet, Seq2Seq, Encoder-Decoder |
| **Object Detection Models** | YOLO, R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN, FPN, RetinaNet |
| **Time Series Models** | ARIMA Model, SARIMA Model, State Space Model, Temporal Convolutional Network, Prophet |
| **Pointer Networks** | PointNet, PointNet++ |
| **Matrix Factorization** | Matrix Factorization, Non-negative Matrix Factorization, Singular Value Decomposition |

---


### 🎓 Table 3: Learning Methods (How to Learn) [11 Categories → 82 Specifics]

| Category | Items |
|----------|-------|
| **Basic Learning Paradigms** | Supervised Learning, Unsupervised Learning, Semi-Supervised Learning, Self-Supervised Learning, Reinforcement Learning |
| **Advanced Learning Paradigms** | Transfer Learning, Multi-Task Learning, Meta-Learning, Few-Shot Learning, Zero-Shot Learning, One-Shot Learning, Active Learning, Online Learning, Incremental Learning, Continual Learning, Lifelong Learning, Curriculum Learning |
| **Training Strategies** | Batch Learning, Mini-Batch Learning, Stochastic Learning, End-to-End Learning, Adversarial Training, Contrastive Learning, Knowledge Distillation, Fine-Tuning, Pre-training, Prompt Learning, In-Context Learning |
| **Optimization Methods** | Gradient Descent, Stochastic Gradient Descent, Backpropagation, Maximum Likelihood Estimation, Maximum A Posteriori, Expectation-Maximization, Variational Inference, Evolutionary Learning |
| **Reinforcement Learning Methods** | Q-Learning, Policy Gradient, Value Iteration, Policy Iteration, Temporal Difference Learning, Monte Carlo Learning, Actor-Critic, Model-Free Learning, Model-Based Learning, Inverse Reinforcement Learning, Imitation Learning, Multi-Agent Learning |
| **Special Learning Settings** | Weakly Supervised Learning, Noisy Label Learning, Positive-Unlabeled Learning, Cost-Sensitive Learning, Imbalanced Learning, Multi-Instance Learning, Multi-View Learning, Co-Training, Self-Training, Pseudo-Labeling |
| **Domain and Distribution** | Domain Adaptation, Domain Generalization, Covariate Shift Adaptation, Out-of-Distribution Learning |
| **Collaborative Learning** | Federated Learning, Distributed Learning, Collaborative Learning, Privacy-Preserving Learning |
| **Ensemble Methods** | Ensemble Learning, Bagging, Boosting, Stacking, Blending |
| **Representation Learning** | Representation Learning, Feature Learning, Metric Learning, Distance Learning, Embedding Learning, Dictionary Learning, Manifold Learning |
| **Learning Modes** | Generative Learning, Discriminative Learning, Transductive Learning, Inductive Learning |


### 📈 Summary of Statistics


<div align="center">

<img src="{{ site.baseurl }}/assets/tasks.svg"  style="width:100%">
<img src="{{ site.baseurl }}/assets/models.svg"  style="width:100%">
<img src="{{ site.baseurl }}/assets/methods.svg"  style="width:100%">

</div>


---

## 📑 ML Infos in 351/404 Papers (Chronological Order)

### [404. A generative artificial intelligence approach for the discovery of antimicrobial peptides against multidrug-resistant bacteria](https://doi.org/10.1038/s41564-025-02114-4), Nature Microbiology *(October 03, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | UniProtKB/Swiss-Prot proteome (non-redundant canonical and isoform sequences),<br>AMP dataset (compiled from public AMP databases),<br>Non-AMP dataset (cytoplasm-filtered sequences),<br>External validation dataset (AMPs and non-AMPs),<br>Toxin and non-toxin dataset,<br>Generated sequences from external unconstrained generation models,<br>Generated non-redundant short-peptide datasets (GNRSPDs) from AMPGenix,<br>Non-redundant short-peptide datasets (NRSPDs) constructed from UniProtKB/Swiss-Prot |
| **Models** | Transformer,<br>GPT,<br>BERT,<br>Random Forest,<br>Support Vector Machine,<br>Multi-Layer Perceptron,<br>Variational Autoencoder,<br>Generative Adversarial Network |
| **Tasks** | Language Modeling,<br>Text Generation,<br>Binary Classification,<br>Synthetic Data Generation,<br>Data Generation |
| **Learning Methods** | Self-Supervised Learning,<br>Transfer Learning,<br>Fine-Tuning,<br>Pre-training,<br>Representation Learning |
| **Performance Highlights** | model_size_parameters: more than 124 million parameters,<br>pretraining_corpus_size: 609,216 protein sequences (Swiss-Prot),<br>generated_sequences_count: 7,798 (AMPGenix default T=1),<br>AUC_benchmarking_set: 0.97,<br>AUPRC_benchmarking_set: 0.96,<br>AUC_test_set: 0.99,<br>AUPRC_test_set: 0.99,<br>Precision: 90.67%,<br>F1_score: 88.89%,<br>MCC: 81.66%,<br>Specificity: 93.93%,<br>Sensitivity: 87.17%,<br>External_validation_precision: 93.99% on independent external validation dataset,<br>AUC_test_set: 0.93,<br>AUPRC_test_set: 0.92,<br>Precision: 84.79%,<br>F1_score: 85.90%,<br>MCC: 72.08%,<br>Specificity: 85.06%,<br>Sensitivity: 87.04%,<br>positive_prediction_rate_on_classifiers: AMPGenix-generated sequences consistently outperformed ProteoGPT across temperature settings when evaluated by 6 AMP classifiers (higher AMP recognition rate),<br>AMPGenix-T1_uniqueness: 0.97,<br>AMPGenix-T1_diversity: 0.98,<br>AMPGenix-T1_novelty: 0.99,<br>AMPGenix-T1_FCD: 10.21,<br>AMPGenix-T2_FCD: 9.26,<br>AMPGenix-T3_FCD: 9.57,<br>ProteoGPT_T1_FCD: 14.87,<br>Macrel_AUC: 0.91 (benchmarking set),<br>Macrel_precision: 95.95% (from Extended Data Table 1) but low sensitivity (52.28%),<br>AmPEP_AUC: not listed; Extended Data Table 1: Precision 32.60%, F1 39.23%, MCC -19.78%,<br>iAMP_Pred_AUC: 0.86 (benchmarking set),<br>iAMP_Pred_precision: 77.86% (Extended Data Table 1),<br>FCD: 13.45 (Extended Data Table 2),<br>uniqueness/diversity/novelty: reported for PepCVAE in Extended Data Table 2 (diversity and novelty ~0.99),<br>FCD: 11.54 (Extended Data Table 2),<br>diversity/novelty: reported in Extended Data Table 2 |
| **Application Domains** | Antimicrobial peptide (AMP) discovery,<br>Microbiology / infectious disease (multidrug-resistant bacteria: CRAB, MRSA),<br>Protein sequence modeling,<br>Drug discovery / therapeutic peptide design,<br>Computational biology / bioinformatics (sequence mining and generation) |

---


### [403. A comprehensive genetic catalog of human double-strand break repair](https://doi.org/10.1126/science.adr5048), Science *(October 02, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | REPAIRome (this study),<br>Toronto KnockOut CRISPR Library v3 (TKOv3),<br>CloneTrackerXP barcode library experiment (representation/depth determination),<br>AAVS1 endogenous locus cut sites (validation),<br>PCAWG tumor cohort (used for mutational signature association),<br>Processed REPAIRome data and code |
| **Models** | _None_ |
| **Tasks** | Clustering,<br>Dimensionality Reduction,<br>Feature Selection,<br>Feature Extraction,<br>Information Retrieval,<br>Pattern Recognition |
| **Learning Methods** | Unsupervised Learning,<br>Representation Learning,<br>Feature Learning |
| **Performance Highlights** | selected_genes_count: 168,<br>STRING_PPI_enrichment: < 1e-16,<br>GO_enrichment_DSB_repair_FDR: 2.1e-13 (GO:0006302),<br>GO_enrichment_DNA_repair_FDR: 1.3e-11 (GO:0006281),<br>GO_enrichment_NHEJ_FDR: 2.2e-11 (GO:0006303),<br>selection_criteria: distance > 5; FDR < 0.01; replicate PCC > 0.3,<br>UMAP_visualizations: Displayed distance/insertion-deletion/microhomology/editing-efficiency gradients across genes; highlighted NHEJ cluster around LIG4/XRCC4/POLL,<br>examples_distance_values: HLTF distance = 19.7; some genes >10,<br>cosine_similarity_match: VHL knockout effect vector best matched COSMIC indel signature ID11,<br>ID11_prevalence_in_ccRCC: > 50% prevalence (in ccRCC),<br>statistical_tests: prevalence p < 0.001 (Fisher's exact test); signature activity p < 0.001 (Wilcoxon test); VHL expression (FPKM) associated with active ID11 p < 0.001,<br>correlation_threshold_for_edges: PCC > 0.45,<br>network_genes_count: 183,<br>STRING_PPI_enrichment_of_POLQ_subnetwork: < 1.0e-16,<br>enriched_complexes_in_POLQ_subnetwork: BTRR (FDR = 6.83e-5), Fanconi anemia pathway (FDR = 7.45e-9), SAGA complex (FDR = 3.20e-8) |
| **Application Domains** | Molecular Biology,<br>Genomics,<br>DNA double-strand break (DSB) repair,<br>CRISPR-Cas gene editing,<br>Cancer Genomics,<br>Computational Biology / Bioinformatics |

---


### [401. Machine learning of charges and long-range interactions from energies and forces](https://doi.org/10.1038/s41467-025-63852-x), Nature Communications *(October 01, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Random point-charge gas (this work),<br>KF aqueous solutions (this work),<br>LODE molecular dimer dataset (charged molecular dimers) (ref. 42 / BFDB),<br>SPICE polar dipeptides subset (ref. 43),<br>4G-HDNNP benchmark datasets (from Ko et al., ref. 12),<br>Pt(111)/KF(aq) dataset (ref. 49),<br>TiO2(101)/NaCl + NaOH + HCl (aq) dataset (ref. 50),<br>LiCl(001)/GaF3(001) interface (this work / generated via on-the-fly FLARE active sampling),<br>Liquid water dataset (ref. 71) used for MD speed benchmarking |
| **Models** | Multi-Layer Perceptron,<br>Message Passing Neural Network,<br>Graph Neural Network,<br>Ensemble Learning |
| **Tasks** | Regression,<br>Regression,<br>Feature Extraction,<br>Representation Learning,<br>Anomaly Detection |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Ensemble Learning,<br>Representation Learning,<br>Backpropagation |
| **Performance Highlights** | C10H2/C10H3+_energy_RMSE_meV_per_atom: 0.73,<br>C10H2/C10H3+_force_RMSE_meV_per_A: 36.9,<br>Na8=9Cl8_energy_RMSE_meV_per_atom: 0.21,<br>Na8=9Cl8_force_RMSE_meV_per_A: 9.78,<br>Au2-MgO(001)_energy_RMSE_meV_per_atom: 0.073,<br>Au2-MgO(001)_force_RMSE_meV_per_A: 7.91,<br>Pt(111)/KF(aq)_energy_RMSE_meV_per_atom: 0.309,<br>Pt(111)/KF(aq)_force_RMSE_meV_per_A: 34.1,<br>TiO2(101)/NaCl+NaOH+HCl(aq)_energy_RMSE_meV_per_atom: 0.435,<br>TiO2(101)/NaCl+NaOH+HCl(aq)_force_RMSE_meV_per_A: 70.5,<br>LiCl/GaF3_ID_force_RMSE_meV_per_A: 78.8,<br>LiCl/GaF3_ID_force_RMSE_meV_per_A_CACE-LR: 67.8,<br>LiCl/GaF3_OOD_force_RMSE_meV_per_A_SR: 116.3,<br>LiCl/GaF3_OOD_force_RMSE_meV_per_A_LR: 40.5,<br>Random_point_charges_charge_prediction_MAE_with_10_configs_e: nearly zero (nearly exact),<br>KF_aq_energy_MAE_meV_per_atom_>=100_samples: < 0.3,<br>KF_aq_charge_learning_converged_after_~couple_hundred_samples: qualitative,<br>Dipole_R2_vs_DFT_on_polar_dipeptides: 0.991,<br>Dipole_MAE_e-angstrom_LE S: 0.089,<br>MBIS_dipole_MAE_e-angstrom: 0.063,<br>Quadrupole_R2_vs_DFT: 0.911,<br>Charges_R2_vs_M BIS: 0.87,<br>Charges_MAE_vs_MBIS_e: 0.24,<br>BEC_diagonal_R2: 0.976,<br>BEC_offdiagonal_R2: 0.838,<br>MD_SR_max_atoms_single_NVIDIA_L40S_GPU: 40000,<br>MD_LR_max_atoms_single_NVIDIA_L40S_GPU: 13000,<br>LR_overhead_vs_SR: minimal with updated implementation (comparable performance) |
| **Application Domains** | Atomistic simulations of materials,<br>Computational chemistry / molecular modeling,<br>Electrolyte / electrode interfaces (electrochemistry),<br>Ionic solutions and electric double layers,<br>Charged molecular complexes (binding curves),<br>Solid–solid interfaces and heterostructures,<br>Molecular property prediction (dipole, quadrupole, Born effective charges),<br>Machine-learned interatomic potentials (MLIPs) development |

---


### [400. Heat-rechargeable computation in DNA logic circuits and neural networks](https://doi.org/10.1038/s41586-025-09570-2), Nature *(October 01, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | MNIST (Modified National Institute of Standards and Technology) database,<br>Custom 9-bit two-memory input patterns (L and T patterns),<br>Custom 100-bit two-memory input patterns,<br>Custom Fibonacci-word input patterns (first 16 elements),<br>Synthetic test patterns used for individual circuit component evaluation (e.g., two-input WTA combinations, thresholds) |
| **Models** | Feedforward Neural Network,<br>Multi-Layer Perceptron |
| **Tasks** | Binary Classification,<br>Image Classification,<br>Classification,<br>Logic (interpreted as Boolean logic within provided list: Binary Classification / Multi-class Classification not directly but Boolean operations implemented),<br>Sequence-to-Sequence |
| **Learning Methods** | Unsupervised Learning,<br>Supervised Learning |
| **Performance Highlights** | reset_success_rate_annihilators_simulated: 93%,<br>reset_success_rate_summation_gates_simulated: 85%,<br>number_of_distinct_strands_in_system: up to 289 distinct strands; 213 present for tested patterns,<br>reusability_rounds_demonstrated: 10 rounds of sequential tests (experiments) with consistent performance; simulations and experiments closely matched,<br>time_to_reset: heating to 95°C and cooling to 20°C in 1 min (reset protocol),<br>rounds_of_computation: 16 rounds (all possible 4-bit inputs),<br>resets_demonstrated: 15 resets over 640 hours,<br>consistency: maintained consistent performance across 16 rounds,<br>kinetics_difference_before_fix: >10-fold difference in kinetics between two hairpin gates sharing same toehold but differing in long domains,<br>reset_success_rates_for_pair_designs: simulations applied 90% and 86% reset success rates for two gates to explain experiments (Extended Data Fig. 6/7),<br>correct_computation_combinations_tested: six input combinations tested with correct behaviour and preserved after reset,<br>reaction_completion_with_hairpin_downstream: approx. 60% reaction completion at high input concentration,<br>reaction_completion_with_two-stranded_downstream: restored full reaction completion,<br>signal_amplification: 10-fold signal amplification within 2 h for chosen catalyst design,<br>rounds_demonstrated: 10 rounds,<br>reusability: consistent off state over 10 rounds when unique inhibitors used,<br>sensitivity_to_inhibitor_quality: performance decay with universal inhibitor due to 5% effective concentration deviation and increasing leak |
| **Application Domains** | DNA nanotechnology / molecular programming,<br>Synthetic biology,<br>Molecular computing,<br>Molecular diagnostics (potential application),<br>Programmable molecular machines / autonomous chemical systems,<br>Origin-of-life / prebiotic chemistry (conceptual inspiration for heat stations) |

---


### [399. InterPLM: discovering interpretable features in protein language models via sparse autoencoders](https://doi.org/10.1038/s41592-025-02836-7), Nature Methods *(September 29, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | UniRef50 (5 million random protein sequences),<br>Swiss-Prot (UniprotKB reviewed subset; sampled 50,000 proteins),<br>AlphaFold Database (AFDB-F1-v4),<br>InterPro annotations (used for validation of missing annotations),<br>ESM-2 embeddings (pretrained model outputs) |
| **Models** | Transformer,<br>Autoencoder,<br>Hidden Markov Model |
| **Tasks** | Feature Extraction,<br>Clustering,<br>Binary Classification,<br>Dimensionality Reduction,<br>Language Modeling,<br>Text Generation,<br>Regression,<br>Feature Selection,<br>Clustering (feature activation patterns: structural vs sequential) |
| **Learning Methods** | Self-Supervised Learning,<br>Unsupervised Learning,<br>Pre-training,<br>Transfer Learning,<br>Representation Learning,<br>Supervised Learning |
| **Performance Highlights** | max_features_with_strong_concept_alignment_in_layer: 2,309 (ESM-2-8M layer 5),<br>features_identified_by_SAE_vs_neurons: SAEs extract 3× the concepts found in 8M ESM neurons and 7× in 650M ESM neurons (summary),<br>expansion_factors: 32× (320→10,240 features for ESM-2-8M), 8× (1,280→10,240 for ESM-2-650M),<br>SAE_feature_max_F1_range_on_ESM-2-650M: 0.95–1.0 (maximum F1 scores observed for SAE features),<br>neuron_max_F1_range: 0.6–0.7 (neurons),<br>concepts_detected_ESM-2-650M_vs_ESM-2-8M: 427 vs 143 concepts (≈1.7× more concepts in 650M subset),<br>example_feature_specificity_f1503_F1: 0.998,<br>other_TBDR_cluster_feature_F1s: 0.793, 0.611,<br>glycine_feature_F1s: 0.995, 0.990, 0.86 (highly glycine-specific features),<br>steering_effects: Steering periodic glycine features increased predicted probability of glycine at both steered and masked positions; effect propagated to multiple subsequent periodic repeats with diminishing intensity (quantitative probability changes shown in Fig. 6; steer amounts up to 2.5× maximum activation),<br>median_Pearson_r_for_LLM_generated_descriptions: 0.72 (median across 1,240 features),<br>example_feature_correlations: 0.83, 0.73, 0.99 (example features shown in Fig. 4),<br>example_confirmation: Independent confirmation of Nudix motif in B2GFH1 via HMM-based InterPro annotation (qualitative validation) |
| **Application Domains** | protein modeling,<br>protein engineering,<br>computational biology,<br>bioinformatics (protein annotation),<br>model interpretability / mechanistic interpretability,<br>biological discovery (novel motif/domain identification) |

---


### [398. SimpleFold: Folding Proteins is Simpler than You Think](https://doi.org/10.48550/arXiv.2509.18480), Preprint *(September 27, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Protein Data Bank (PDB),<br>AFDB SwissProt subset (from AlphaFold Protein Structure Database),<br>AFESM (representative clusters),<br>AFESM-E (extended AFESM),<br>CAMEO22 benchmark,<br>CASP14 benchmark (subset),<br>ATLAS (MD ensemble dataset) |
| **Models** | Transformer,<br>Attention Mechanism,<br>Multi-Head Attention,<br>Pretrained Transformer (ESM2-3B embeddings) |
| **Tasks** | Regression,<br>Sequence-to-Sequence,<br>Synthetic Data Generation,<br>Distribution Estimation,<br>Multi-class Classification |
| **Learning Methods** | Generative Learning,<br>Pre-training,<br>Fine-Tuning,<br>Transfer Learning,<br>Supervised Learning,<br>Representation Learning |
| **Performance Highlights** | CAMEO22_TM-score_mean/median: 0.837 / 0.916,<br>CAMEO22_GDT-TS_mean/median: 0.802 / 0.867,<br>CAMEO22_LDDT_mean/median: 0.773 / 0.802,<br>CAMEO22_LDDT-Cα_mean/median: 0.852 / 0.884,<br>CAMEO22_RMSD_mean/median: 4.225 / 2.175,<br>CASP14_TM-score_mean/median: 0.720 / 0.792,<br>CASP14_GDT-TS_mean/median: 0.639 / 0.703,<br>CASP14_LDDT_mean/median: 0.666 / 0.709,<br>CASP14_LDDT-Cα_mean/median: 0.747 / 0.829,<br>CASP14_RMSD_mean/median: 7.732 / 3.923,<br>SimpleFold-100M_CAMEO22_TM-score_mean/median: 0.803 / 0.878,<br>SimpleFold-360M_CAMEO22_TM-score_mean/median: 0.826 / 0.905,<br>SimpleFold-700M_CAMEO22_TM-score_mean/median: 0.829 / 0.915,<br>SimpleFold-1.1B_CAMEO22_TM-score_mean/median: 0.833 / 0.924,<br>SimpleFold-1.6B_CAMEO22_TM-score_mean/median: 0.835 / 0.916,<br>SimpleFold-100M_CASP14_TM-score_mean/median: 0.611 / 0.628,<br>SimpleFold-360M_CASP14_TM-score_mean/median: 0.674 / 0.758,<br>SimpleFold-700M_CASP14_TM-score_mean/median: 0.680 / 0.767,<br>SimpleFold-1.1B_CASP14_TM-score_mean/median: 0.697 / 0.796,<br>SimpleFold-1.6B_CASP14_TM-score_mean/median: 0.712 / 0.801,<br>Pairwise_RMSD_r_no_tuning: 0.44,<br>Global_RMSF_r_no_tuning: 0.45,<br>Per_target_RMSF_r_no_tuning: 0.60,<br>RMWD_no_tuning: 4.22,<br>MD_PCA_W2_no_tuning: 1.62,<br>Joint_PCA_W2_no_tuning: 2.59,<br>%PC_sim_>0.5_no_tuning: 28,<br>Weak_contacts_J_no_tuning: 0.36,<br>Transient_contacts_J_no_tuning: 0.27,<br>Exposed_residue_J_no_tuning: 0.39,<br>Exposed_MI_matrix_rho_no_tuning: 0.24,<br>Pairwise_RMSD_r_tuned_SF-MD-3B: 0.45,<br>Global_RMSF_r_tuned_SF-MD-3B: 0.48,<br>Per_target_RMSF_r_tuned_SF-MD-3B: 0.67,<br>RMWD_tuned_SF-MD-3B: 4.17,<br>MD_PCA_W2_tuned_SF-MD-3B: 1.34,<br>Joint_PCA_W2_tuned_SF-MD-3B: 2.18,<br>%PC_sim_>0.5_tuned_SF-MD-3B: 38,<br>Weak_contacts_J_tuned_SF-MD-3B: 0.56,<br>Transient_contacts_J_tuned_SF-MD-3B: 0.34,<br>Exposed_residue_J_tuned_SF-MD-3B: 0.60,<br>Exposed_MI_matrix_rho_tuned_SF-MD-3B: 0.32,<br>SimpleFold-3B_Apo/holo_res_flex_global: 0.639,<br>SimpleFold-3B_Apo/holo_res_flex_per-target_mean/median: 0.550 / 0.552,<br>SimpleFold-3B_Apo/holo_TM-ens_mean/median: 0.893 / 0.916,<br>SimpleFold-3B_Fold-switch_res_flex_global: 0.292,<br>SimpleFold-3B_Fold-switch_res_flex_per-target_mean/median: 0.288 / 0.263,<br>SimpleFold-3B_Fold-switch_TM-ens_mean/median: 0.734 / 0.766,<br>pLDDT_vs_LDDT-Cα_Pearson_correlation: 0.77,<br>SimpleFold-3B_inference_time_200steps_seq256_s: 15.6,<br>SimpleFold-3B_inference_time_200steps_seq512_s: 27.8,<br>SimpleFold-3B_inference_time_500steps_seq256_s: 37.2,<br>SimpleFold-100M_inference_time_200steps_seq256_s: 3.8 |
| **Application Domains** | Protein structure prediction / computational structural biology,<br>Molecular dynamics ensemble generation / protein flexibility modeling,<br>De novo protein design / protein generation,<br>Drug discovery (ensemble observables and cryptic pocket identification),<br>Generative modeling for scientific domains (analogy to text-to-image / text-to-3D) |

---


### [397. Design of a potent interleukin-21 mimic for cancer immunotherapy](https://doi.org/10.1126/sciimmunol.adx1582), Science Immunology *(September 26, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | PDB structures (hIL-21/hIL-21R: PDB 3TGX; native hIL-21 complex PDB 8ENT; hγc complex PDB 7S2R; 21h10 complex PDB 9E2T),<br>Computational design candidate set (Rosetta-generated designs),<br>MC38 syngeneic murine tumor model (MC38 adenocarcinoma),<br>B16F10 murine melanoma model (with adoptive TRP1high/low T cell transfer),<br>LCMV-infected mice (virus-specific CD8 T cell analysis),<br>PDOTS (patient-derived organotypic tumor spheroids) from advanced melanoma patients,<br>Bulk RNA-seq (murine CD8 T cells treated in vitro),<br>Single-cell RNA-seq (scRNA-seq) of tumor-infiltrating CD45+ cells,<br>Crystallography diffraction data (21h10/hIL-21R/hγc complex) |
| **Models** | Message Passing Neural Network,<br>Graph Neural Network,<br>Other (non-ML computational tools) |
| **Tasks** | Synthetic Data Generation,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Extraction,<br>Data Generation,<br>Image Classification |
| **Learning Methods** | Supervised Learning,<br>Generative Learning,<br>Unsupervised Learning |
| **Performance Highlights** | functional_outcome: Generated variant 21AT36 binds IL-21R but not γc; antagonist did not induce STAT phosphorylation,<br>context_readouts: 21AT36 did not induce STAT phosphorylation in murine CD8 T cells; in MC38 tumors an equimolar dose of 21AT36 did not show antitumor activity (Fig. 2F and 2G),<br>design_pool_size: 185 Rosetta designs generated and filtered; downstream selection and mutagenesis led to 21h10,<br>structural_validation: Crystal structure of 21h10 complex solved (PDB ID: 9E2T) with resolution between 2.3 and 3.4 Å,<br>single_cell_input: ≈4000 cells per tumor; 10 mice (PBS, 21h10) or 5 mice (Neo-2/15, mIL-21) per group pooled; 50,000 read pairs per cell sequencing depth,<br>biological_findings: Identification of multiple immune and nonimmune clusters; 21h10 expanded highly activated CD8 T cells and TRP1low tumor-specific T cells and decreased Treg frequency,<br>design_to-function: 21h10 showed STAT1/STAT3 phosphorylation potency equivalent to native hIL-21 and mIL-21 in both human and murine cells; 21h10 elicits similar gene-expression profile at 100 pM compared with 1 nM mIL-21,<br>thermal_stability: 21h10 melting temperature (Tm) ≈ 75°C |
| **Application Domains** | cancer immunotherapy,<br>computational protein design / de novo protein design,<br>structural biology (X-ray crystallography),<br>immunology (T cell biology, cytokine signaling),<br>single-cell transcriptomics / tumor microenvironment profiling,<br>ex vivo functional profiling of human tumors (PDOTS) |

---


### [396. EpiAgent: foundation model for single-cell epigenomics](https://doi.org/10.1038/s41592-025-02822-z), Nature Methods *(September 25, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Human-scATAC-Corpus,<br>Buenrostro2018,<br>Kanemaru2023,<br>Li2023b,<br>Ameen2022,<br>Li2023a,<br>Lee2023,<br>Zhang2021,<br>Pierce2021,<br>Liscovitch-Brauer2021,<br>Long et al. (ccRCC single-cell multi-omics),<br>10x Genomics single-cell multi-omics human brain dataset |
| **Models** | Transformer,<br>Attention Mechanism,<br>Multi-Head Attention,<br>Graph Neural Network,<br>Multi-Layer Perceptron,<br>Feedforward Neural Network,<br>Convolutional Neural Network,<br>Support Vector Machine,<br>Transformer,<br>Graph Neural Network |
| **Tasks** | Feature Extraction,<br>Dimensionality Reduction,<br>Multi-class Classification,<br>Data Generation,<br>Treatment Effect Estimation,<br>Counterfactual Reasoning,<br>Domain Adaptation,<br>Zero-Shot Learning,<br>Clustering |
| **Learning Methods** | Pre-training,<br>Fine-Tuning,<br>Self-Supervised Learning,<br>Supervised Learning,<br>Transfer Learning,<br>Zero-Shot Learning,<br>Domain Adaptation,<br>Representation Learning,<br>Graph Neural Network |
| **Performance Highlights** | NMI: higher than all six baseline methods after fine-tuning (exact numeric values reported in Supplementary materials),<br>ARI: higher than all six baseline methods after fine-tuning (exact numeric values reported in Supplementary materials),<br>accuracy_improvement_vs_second_best: 11.036% (average),<br>macro_F1_improvement_vs_second_best: 21.549% (average),<br>NMI_improvement_over_raw: 11.123% (average),<br>ARI_improvement_over_raw: 18.605% (average),<br>Pearson_correlation_median: >0.8 (between imputed signals and average raw signals of corresponding cell types),<br>R2_top_1000_DA_cCREs: >0.7,<br>direction_accuracy_top_100_DA_cCREs: >90% (average across cell types),<br>Wasserstein_distance: lower than baselines (better alignment of predicted and real perturbed cell distributions),<br>Pearson_correlation_vs_GEARS_on_Pierce2021: EpiAgent Pearson correlation average 24.177% higher than GEARS,<br>direction_accuracy_top_DA_cCREs: EpiAgent significantly higher than GEARS (GEARS near random),<br>NMI/ARI/kBET/iLISI: EpiAgent achieves best overall performance vs baselines on clustering and batch-correction metrics (exact numeric values in Fig.4e and Supplementary Figs.),<br>embedding_quality: Zero-shot EpiAgent competitive with baselines on datasets with similar cell populations to pretraining corpus (notably Li2023b),<br>clustering_and_metrics: visual and metric superiority reported (UMAP separation; NMI/ARI comparisons in Fig.2b,c),<br>EpiAgent-B_accuracy: >0.88,<br>EpiAgent-NT_accuracy: >0.95,<br>balanced_accuracy_and_macro_F1: >0.8 (for per-cell-type performance),<br>Wasserstein_distance_change_significance: one-sided t-tests yield P values < 0.05 for majority of perturbations (Liscovitch-Brauer2021 dataset),<br>directional_shift_in_synthetic_ccRCC_experiment: knockouts (EGLN3, ABCC1, VEGFA, Group) shift synthetic cells away from cancer-proportion profiles (quantified by average change in proportion of cancer cell-derived cCREs; exact numeric values in Fig.5g) |
| **Application Domains** | single-cell epigenomics (scATAC-seq),<br>cell type annotation and atlas construction (human brain, normal tissues),<br>hematopoiesis and developmental trajectory analysis,<br>cardiac niches and heart tissues,<br>stem cell differentiation,<br>cancer epigenetics (clear cell renal cell carcinoma),<br>perturbation response prediction (drug stimulation, CRISPR knockouts),<br>batch correction and multi-dataset integration |

---


### [395. Activation entropy of dislocation glide in body-centered cubic metals from atomistic simulations](https://doi.org/10.1038/s41467-025-62390-w), Nature Communications *(September 24, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Fe and W MLIP training datasets (extended from refs. 23, 26),<br>PAFI sampling datasets (finite-temperature sampled configurations along reaction coordinates),<br>Empirical potential (EAM) reference calculations,<br>Experimental yield stress datasets (from literature) |
| **Models** | Machine-Learning Interatomic Potentials (MLIP),<br>Embedded Atom Method (EAM) potentials |
| **Tasks** | Regression,<br>Data Generation,<br>Image Classification |
| **Learning Methods** | Supervised Learning,<br>Transfer Learning |
| **Performance Highlights** | activation_entropy_harmonic_regime_Fe: ΔS(z2) = 6.3 kB,<br>activation_entropy_difference_above_T0_Fe: ΔS(z2)-ΔS(z1) = 1.6 kB,<br>activation_entropy_harmonic_regime_W: approx. 8 kB,<br>MD_velocity_prefactor_fit_HTST: ν = 3.8×10^9 Hz (HTST fit),<br>MD_velocity_prefactor_fit_VHTST: ν = 9.2×10^10 Hz (variational HTST fit),<br>simulation_cell_size: 96,000 atoms (per atomistic simulation cell),<br>PAFI_computational_cost_per_condition: 5×10^4 to 2.5×10^5 CPU hours (for anharmonic Gibbs energy calculations),<br>Hessian_diagonalization_cost: ≈5×10^4 CPU-hours per atomic system using MLIP,<br>effective_entropy_variation_range_Fe_EAM: ΔSeff varies by ~10 kB between 0 and 700 MPa (Fe, EAM),<br>departure_from_harmonicity_temperature: marked departure from harmonic prediction above ~20 K (Fe, EAM),<br>inverse_Meyer_Neldel_TMNs: Fe: TMN = -406 K (effective fit), W: TMN = -1078 K (effective fit) |
| **Application Domains** | materials science,<br>computational materials / atomistic simulation,<br>solid mechanics / metallurgy (dislocation glide & yield stress in BCC metals),<br>physics of defects (dislocations, kink-pair nucleation) |

---


### [394. Design of facilitated dissociation enables timing of cytokine signalling](https://doi.org/10.1038/s41586-025-09549-z), Nature *(September 24, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | PDB structures (design models and solved crystal structures; accession codes 9DCX, 9DCY, 9DCZ, 9DD0, 9DD1, 9DD2, 9DD3, 9DD4, 9DD5, 9OLQ),<br>Designs and analysis code, sequences and source data (Zenodo deposit),<br>Single-molecule tracking (SMT) raw data (Zenodo DOIs: multiple entries),<br>RNA-seq raw data (BioProject PRJNA1302552),<br>SKEMPI database (referenced),<br>Reference genome and gene sets (GRCh38, MSigDB Hallmark gene sets) |
| **Models** | Diffusion Model |
| **Tasks** | Clustering,<br>Dimensionality Reduction,<br>Synthetic Data Generation |
| **Learning Methods** | Unsupervised Learning |
| **Performance Highlights** | AF2_predicted_RMSD_to_crystal_Cα: <= 1.0 Å,<br>designs_tested_initial_pipeline: 24 designs tested; multiple working designs obtained on first attempt,<br>MD_simulation_length_per_trajectory: 1 μs (triplicate trajectories),<br>agreement_with_DEER_distance_distributions: MD-simulated distance distributions span experimental DEER distribution (qualitative agreement) |
| **Application Domains** | De novo protein design / computational protein engineering,<br>Structural biology (X-ray crystallography, DEER spectroscopy),<br>Biophysics (kinetic design and measurement of protein–protein interactions),<br>Synthetic biology / biosensor design (rapid luciferase sensors),<br>Immunology / cytokine signalling (design and temporal control of IL-2 mimics),<br>Single-molecule microscopy (live-cell receptor dimerization dynamics),<br>Molecular simulation (MD, integrative modelling) |

---


### [393. Tailoring polymer electrolyte solvation for 600 Wh kg−1 lithium batteries](https://doi.org/10.1038/s41586-025-09565-z), Nature *(September 24, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Electrochemical cycling data (coin cells),<br>Electrochemical cycling data (anode-free pouch cells),<br>Materials characterization datasets (NMR, Raman, XPS, TOF-SIMS, TEM, SEM, DSC, GITT, EIS, DEMS),<br>DFT calculation data |
| **Models** | _None_ |
| **Tasks** | Experimental Design,<br>Optimization,<br>Data Generation |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | Battery materials,<br>Solid-state batteries,<br>Lithium metal batteries (Li-rich Mn-based layered oxide cathodes),<br>Electrochemistry,<br>Energy storage,<br>Materials science / polymer electrolytes,<br>Computational chemistry (DFT) |

---


### [392. EDBench: Large-Scale Electron Density Data for Molecular Modeling](https://doi.org/10.48550/arXiv.2505.09262), Preprint *(September 24, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | EDBench,<br>ED5-EC,<br>ED5-OE,<br>ED5-MM,<br>ED5-OCS,<br>ED5-MER,<br>ED5-EDP,<br>PCQM4Mv2,<br>Referenced QC datasets (QM7, QM9, QM7-X, PubChemQC, MD17, MD22, WS22, QH9, MultiXC-QM9, MP, ECD, QMugs, ∇2DFT, QM9-VASP, Materials Project) |
| **Models** | Transformer,<br>Multi-Layer Perceptron,<br>Graph Neural Network |
| **Tasks** | Regression,<br>Binary Classification,<br>Information Retrieval,<br>Density Estimation,<br>Data Generation |
| **Learning Methods** | Supervised Learning,<br>Contrastive Learning,<br>Pre-training,<br>Fine-Tuning,<br>Representation Learning,<br>End-to-End Learning |
| **Performance Highlights** | E1_MAE: 243.49 ± 74.72,<br>E2_MAE: 325.65 ± 160.17,<br>E3_MAE: 858.77 ± 496.74,<br>E4_MAE: 389.24 ± 217.51,<br>E5_MAE: 17.54 ± 10.85,<br>E6_MAE: 243.49 ± 74.73,<br>E1_MAE: 190.77 ± 1.98,<br>E2_MAE: 109.21 ± 2.82,<br>E3_MAE: 369.88 ± 1.34,<br>E4_MAE: 150.05 ± 0.27,<br>E5_MAE: 8.13 ± 0.51,<br>E6_MAE: 190.77 ± 1.98,<br>HOMO-2_MAE_x100: 1.73 ± 0.01,<br>HOMO-1_MAE_x100: 1.68 ± 0.01,<br>HOMO-0_MAE_x100: 1.92 ± 0.01,<br>LUMO+0_MAE_x100: 3.08 ± 0.05,<br>LUMO+1_MAE_x100: 2.86 ± 0.05,<br>LUMO+2_MAE_x100: 3.05 ± 0.02,<br>LUMO+3_MAE_x100: 3.01 ± 0.02,<br>HOMO-2_MAE_x100: 1.75 ± 0.02,<br>HOMO-1_MAE_x100: 1.72 ± 0.02,<br>HOMO-0_MAE_x100: 1.98 ± 0.00,<br>LUMO+0_MAE_x100: 3.21 ± 0.01,<br>LUMO+1_MAE_x100: 3.02 ± 0.02,<br>LUMO+2_MAE_x100: 3.25 ± 0.04,<br>LUMO+3_MAE_x100: 3.20 ± 0.03,<br>Dipole_X_MAE: 0.9123 ± 0.0203,<br>Dipole_Y_MAE: 0.9605 ± 0.0053,<br>Dipole_Z_MAE: 0.7540 ± 0.0068,<br>Magnitude_MAE: 0.7397 ± 0.0467,<br>Dipole_X_MAE: 0.8818 ± 0.0010,<br>Dipole_Y_MAE: 0.9427 ± 0.0008,<br>Dipole_Z_MAE: 0.7416 ± 0.0023,<br>Magnitude_MAE: 0.6820 ± 0.0005,<br>Accuracy: 55.57 ± 2.14,<br>ROC-AUC: 55.97 ± 5.17,<br>AUPR: 57.62 ± 3.91,<br>F1-Score: 66.96 ± 2.08,<br>Accuracy: 57.65 ± 0.18,<br>ROC-AUC: 60.48 ± 0.38,<br>AUPR: 61.54 ± 0.31,<br>F1-Score: 61.41 ± 1.02,<br>GeoFormer + PointVector_ED→MS_Top-1: 17.67 ± 2.10,<br>GeoFormer + PointVector_ED→MS_Top-3: 46.09 ± 4.53,<br>GeoFormer + PointVector_ED→MS_Top-5: 67.63 ± 5.92,<br>GeoFormer + PointVector_MS→ED_Top-1: 27.01 ± 1.69,<br>GeoFormer + PointVector_MS→ED_Top-3: 59.02 ± 2.49,<br>GeoFormer + PointVector_MS→ED_Top-5: 77.42 ± 3.01,<br>GeoFormer + X-3D_ED→MS_Top-1: 68.32 ± 3.70,<br>GeoFormer + X-3D_ED→MS_Top-3: 92.18 ± 2.41,<br>GeoFormer + X-3D_ED→MS_Top-5: 97.31 ± 1.29,<br>GeoFormer + X-3D_MS→ED_Top-1: 70.01 ± 2.93,<br>GeoFormer + X-3D_MS→ED_Top-3: 92.08 ± 2.01,<br>GeoFormer + X-3D_MS→ED_Top-5: 97.17 ± 0.92,<br>EquiformerV2 + PointVector_ED→MS_Top-1: 10.24 ± 1.28,<br>EquiformerV2 + PointVector_ED→MS_Top-3: 32.47 ± 2.69,<br>EquiformerV2 + PointVector_ED→MS_Top-5: 53.42 ± 2.67,<br>EquiformerV2 + PointVector_MS→ED_Top-1: 22.18 ± 0.64,<br>EquiformerV2 + PointVector_MS→ED_Top-3: 54.61 ± 2.89,<br>EquiformerV2 + PointVector_MS→ED_Top-5: 76.83 ± 2.90,<br>EquiformerV2 + X-3D_ED→MS_Top-1: 78.71 ± 0.69,<br>EquiformerV2 + X-3D_ED→MS_Top-3: 94.78 ± 0.40,<br>EquiformerV2 + X-3D_ED→MS_Top-5: 98.13 ± 0.07,<br>EquiformerV2 + X-3D_MS→ED_Top-1: 78.36 ± 0.65,<br>EquiformerV2 + X-3D_MS→ED_Top-3: 94.19 ± 0.14,<br>EquiformerV2 + X-3D_MS→ED_Top-5: 97.74 ± 0.29,<br>ρτ=0.1_MAE: 0.3362 ± 0.2900,<br>ρτ=0.1_Pearson(%): 81.0 ± 8.1,<br>ρτ=0.1_Spearman(%): 56.4 ± 13.7,<br>ρτ=0.1_Time_sec_per_mol: 0.024,<br>ρτ=0.15_MAE: 0.0463 ± 0.0157,<br>ρτ=0.15_Pearson(%): 98.0 ± 6.3,<br>ρτ=0.15_Spearman(%): 87.0 ± 2.7,<br>ρτ=0.15_Time_sec_per_mol: 0.015,<br>ρτ=0.2_MAE: 0.0448 ± 0.0133,<br>ρτ=0.2_Pearson(%): 99.2 ± 0.8,<br>ρτ=0.2_Spearman(%): 91.0 ± 9.1,<br>ρτ=0.2_Time_sec_per_mol: 0.013,<br>DFT_Time_sec_per_mol_for_comparison: 245.8,<br>ED5-EDP_MAE: 0.018 ± 0.003,<br>ED5-EDP_Pearson: 0.993 ± 0.004,<br>ED5-EDP_Spearman: 0.381 ± 0.162,<br>EDMaterial-EDP_MAE: 0.118 ± 0.029,<br>EDMaterial-EDP_Pearson: 0.918 ± 0.034,<br>EDMaterial-EDP_Spearman: 0.633 ± 0.115,<br>X-3D_original_HOMO-2_MAE_x100: 1.75 ± 0.02,<br>X-3D (full)_HOMO-2_MAE_x100: 1.5797,<br>X-3D_original_HOMO-1_MAE_x100: 1.72 ± 0.02,<br>X-3D (full)_HOMO-1_MAE_x100: 1.6359,<br>X-3D_original_HOMO-0_MAE_x100: 1.98 ± 0.00,<br>X-3D (full)_HOMO-0_MAE_x100: 1.9104,<br>X-3D_original_LUMO+0_MAE_x100: 3.21 ± 0.01,<br>X-3D (full)_LUMO+0_MAE_x100: 2.9981,<br>X-3D_original_LUMO+1_MAE_x100: 3.02 ± 0.02,<br>X-3D (full)_LUMO+1_MAE_x100: 2.7028,<br>X-3D_original_LUMO+2_MAE_x100: 3.25 ± 0.04,<br>X-3D (full)_LUMO+2_MAE_x100: 2.8725,<br>X-3D_original_LUMO+3_MAE_x100: 3.20 ± 0.03,<br>X-3D (full)_LUMO+3_MAE_x100: 2.8708,<br>DFT_E1_MAE: 224.13 ± 43.47,<br>DFT_E2_MAE: 155.85 ± 28.75,<br>DFT_E3_MAE: 451.59 ± 58.53,<br>DFT_E4_MAE: 190.47 ± 25.62,<br>DFT_E5_MAE: 9.57 ± 1.56,<br>DFT_E6_MAE: 224.13 ± 43.47,<br>DFT_Mean: 209.29,<br>HGEGNN(2024)_Mean: 186.38,<br>HGEGNN(2025)_Mean: 196.11,<br>HGEGNN(2026)_Mean: 182.75,<br>ED5-OE_xi=2048_mean_MAE_x100: 2.48,<br>ED5-OE_xi=512_mean_MAE_x100: 2.56,<br>ED5-OE_xi=1024_mean_MAE_x100: 2.75,<br>ED5-OE_xi=4096_mean_MAE_x100: 2.70,<br>ED5-OE_xi=8192_mean_MAE_x100: 2.60 |
| **Application Domains** | Molecular modeling,<br>Quantum chemistry,<br>Machine-learning force fields (MLFFs),<br>Drug discovery / virtual screening,<br>Materials science (periodic systems / crystalline solids),<br>High-throughput quantum-aware modeling |

---


### [391. Active Learning for Machine Learning Driven Molecular Dynamics](https://doi.org/10.48550/arXiv.2509.17208), Preprint *(September 21, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Chignolin protein (in-house benchmark suite) |
| **Models** | Graph Neural Network |
| **Tasks** | Regression,<br>Data Generation,<br>Dimensionality Reduction,<br>Distribution Estimation |
| **Learning Methods** | Active Learning,<br>Supervised Learning |
| **Performance Highlights** | TICA_W1_before: 1.15023,<br>TICA_W1_after: 0.77003,<br>TICA_W1_percent_change: -33.05%,<br>Bond_length_W1_before: 0.00043,<br>Bond_length_W1_after: 0.00022,<br>Bond_length_W1_percent_change: -48.84%,<br>Bond_angle_W1_before: 0.11036,<br>Bond_angle_W1_after: 0.10148,<br>Bond_angle_W1_percent_change: -8.05%,<br>Dihedral_W1_before: 0.25472,<br>Dihedral_W1_after: 0.36378,<br>Reaction_coordinate_W1_before: 0.15141,<br>Reaction_coordinate_W1_after: 0.38302,<br>loss_function: mean-squared error (MSE) between predicted CG forces and projected AA forces (force matching),<br>W1_TICA_after_active_learning: 0.77003 |
| **Application Domains** | Molecular Dynamics,<br>Protein conformational modeling,<br>Coarse-grained simulations for biomolecules,<br>ML-driven drug discovery / computational biophysics |

---


### [390. 3D multi-omic mapping of whole nondiseased human fallopian tubes at cellular resolution reveals a large incidence of ovarian cancer precursors](https://doi.org/10.1101/2025.09.21.677628), Preprint *(September 21, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | nPOD donor fallopian tube cohort (this paper),<br>Visium Cytassist spatial transcriptomics data (ROIs from donor tubes),<br>CODEX multiplexed imaging dataset (25-marker panel),<br>SRS-HSI spatial metabolomics ROIs,<br>Derived/processed imaging dataset (H&E + IHC stacks) |
| **Models** | Convolutional Neural Network,<br>Convolutional Neural Network,<br>Clustering (unsupervised) |
| **Tasks** | Structured Prediction,<br>Object Detection,<br>Clustering,<br>Dimensionality Reduction,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Transfer Learning,<br>Fine-Tuning,<br>Unsupervised Learning,<br>Dimensionality Reduction |
| **Performance Highlights** | tissue_segmentation_accuracy: 95.2%,<br>epithelial_subtyping_accuracy: 93.2%,<br>nuclei_segmentations_extracted: 2.19 billion,<br>images_processed_for_nuclei: 2,452 H&E-stained images,<br>auto_highlighted_p53_Ki67_locations: 1,285 (mean 257, median 211 per fallopian tube),<br>pathologist_validated_STICs: 99 STICs identified (13 proliferatively active STICs, 86 proliferative dormant STICs) and 11 p53 signatures across 5 donors,<br>SRS_HSI_PCA_kmeans_cluster_finding: distinct metabolite clusters separating lesion vs control ROIs (qualitative),<br>CODEX_clusters: 30 unsupervised clusters combined into 19 annotated cell phenotypes,<br>single_cell_count: 972,276 cells segmented for CODEX WSI,<br>PAGA_connectivity_insights: identified interactions linking TAMs, regulatory DCs, activated T cells and CD8+ memory T cells; STIC cells associated with proliferating epithelial cells (qualitative topology results) |
| **Application Domains** | Histopathology / Digital Pathology,<br>Oncology (ovarian cancer precursor detection and characterization),<br>Spatial multi-omics integration (spatial proteomics, spatial transcriptomics, spatial metabolomics),<br>Medical image analysis (3D reconstruction and registration) |

---


### [389. De novo Design of All-atom Biomolecular Interactions with RFdiffusion3](https://doi.org/10.1101/2025.09.18.676967), Preprint *(September 18, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Protein Data Bank (PDB) - all complexes deposited through December 2024,<br>AlphaFold2 (AF2) distillation structures (Hsu et al.),<br>Atomic Motif Enzyme (AME) benchmark,<br>Binder design benchmark targets (PD-L1, InsulinR, IL-7Ra, Tie2, IL-2Ra),<br>DNA binder evaluation targets (PDB IDs: 7RTE, 7N5U, 7M5W),<br>Small-molecule binding benchmark (four molecules: FAD, SAM, IAI, OQO),<br>Experimental enzyme design screening set (esterase / cysteine hydrolase designs),<br>Experimental DNA-binding designs |
| **Models** | Denoising Diffusion Probabilistic Model,<br>Transformer,<br>U-Net,<br>Attention Mechanism,<br>Cross-Attention |
| **Tasks** | Synthetic Data Generation,<br>Data Generation,<br>Clustering,<br>Sequence-to-Sequence |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Fine-Tuning |
| **Performance Highlights** | inference_speedup_vs_RFD2: approximately 10x,<br>parameters: 168M trainable parameters (RFD3) vs ~350M for AF3,<br>unconditional_refolding_rate: 98% of designs have at least one sequence predicted by AF3 to fold within 1.5 Å RMSD (out of 8 ProteinMPNN sequences),<br>diversity_example: 41 clusters out of 96 generations between length 100-250 (TM-score cutoff 0.5),<br>binder_unique_successful_clusters_RFD3_avg: 8.2 (average unique successful clusters per target, TM-score clustering threshold 0.6),<br>binder_unique_successful_clusters_RFD1_avg: 1.4 (comparison),<br>DNA_monomer_pass_rate_<5Å_DNA-aligned_RMSD: 8.67%,<br>DNA_dimer_pass_rate_<5Å_DNA-aligned_RMSD: 6.67%,<br>DNA_monomer_pass_rate_interface_fixed_after_LigandMPNN: 6.5%,<br>DNA_dimer_pass_rate_interface_fixed_after_LigandMPNN: 5.5%,<br>small_molecule_binder_success_criteria: AF3: backbone RMSD ≤ 1.5 Å; backbone-aligned ligand RMSD ≤ 5 Å; Interface min PAE ≤ 1.5; ipTM ≥ 0.8,<br>small_molecule_result_summary: RFD3 significantly outperforms RFdiffusionAA across the four tested molecules; RFD3 designs are more diverse, novel relative to training set, and have lower Rosetta ∆∆G binding energies (no single-number aggregates reported in main text),<br>AME_win_count: RFD3 outperforms RFD2 on 37 of 41 cases (90%),<br>AME_residue_islands_>4_pass_rate_RFD3: 15%,<br>AME_residue_islands_>4_pass_rate_RFD2: 4%,<br>experimental_DNA_binder_screen_results: 5 designs synthesized; 1 bound with EC50 = 5.89 ± 2.15 μM (yeast surface display),<br>experimental_enzyme_screen_results: 190 designs screened; 35 multi-turnover designs observed; best enzyme Kcat/Km = 3557 |
| **Application Domains** | de novo protein design (generative biomolecular design),<br>protein-protein binder design,<br>protein-DNA binder design,<br>protein-small molecule binder design,<br>enzyme active site scaffolding and enzyme design,<br>symmetric oligomer design,<br>biomolecular modeling and structural prediction (evaluation with AlphaFold3) |

---


### [388. DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning](https://doi.org/10.1038/s41586-025-09422-z), Nature *(September 17, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | AIME 2024,<br>MMLU,<br>MMLU-Redux,<br>MMLU-Pro,<br>DROP,<br>C-Eval,<br>IF-Eval (IFEval),<br>FRAMES,<br>GPQA Diamond,<br>SimpleQA / C-SimpleQA,<br>CLUEWSC,<br>AlpacaEval 2.0,<br>Arena-Hard,<br>SWE-bench Verified,<br>Aider-Polyglot,<br>LiveCodeBench,<br>Codeforces,<br>CNMO 2024 (Chinese National High School Mathematics Olympiad),<br>MATH-500,<br>Cold-start conversational dataset (paper-curated),<br>Preference pairs for helpful reward model,<br>Safety dataset for safety reward model,<br>Released RL prompts and rejection-sampling data samples |
| **Models** | Transformer,<br>Attention Mechanism |
| **Tasks** | Question Answering,<br>Reading Comprehension,<br>Multi-class Classification,<br>Text Generation,<br>Sequence-to-Sequence,<br>Information Retrieval |
| **Learning Methods** | Reinforcement Learning,<br>Supervised Learning,<br>Fine-Tuning,<br>Knowledge Distillation,<br>Policy Gradient |
| **Performance Highlights** | AIME 2024 (pass@1): 77.9%,<br>AIME 2024 (self-consistency cons@16): 86.7% (with self-consistency decoding),<br>AIME training start baseline (pass@1): 15.6% (initial during RL trajectory),<br>English MMLU (EM): 90.8,<br>MMLU-Redux (EM): 92.9,<br>MMLU-Pro (EM): 84.0,<br>DROP (3-shot F1): 92.2,<br>IF-Eval (Prompt Strict): 83.3,<br>GPQA Diamond (Pass@1): 71.5,<br>SimpleQA (Correct): 30.1,<br>FRAMES (Acc.): 82.5,<br>AlpacaEval 2.0 (LC-winrate): 87.6,<br>Arena-Hard (vs GPT-4-1106): 92.3,<br>Code LiveCodeBench (Pass@1-COT): 65.9,<br>Codeforces (Percentile): 96.3,<br>Codeforces (Rating): 2,029,<br>SWE-bench Verified (Resolved): 49.2,<br>Aider-Polyglot (Acc.): 53.3,<br>AIME 2024 (Pass@1): 79.8,<br>MATH-500 (Pass@1): 97.3,<br>CNMO 2024 (Pass@1): 78.8,<br>CLUEWSC (EM): 92.8,<br>C-Eval (EM): 91.8,<br>C-SimpleQA (Correct): 63.7,<br>Code LiveCodeBench (Pass@1-COT): 63.5,<br>Codeforces (Percentile): 90.5,<br>AIME 2024 (Pass@1): 74.0,<br>MATH-500 (Pass@1): 95.9,<br>CNMO 2024 (Pass@1): 73.9,<br>helpful preference pairs curated: 66,000 pairs,<br>safety annotations curated: 106,000 prompts,<br>qualitative statement: Distilled models "exhibit strong reasoning capabilities, surpassing the performance of their original instruction-tuned counterparts." |
| **Application Domains** | Mathematics (math competitions, AIME, CNMO, MATH-500),<br>Computer programming / Software engineering (Codeforces, LiveCodeBench, LiveCodeBench, SWE-bench Verified),<br>Biology (graduate-level problems),<br>Physics (graduate-level problems),<br>Chemistry (graduate-level problems),<br>Instruction following / conversational AI (IF-Eval, AlpacaEval, Arena-Hard),<br>Information retrieval / retrieval-augmented generation (FRAMES),<br>Safety and alignment evaluation (safety datasets, reward models) |

---


### [386. Modeling-Making-Modulating High-Entropy Alloy with Activated Water-Dissociation Centers for Superior Electrocatalysis](https://doi.org/10.1021/jacs.5c08012), Journal of the American Chemical Society *(September 17, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | DFT adsorption dataset for PtPdRhRuMo HEA (this work),<br>Open Catalyst Project pretrained graph neural networks (OC20/OC22),<br>Predicted composition screening set (CatBoost evaluations) |
| **Models** | CatBoost,<br>Linear Model,<br>Support Vector Machine,<br>Random Forest,<br>Gradient Boosting Tree,<br>XGBoost,<br>Multi-Layer Perceptron,<br>Graph Neural Network,<br>Graph Convolutional Network |
| **Tasks** | Regression,<br>Hyperparameter Optimization,<br>Feature Selection,<br>Optimization,<br>Representation Learning |
| **Learning Methods** | Supervised Learning,<br>Pre-training,<br>Transfer Learning,<br>Representation Learning |
| **Performance Highlights** | MAE_train_eV: ∼0.03,<br>MAE_test_eV: ∼0.07,<br>MAE_test_eV: < 0.1 |
| **Application Domains** | Electrocatalysis,<br>Methanol Oxidation Reaction (MOR),<br>Catalyst design for energy conversion,<br>High-Entropy Alloy (HEA) materials discovery,<br>Computational materials science (DFT + ML integration) |

---


### [385. Learning the natural history of human disease with generative transformers](https://doi.org/10.1038/s41586-025-09529-3), Nature *(September 17, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | UK Biobank (first-occurrence disease data),<br>Danish national registries (Danish National Patient Registry, Danish Register of Causes of Death),<br>Delphi-2M-sampled synthetic dataset |
| **Models** | Transformer,<br>GPT,<br>BERT,<br>Linear Model,<br>Encoder-Decoder |
| **Tasks** | Language Modeling,<br>Multi-class Classification,<br>Binary Classification,<br>Survival Analysis,<br>Time Series Forecasting,<br>Data Generation,<br>Representation Learning |
| **Learning Methods** | Self-Supervised Learning,<br>Supervised Learning,<br>Transfer Learning,<br>Stochastic Learning,<br>Backpropagation,<br>Maximum Likelihood Estimation,<br>Representation Learning,<br>End-to-End Learning |
| **Performance Highlights** | average age–sex-stratified AUC (internal validation, next-token prediction, averaged across diagnoses): ≈0.76,<br>AUC (death, age-stratified, internal validation): 0.97,<br>AUC (long-term, 10 years horizon average): 0.70 (average AUC decreases from ~0.76 to 0.70 after 10 years),<br>calibration: Predicted rates closely match observed counts in calibration analyses in 5-year age brackets (qualitative, shown in Extended Data Fig. 3),<br>time-to-event prediction accuracy (aggregate): Model provides consistent estimates of inter-event times (Fig. 1g and methods describe log-likelihood for exponential waiting times),<br>synthetic-trained-model AUC (age–sex-stratified average on observed validation data): 0.74 (trained exclusively on Delphi-2M synthetic data; ~3 percentage points lower than original Delphi-2M),<br>fraction of correctly predicted disease tokens in year 1 of sampling: 17% (compared with 12–13% using sex and age alone),<br>fraction correct after 20 years: <14%,<br>Dementia AUC (Transformer baseline): 0.79,<br>Death AUC (Transformer baseline): 0.78,<br>CVD AUC (Transformer baseline shown alongside others): 0.69 (Transformer as listed in Fig. 2f) |
| **Application Domains** | Population-scale human disease progression modeling,<br>Epidemiology / public health planning (disease burden projection),<br>Clinical risk prediction (CVD, dementia, diabetes, death, and >1,000 ICD-10 diagnoses),<br>Synthetic data generation for privacy-preserving biomedical model training,<br>Explainable AI for healthcare (embedding/SHAP-based interpretability),<br>Precision medicine / individualized prognostication |

---


### [384. Discovery of Unstable Singularities](https://doi.org/10.48550/arXiv.2509.14185), Preprint *(September 17, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Córdoba-Córdoba-Fontelos (CCF) model collocation data (synthetic, generated in self-similar coordinates),<br>Incompressible Porous Media (IPM) with boundary collocation data (synthetic, generated in self-similar coordinates),<br>2D Boussinesq (with boundary) collocation data (synthetic, generated in self-similar coordinates) |
| **Models** | Multi-Layer Perceptron,<br>Feedforward Neural Network |
| **Tasks** | Regression,<br>Optimization,<br>Hyperparameter Optimization |
| **Learning Methods** | Self-Supervised Learning,<br>Multi-Stage Training,<br>Backpropagation,<br>Gradient Descent,<br>Stochastic Learning,<br>Mini-Batch Learning |
| **Performance Highlights** | CCF stable log10(max residual): -13.714,<br>CCF 1st unstable log10(max residual): -13.589,<br>CCF 2nd unstable log10(max residual): -6.664,<br>IPM stable log10(max residual): -11.183,<br>IPM 1st unstable log10(max residual): -10.510,<br>IPM 2nd unstable log10(max residual): -8.101,<br>IPM 3rd unstable log10(max residual): -7.526,<br>Boussinesq stable log10(max residual): -8.178,<br>Boussinesq 1st unstable log10(max residual): -8.038,<br>Boussinesq 2nd unstable log10(max residual): -7.772,<br>Boussinesq 3rd unstable log10(max residual): -7.558,<br>Boussinesq 4th unstable log10(max residual): -7.020,<br>CCF stable residual (order): O(10^-13),<br>CCF 1st unstable residual (order): O(10^-13),<br>IPM stable residual (order): O(10^-11),<br>IPM 1st unstable residual (order): O(10^-10),<br>convergence to O(10^-8) with GN: ≈50k iterations (~3 A100 GPU hours),<br>λ for CCF 1st unstable (from literature and this work agreement): λ1 ≈ 0.6057 (Wang et al.); reproduced/improved here,<br>λ for CCF 2nd unstable (this work): λ2 = 0.4703 (text) |
| **Application Domains** | Mathematical fluid dynamics,<br>Partial differential equations (PDEs) / numerical analysis,<br>Singularity formation and mathematical physics,<br>Computer-assisted proofs (rigorous numerics) |

---


### [382. A Generative Foundation Model for Antibody Design](https://doi.org/10.1101/2025.09.12.675771), Preprint *(September 16, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | SAbDab (training set up to 2022-12-31),<br>SAb23H2 / SAb-23H2-Ab (test set),<br>SAb-23H2-Nano (nanobody test set),<br>PPSM pre-training corpora (UniRef50, PDB multimers, PPI, OAS antibody pairs),<br>IgDesign test set (used for inverse design benchmarking),<br>PD-L1 experimental de novo design dataset (wet-lab candidates),<br>Case-study experimental targets (datasets of antigens used in experiments) |
| **Models** | Denoising Diffusion Probabilistic Model,<br>Diffusion Model,<br>Transformer,<br>Attention Mechanism,<br>Self-Attention Network,<br>Multi-Head Attention,<br>Graph Neural Network |
| **Tasks** | Structured Prediction,<br>Sequence-to-Sequence,<br>Data Generation,<br>Optimization,<br>Ranking,<br>Regression |
| **Learning Methods** | Pre-training,<br>Fine-Tuning,<br>Self-Supervised Learning,<br>Curriculum Learning,<br>Transfer Learning,<br>Representation Learning,<br>Fine-Tuning (task-specific) + Frequency-based selection (screening) |
| **Performance Highlights** | TM-Score: 0.9591,<br>lDDT: 0.8956,<br>RMSD (antibody): 2.1997 Å,<br>DockQ: 0.2986,<br>iRMS: 6.2195 Å,<br>LRMS: 19.4888 Å,<br>Success Rate (SR, DockQ>0.23): 0.4667,<br>AAR (CDR L2): median ≈ 0.8 (Fig.2f described); (Table B4 AAR values for IgGM: L1 0.750, L2 0.743, L3 0.635, H1 0.740, H2 0.644, H3 0.360),<br>Protein A binding (VHH3 variants): VHH3-4M KD = 387 nM; VHH3-5M KD = 384 nM; VHH3-WT no binding (KD >10,000 nM),<br>Humanization (mouse→human) KD comparisons: Mouse KD = 0.120 nM; Human-1 KD = 0.171 nM; Human-2 KD = 0.195 nM; Human-3 KD = 0.395 nM; Human-4 KD = 0.486 nM; Human-5 KD = 0.139 nM,<br>VH-Humanness Score (average): IgGM designed average 0.909 vs original murine 0.676 (text); CDR3 RMSD: IgGM 0.750 Å vs BioPhi 0.983 Å,<br>Structural preservation backbone RMSD: average backbone RMSD reported as 1.10 ± 0.06 Å,<br>I7 (anti-IL-33) KD improvement: Original KD = 52.02 nM → Best variant KD = 9.753 nM (5.3-fold improvement),<br>I7 EC50 improvements (first round): ['Mutants M1, M7, M10 achieved 4–6× increase in affinity (ELISA)'],<br>Broadly neutralizing R1-32 variants (SARS-CoV-2 RBD) KD changes: Q61E enabled binding to Lambda and BQ.1.1 from no binding (examples: Q61E-Lambda KD 948 nM; Q61E-BQ.1.1 KD 948 nM), subsequent N58D,Q61E improved BQ.1.1 KD to 107 nM (~9–10× improvement compared to earlier),<br>PD-L1 de novo design success rate: 7/60 candidates had nanomolar or picomolar KD (success rate 7/60 ≈ 11.7%),<br>Top de novo binder KD range: 0.084 nM (D1) to 2.89 nM (D7),<br>Example D1 KD: 0.084 nM; D1 IC50 = 7.29 nM; D1 displaced PD-1 from PD-L1 in competition assays,<br>Ablation impact (w/o PPSM): Comparative ablation: w/o PPSM AAR 0.322 vs full IgGM AAR 0.360; DockQ 0.233 vs 0.246; SR 0.426 vs 0.433 (Table B5),<br>Contextual effect: PPSM improves interface bias and sequence recovery marginally |
| **Application Domains** | Antibody engineering / design,<br>Structural biology (protein structure prediction and docking),<br>Therapeutic antibody discovery (immuno-oncology PD-L1, anti-viral SARS-CoV-2),<br>Protein engineering (framework optimization, humanization),<br>Computational biology / bioinformatics (protein sequence-structure co-design) |

---


### [381. MSnLib: efficient generation of open multi-stage fragmentation mass spectral libraries](https://doi.org/10.1038/s41592-025-02813-0), Nature Methods *(September 15, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | MSnLib (this work),<br>MCEBIO library (subset used in analyses),<br>NIH NPAC ACONN (NIHNP),<br>OTAVAPEP (peptidomimetic library),<br>ENAMDISC (Discovery Diverse Set DDS-10 from Enamine),<br>ENAMMOL (Enamine + Molport mixture, incl. carboxylic acid fragment library),<br>MCESCAF (MCE 5K Scaffold Library),<br>MCEDRUG (FDA-approved drugs subset from MCE),<br>Evaluation dataset: drug-incubated bacterial cultures (MSV000096589),<br>Public spectral libraries (comparison references) |
| **Models** | _None_ |
| **Tasks** | Classification,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Extraction,<br>Information Retrieval |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | clinical metabolomics,<br>natural product discovery,<br>exposomics,<br>untargeted liquid chromatography–mass spectrometry (LC–MS) annotation,<br>microbial metabolite analysis / metabolomics |

---


### [380. Spatial gene expression at single-cell resolution from histology using deep learning with GHIST](https://doi.org/10.1038/s41592-025-02795-z), Nature Methods *(September 15, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | BreastCancer1 (10x Xenium),<br>BreastCancer2 (10x Xenium),<br>LungAdenocarcinoma (10x Xenium),<br>Melanoma (10x Xenium),<br>BreastCancerILC and BreastCancerIDC (10x Xenium),<br>HER2ST spatial transcriptomics dataset,<br>NuCLS dataset,<br>TCGA-BRCA (The Cancer Genome Atlas - Breast Invasive Carcinoma),<br>Mixed DCIS cohort (in-house),<br>Single-cell reference datasets (breast, melanoma, lung) |
| **Models** | U-Net,<br>Convolutional Neural Network,<br>DenseNet,<br>VGG,<br>ResNet,<br>Transformer,<br>Graph Neural Network,<br>Multi-Head Attention,<br>Cross-Attention,<br>Encoder-Decoder,<br>Multi-Layer Perceptron,<br>Linear Model |
| **Tasks** | Regression,<br>Image-to-Image Translation,<br>Semantic Segmentation,<br>Multi-class Classification,<br>Weakly Supervised Learning,<br>Survival Analysis,<br>Clustering,<br>Feature Extraction |
| **Learning Methods** | Multitask Learning,<br>Supervised Learning,<br>Weakly Supervised Learning,<br>End-to-End Learning,<br>Pre-training,<br>Self-Supervised Learning,<br>Transfer Learning,<br>Gradient Descent |
| **Performance Highlights** | cell-type_accuracy_BreastCancer1: 0.75,<br>cell-type_accuracy_BreastCancer2: 0.66,<br>median_PCC_top20_SVGs: 0.7,<br>median_PCC_top50_SVGs: 0.6,<br>gene_corr_SCD: 0.74,<br>gene_corr_FASN: 0.77,<br>gene_corr_FOXA1: 0.8,<br>gene_corr_EPCAM: 0.84,<br>melanoma_celltype_proportion_corr: 0.92,<br>lung_adenocarcinoma_celltype_proportion_corr: 0.97,<br>PCC_all_genes: 0.16,<br>SSIM_all_genes: 0.1,<br>PCC_HVGs: 0.2,<br>PCC_SVGs: 0.27,<br>SSIM_HVGs: 0.17,<br>SSIM_SVGs: 0.26,<br>RMSE_all_genes: 0.2,<br>RMSE_SVGs: 0.22,<br>top_gene_correlations: {'GNAS': 0.42, 'FASN': 0.42, 'SCD': 0.34, 'MYL12B': 0.32, 'CLDN4': 0.32},<br>C-index_GHIST: 0.57,<br>C-index_RNASeq_STgene_baseline: 0.55,<br>Kaplan_Meier_logrank_P: 0.017,<br>PCC_all_genes: 0.14,<br>SSIM_all_genes: 0.08,<br>PCC_all_genes: 0.11,<br>SSIM_all_genes: 0.07 |
| **Application Domains** | Histopathology (H&E imaging),<br>Spatial transcriptomics (subcellular and spot-based SRT),<br>Cancer (breast cancer, HER2+ subtype, luminal cohort),<br>Lung adenocarcinoma,<br>Melanoma,<br>Multi-omics integration (TCGA multi-omics),<br>Biomarker discovery and survival prognosis |

---


### [378. Bridging histology and spatial gene expression across scales](https://doi.org/10.1038/s41592-025-02806-z), Nature Methods *(September 15, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | 10x Xenium,<br>10x Visium,<br>The Cancer Genome Atlas (TCGA) H&E slides,<br>Various cancer datasets (breast cancer, lung adenocarcinoma, melanoma),<br>Gastric cancer samples (used with iSCALE),<br>Multiple sclerosis brain tissue (used with iSCALE),<br>Single-cell RNA-sequencing reference atlas,<br>Subcellular-resolution spatial transcriptomics data (general) |
| **Models** | Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>Encoder-Decoder,<br>Graph Neural Network |
| **Tasks** | Regression,<br>Multi-Task Learning,<br>Image Super-Resolution,<br>Multi-class Classification,<br>Semantic Segmentation,<br>Synthetic Data Generation |
| **Learning Methods** | Multi-Task Learning,<br>Supervised Learning,<br>Unsupervised Learning,<br>Transfer Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | Spatial transcriptomics,<br>Histopathology / H&E image analysis,<br>Cancer biology (breast cancer, lung adenocarcinoma, melanoma, gastric cancer),<br>Neuropathology (multiple sclerosis brain tissue),<br>Biobanks and archived clinical cohorts (e.g., TCGA),<br>Digital spatial omics and tissue-wide molecular mapping |

---


### [377. Structural Insights into Autophagy in the AlphaFold Era](https://doi.org/10.1016/j.jmb.2025.169235), Journal of Molecular Biology *(September 15, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | _None_ |
| **Models** | _None_ |
| **Tasks** | _None_ |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | Autophagy research,<br>Structural biology / protein structure prediction,<br>Molecular biology,<br>Biophysics,<br>Therapeutic/drug discovery (rational drug design) |

---


### [376. Scaling up spatial transcriptomics for large-sized tissues: uncovering cellular-level tissue architecture beyond conventional platforms with iSCALE](https://doi.org/10.1038/s41592-025-02770-8), Nature Methods *(September 15, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Gastric cancer Xenium sample - BS06-9313-8_Tumor (Tumor),<br>Gastric Xenium sample - Normal 1 (Gastric Patient 1, Normal 1),<br>Gastric Xenium sample - Normal 2 (Gastric Patient 2, Normal 2),<br>Human large-sized MS brain sample - MS330-AL (MS Sample 1),<br>Human large-sized MS brain sample - MS330-CAL (MS Sample 2),<br>Pseudo-Visium daughter captures (simulated from Xenium full-slide data) |
| **Models** | Multi-Layer Perceptron,<br>Vision Transformer |
| **Tasks** | Regression,<br>Semantic Segmentation,<br>Multi-class Classification,<br>Clustering,<br>Feature Extraction,<br>Dimensionality Reduction |
| **Learning Methods** | Weakly Supervised Learning,<br>Supervised Learning,<br>Transfer Learning,<br>Out-of-Distribution Learning,<br>Mini-Batch Learning,<br>Gradient Descent,<br>End-to-End Learning |
| **Performance Highlights** | alignment_accuracy: 99% (semiautomatic alignment algorithm accuracy for daughter captures),<br>RMSE: iSCALE-Seq outperformed iStar across RMSE (displayed in Fig. 3a; lower is better),<br>SSIM: iSCALE-Seq outperformed iStar across SSIM (displayed in Fig. 3a; higher is better),<br>Pearson_correlation: iSCALE-Seq achieved higher Pearson correlations vs iStar; ~50% of genes achieved r > 0.45 at 32 µm resolution for iSCALE-Seq; example per-gene r values shown (e.g., iSCALE-Seq r = 0.5037 for one gene in Fig. 3c),<br>adjusted_Rand_index: 0.74 (segmentation result from out-of-sample predictions closely aligns with in-sample segmentation; reported when comparing segmentations in normal gastric out-of-sample experiment),<br>RMSE: iSCALE-Img achieves low RMSE in in-sample evaluations; comparable performance to iSCALE-Seq (Fig. 3a),<br>SSIM: High SSIM relative to competing methods (Fig. 3a),<br>Pearson_correlation: iSCALE-Img had generally low Pearson at superpixel level but improved with larger superpixel sizes; example reported correlations improved with resolution,<br>Spearman_correlation: Out-of-sample predictions: at 64 µm resolution ≈50% of genes achieved Spearman r > 0.45; overall Spearman reported across resolutions (Fig. 4c),<br>chi_squared_concordance: 99 of the top 100 HVGs exhibited significantly concordant out-of-sample predicted expression patterns vs ground truth (chi-squared statistic with Bonferroni correction),<br>adjusted_Rand_index: 0.74 (segmentation agreement between out-of-sample and in-sample segmentations in Normal gastric data) |
| **Application Domains** | Spatial transcriptomics,<br>Histopathology / digital pathology (H&E image analysis),<br>Oncology (gastric cancer tissue analysis),<br>Neurology / Multiple sclerosis brain tissue analysis,<br>Single-cell and spatial genomics integration |

---


### [375. Integrating diverse experimental information to assist protein complex structure prediction by GRASP](https://doi.org/10.1038/s41592-025-02820-1), Nature Methods *(September 15, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | PSP dataset,<br>Self-curated benchmark dataset (contact RPR and IR),<br>Simulated XL dataset (SDA XL simulations),<br>Experimental XL dataset (real-world XL cases),<br>CL dataset (covalent labeling),<br>CSP dataset (chemical shift perturbation),<br>Simulated DMS (BM5.5) dataset,<br>Hitawala–Gray simulated DMS dataset,<br>Experimental DMS dataset (SARS-CoV-2 RBD antibodies),<br>Mitochondria in situ XL-MS dataset |
| **Models** | Transformer,<br>Attention Mechanism,<br>Self-Attention Network,<br>Multi-Head Attention,<br>Ensemble (ensemble prediction from multiple checkpoints) |
| **Tasks** | Link Prediction,<br>Ranking,<br>Clustering |
| **Learning Methods** | Fine-Tuning,<br>Pre-training,<br>Transfer Learning,<br>Ensemble Learning,<br>Supervised Learning |
| **Performance Highlights** | benchmark_mean_DockQ_with_2_contact_RPRs: 0.35,<br>benchmark_success_rate_>0.23_with_2_contact_RPRs: 52.7%,<br>IRs_mean_DockQ_for_4_10_20_restraints: 0.24 / 0.34 / 0.41,<br>IRs_success_rate_for_4_10_20_restraints: 35.3% / 51.9% / 63.2%,<br>AFM_without_restraints_mean_DockQ: 0.17,<br>AF3_without_restraints_mean_DockQ: 0.23,<br>simulated_XL_mean_DockQ_1%: 0.18,<br>simulated_XL_mean_DockQ_2%: 0.21,<br>simulated_XL_mean_DockQ_5%: 0.27,<br>HADDOCK_mean_DockQ_1%_2%_5%: 0.06 / 0.08 / 0.10,<br>AlphaLink_mean_DockQ_1%_2%_5%: 0.12 / 0.13 / 0.20,<br>restraint_satisfaction_median_at_2%_coverage_all_correct: 69% (all) / 81% (correct restraints),<br>iterative_noise_filtering_effect: improved DockQ and pLDDT across coverage levels,<br>experimental_XL_mean_DockQ_for_9_samples: 0.48 (GRASP) vs 0.31 (AFM) vs 0.38 (AlphaLink) vs 0.05 (ClusPro) vs 0.05 (HADDOCK),<br>example_4G3Y_DockQ: GRASP 0.77 vs AFM 0.03; AlphaLink 0.67; HADDOCK 0.02,<br>CL_average_DockQ_GRASP: 0.58,<br>CL_average_DockQ_AFM: 0.45,<br>ColabDock_average_DockQ: 0.56,<br>example_4INS8_DockQ: GRASP 0.56 vs AFM 0.43 vs HADDOCK 0.18 vs ClusPro 0.06 vs ColabDock 0.33,<br>CSP_average_DockQ_GRASP: 0.81 (4 cases),<br>CSP_average_DockQ_AF M/HADDOCK/ClusPro/ColabDock: AFM 0.39 / HADDOCK 0.28 / ClusPro 0.21 / ColabDock 0.5,<br>example_4G6M_DockQ_GRASP: 0.9,<br>example_4G6J_DockQ_GRASP: 0.79,<br>BM5.5_median_DockQ_GRASP: 0.64,<br>BM5.5_success_rate_GRASP: 71.6%,<br>Hitawala–Gray_median_DockQ_antibodies_GRASP: 0.477,<br>Hitawala–Gray_median_DockQ_nanobodies_GRASP: 0.541,<br>Hitawala–Gray_success_rate_antibodies_GRASP: 60.0%,<br>Hitawala–Gray_success_rate_nanobodies_GRASP: 88.8%,<br>AF3_second_best_median_DockQ_antibodies_AF3: 0.069,<br>AF3_second_best_median_DockQ_nanobodies_AF3: 0.237,<br>Experimental_DMS_median_DockQ_GRASP: 0.25,<br>Experimental_DMS_success_rate_GRASP: 53.6%,<br>Experimental_DMS_median_DockQ_AF3: 0.07,<br>Experimental_DMS_success_rate_AF3: 39.3%,<br>Combined_protocol_median_DockQ: 0.28,<br>Combined_protocol_success_rate: 56.0%,<br>mitochondria_predicted_PPIs_total: 144 PPIs predicted (121 had pLDDT > 75),<br>XLs_satisfied_GRASP: 140/144 predicted PPIs satisfied XLs,<br>XLs_satisfied_AFM: 31/144,<br>median_TM_score_on_17_ground_truth_pairs_GRASP: 0.881,<br>median_TM_score_on_17_ground_truth_pairs_AFM: 0.838,<br>pLDDT_Pearson_correlation_with_DockQ: r = 0.39,<br>pLDDT_Pearson_correlation_with_TM_score: r = 0.65,<br>pLDDT_Pearson_correlation_with_LDDT: r = 0.87,<br>improvements_in_pLDDT_vs_gains_in_DockQ_TM_LDDT: r = 0.38 (DockQ), 0.58 (TM), 0.77 (LDDT) |
| **Application Domains** | protein complex structure prediction / structural biology,<br>antigen–antibody modelling and antibody design / immunotherapy,<br>integrative structural biology (integrating XL-MS, CL, CSP, DMS, cryo-EM, NMR PRE, mutagenesis),<br>in situ interactome modelling (mitochondrial PPI mapping),<br>computational docking and restrained docking workflows |

---


### [374. De novo discovery of conserved gene clusters in microbial genomes with Spacedust](https://doi.org/10.1038/s41592-025-02816-x), Nature Methods *(September 15, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | 1,308 representative bacterial genomes (reference database),<br>Gold-standard BGC dataset (nine complete genomes),<br>GTDB (Genome Taxonomy Database),<br>AlphaFold structure database (and other structure DBs: PDB, ESMAtlas),<br>PADLOC antiviral defense annotations,<br>AntiSMASH functional annotation (version 8) |
| **Models** | T5,<br>Transformer,<br>Hidden Markov Model |
| **Tasks** | Clustering,<br>Information Retrieval,<br>Feature Extraction,<br>Classification |
| **Learning Methods** | Representation Learning,<br>Pre-training |
| **Performance Highlights** | AUC (precision–recall) i,i+1: 0.93,<br>AUC (precision–recall) i,i+2: 0.93,<br>AUC (precision–recall) i,i+3: 0.86,<br>AUC (precision–recall) i,i+4: 0.81,<br>AUC (precision–recall) i,i+1: 0.91,<br>AUC (precision–recall) i,i+2: 0.89,<br>AUC (precision–recall) i,i+3: 0.83,<br>AUC (precision–recall) i,i+4: 0.77,<br>PADLOC multi-gene defense clusters (reference): 5,520,<br>Spacedust recovery of PADLOC clusters: 5,255 (95%),<br>Spacedust full-length matches: 4,888 (93% of recovered clusters),<br>Spacedust partial matches: 367 (7% of recovered clusters),<br>Non-redundant clusters (paired matches grouped): 72,483 nonredundant clusters comprising 2.45M genes (58% of dataset),<br>All pairwise cluster hits: 321.2M cluster hits in 106.6M cluster matches; mean genes per cluster match = 3.01,<br>Spacedust assignment: 58% of all 4.2M genes assigned to conserved gene clusters; 35% of genes without any annotation assigned to clusters,<br>Average F1 score (Spacedust): 0.61,<br>Average F1 score (ClusterFinder): 0.44,<br>Average F1 score (DeepBGC): 0.39,<br>Average F1 score (GECCO): 0.43 |
| **Application Domains** | Microbial genomics (bacteria and archaea),<br>Metagenomics / metagenome-assembled genomes,<br>Microbiome research (environmental and human-associated microbiomes),<br>Comparative genomics / evolutionary genomics (gene neighborhood conservation),<br>Functional annotation of genes (operons, antiviral defense systems, biosynthetic gene clusters),<br>CRISPR–Cas systems discovery (e.g., expansion of subtype III-E),<br>Biosynthetic gene cluster discovery and natural product genome mining |

---


### [372. Guided multi-agent AI invents highly accurate, uncertainty-aware transcriptomic aging clocks](https://doi.org/10.1101/2025.09.08.674588), Preprint *(September 12, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | ARCHS4,<br>ARCHS4 — blood subset,<br>ARCHS4 — colon subset,<br>ARCHS4 — lung subset,<br>ARCHS4 — ileum subset,<br>ARCHS4 — heart subset,<br>ARCHS4 — adipose subset,<br>ARCHS4 — retina subset |
| **Models** | XGBoost,<br>LightGBM,<br>Support Vector Machine,<br>Linear Model,<br>Transformer |
| **Tasks** | Regression,<br>Feature Selection,<br>Feature Extraction,<br>Clustering |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Imbalanced Learning |
| **Performance Highlights** | R2: 0.619,<br>R2: 0.604,<br>R2: 0.574,<br>R2_Ridge: 0.539,<br>R2_ElasticNet: 0.310,<br>R2: 0.957,<br>MAE_years: 3.7,<br>R2_all: 0.726,<br>MAE_all_years: 6.17,<br>R2_confidence_weighted: 0.854,<br>MAE_confidence_weighted_years: 4.26,<br>mean_calibration_error: 0.7%,<br>R2_per_window_range: ≈0.68–0.74,<br>lung_R2: 0.969,<br>blood_R2: 0.958,<br>ileum_R2: 0.958,<br>heart_R2: 0.910,<br>adipose_R2: 0.887,<br>retina_R2: 0.594 |
| **Application Domains** | aging biology / geroscience,<br>transcriptomics,<br>biomarker discovery,<br>computational biology / bioinformatics,<br>clinical biomarker development (biological age clocks),<br>AI-assisted scientific discovery (multi-agent workflows) |

---


### [371. Flexynesis: A deep learning toolkit for bulk multi-omics data integration for precision oncology and beyond](https://doi.org/10.1038/s41467-025-63688-5), Nature Communications *(September 12, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | CCLE,<br>GDSC2,<br>TCGA (multiple cohorts: pan-cancer, COAD, ESCA, PAAD, READ, STAD, UCEC, UCS, LGG, GBM),<br>METABRIC,<br>DepMap,<br>ProtTrans protein sequence embeddings (precomputed),<br>describePROT features,<br>STRING interaction networks,<br>METABRIC (as dataset entry repeated for clarity),<br>Single-cell CITE-Seq of bone marrow,<br>PRISM drug screening and CRISPR screens (DepMap components) |
| **Models** | Multi-Layer Perceptron,<br>Feedforward Neural Network,<br>Variational Autoencoder,<br>Triplet Network,<br>Graph Neural Network,<br>Graph Convolutional Network,<br>Support Vector Machine,<br>Random Forest,<br>XGBoost,<br>Random Survival Forest,<br>Graph Attention Network |
| **Tasks** | Regression,<br>Classification,<br>Survival,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Selection,<br>Representation Learning |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Multi-Task Learning,<br>Transfer Learning,<br>Fine-Tuning,<br>Hyperparameter Optimization,<br>Contrastive Learning,<br>Representation Learning,<br>Domain Adaptation |
| **Performance Highlights** | Pearson_correlation_Lapatinib: r = 0.6 (p = 7.750175e-42),<br>Pearson_correlation_Selumetinib: r = 0.61 (p = 3.873949e-50),<br>AUC_MSI_prediction: AUC = 0.981,<br>logrank_p: p = 9.94475168880626e-10,<br>per-cell-line_correlation_distribution: median correlations shown per cell line (N=1064) with improvement when adding ProtTrans embeddings; exact medians not specified numerically in text,<br>cross-domain_TCGA->CCLE_before_finetuning_F1: approx. 0.16,<br>cross-domain_TCGA->CCLE_after_finetuning_F1: up to 0.8 |
| **Application Domains** | Precision oncology / cancer genomics,<br>Pharmacogenomics (drug response prediction),<br>Clinical genomics (survival prediction, biomarker discovery),<br>Functional genomics (gene essentiality prediction, DepMap analyses),<br>Multi-omics data integration (transcriptome, methylome, CNV, mutation),<br>Proteomics / protein sequence analysis (ProtTrans embeddings, describePROT),<br>Single-cell multi-omics (CITE-Seq cell type classification proof-of-concept),<br>Bioinformatics tool development and benchmarking |

---


### [370. Biophysics-based protein language models for protein engineering](https://doi.org/10.1038/s41592-025-02776-2), Nature Methods *(September 11, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Rosetta simulated pretraining data (METL-Local),<br>Rosetta simulated pretraining data (METL-Global),<br>GFP (green fluorescent protein) experimental dataset,<br>DLG4-Abundance (DLG4-A) experimental dataset,<br>DLG4-Binding (DLG4-B) experimental dataset,<br>GB1 experimental dataset,<br>GRB2-Abundance (GRB2-A) experimental dataset,<br>GRB2-Binding (GRB2-B) experimental dataset,<br>Pab1 experimental dataset,<br>PTEN-Abundance (PTEN-A) experimental dataset,<br>PTEN-Activity (PTEN-E) experimental dataset,<br>TEM-1 experimental dataset,<br>Ube4b experimental dataset,<br>METL Rosetta datasets (archived) |
| **Models** | Transformer,<br>Linear Model,<br>Convolutional Neural Network,<br>Feedforward Neural Network,<br>Attention Mechanism,<br>Multi-Head Attention |
| **Tasks** | Regression,<br>Synthetic Data Generation,<br>Data Augmentation,<br>Optimization,<br>Representation Learning |
| **Learning Methods** | Pre-training,<br>Fine-Tuning,<br>Supervised Learning,<br>Self-Supervised Learning,<br>Transfer Learning,<br>Feature Extraction,<br>Zero-Shot Learning,<br>Representation Learning |
| **Performance Highlights** | mean_Spearman_correlation: 0.91,<br>in_distribution_mean_Spearman: 0.85,<br>out_of_distribution_mean_Spearman: 0.16,<br>mutation_extrapolation_avg_Spearman_range: ~0.70-0.78,<br>ProteinNPT_Spearman: 0.65,<br>METL-Local_Spearman: 0.59,<br>supervised_models_avg_Spearman: >0.75,<br>ProteinNPT_avg_Spearman: 0.67,<br>typical_Spearman: <0.3,<br>GB1_supervised_models_Spearman: >=0.55,<br>GB1_METL-Local_METL-Global_Spearman: >0.7,<br>METL-Bind_median_Spearman: 0.94,<br>METL-Local_median_Spearman: 0.93,<br>Linear_median_Spearman: 0.92,<br>Regime_METL-Bind_Spearman: 0.76,<br>Regime_METL-Local_Spearman: 0.74,<br>Regime_Linear_Spearman: 0.56,<br>designed_variants_with_measurable_fluorescence: 16/20,<br>Observed_5-mutants_success_rate: 5/5 (100%),<br>Observed_10-mutants_success_rate: 5/5 (100%),<br>Unobserved_5-mutants_success_rate: 4/5 (80%),<br>Unobserved_10-mutants_success_rate: 2/5 (40%),<br>competitive_on_small_N: Linear-EVE and Linear sometimes competitive with METL-Local on small training sets (dataset dependent),<br>general_performance: CNN baselines were generally outperformed by METL-Local; specific numbers in Supplementary Fig.7 |
| **Application Domains** | Protein engineering,<br>Biophysics / molecular simulation,<br>Computational protein design,<br>Structural biology,<br>Enzyme engineering (catalysis),<br>Fluorescent protein engineering (GFP brightness),<br>Stability and expression prediction |

---


### [369. Towards agentic science for advancing scientific discovery](https://doi.org/10.1038/s42256-025-01110-x), Nature Machine Intelligence *(September 10, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | AFMBench (autonomous microscopy benchmark),<br>METR (benchmark for long-horizon multi-step tasks),<br>Crystallographic databases (generic structured resources),<br>Gene ontologies (as structured resources),<br>Chemical reaction networks (structured representations) |
| **Models** | Transformer,<br>Attention Mechanism,<br>Multi-Head Attention,<br>Multi-Layer Perceptron |
| **Tasks** | Experimental Design,<br>Novelty Detection,<br>Text Generation,<br>Language Modeling,<br>Sequence-to-Sequence,<br>Graph Generation,<br>Question Answering,<br>Novelty Detection |
| **Learning Methods** | Active Learning,<br>Transfer Learning,<br>Self-Supervised Learning,<br>Reinforcement Learning,<br>Fine-Tuning,<br>Supervised Learning |
| **Performance Highlights** | qualitative_outcome: struggles / compounds errors over time,<br>qualitative_outcome: revealed critical failure modes |
| **Application Domains** | Chemistry,<br>Materials Science,<br>Microscopy / Laboratory Automation,<br>Crystallography / Materials Characterization,<br>Biology / Genomics (via gene ontologies),<br>Clinical domains (clinical diagnosis mentioned as boundary condition),<br>Social Sciences (noted as challenging domain),<br>Autonomous Laboratories / Robotics-enabled synthesis |

---


### [368. Molecular-dynamics-simulation-guided directed evolution of flavoenzymes for atroposelective desaturation](https://doi.org/10.1038/s44160-025-00882-9), Nature Synthesis *(September 10, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | MD simulation trajectories (monomer and tetramer),<br>Docking inputs (substrate enantiomers into AlphaFold2-generated structure),<br>X-ray crystal structures and small-molecule crystal data,<br>Experimental enzymatic dataset: kinetics, yields and enantioselectivities (substrate scope),<br>In-house enzyme collection (purified flavoenzymes panel) |
| **Models** | Hidden Markov Model |
| **Tasks** | Clustering,<br>Experimental Design |
| **Learning Methods** | Unsupervised Learning |
| **Performance Highlights** | transition_time_ADes-1_vs_ADes-5: ADes-1 transition time from macrostate A to B was an order of magnitude longer than in ADes-5,<br>transition_probability_ADes-1_vs_ADes-5: transition probability from the initial state to the final state in the ADes-1 system was an order of magnitude smaller than in ADes-5,<br>kcat_ADes-1: (1.6 ± 0.1) × 10^-3 min^-1,<br>KM_ADes-1: 1.7 ± 0.3 mM,<br>kcat_ADes-5: (9.3 ± 1.6) × 10^-2 min^-1,<br>KM_ADes-5: 1.3 ± 0.4 mM,<br>kcat/KM_improvement: 70-fold improvement in kcat/KM for ADes-5 relative to ADes-1,<br>yield_best: >99% (selected substrates),<br>ee_best: >99% e.e. (selected substrates),<br>yield_ADes-5_for_2a: 98% yield with 89% e.e. (ADes-5 produced 98% yield of desired product with 89% e.e. in one round; later preparative scale 93% yield, 87% e.e.) |
| **Application Domains** | Biocatalysis,<br>Enzyme engineering / directed evolution,<br>Computational structural biology / molecular dynamics,<br>Synthetic organic chemistry (atroposelective synthesis of biaryls),<br>Drug discovery (synthesis of pharmaceutically relevant atropisomers) |

---


### [367. RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation](https://doi.org/10.48550/arXiv.2509.08820), Preprint *(September 10, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | RoboChemist collected VLA fine-tuning dataset,<br>Training configurations for inner-loop enhancement experiments,<br>Referenced external benchmarks and datasets (not directly used for training) |
| **Models** | Transformer,<br>Diffusion Model,<br>GPT |
| **Tasks** | Control,<br>Planning,<br>Policy Learning,<br>Object Localization,<br>Decision Making |
| **Learning Methods** | Fine-Tuning,<br>Prompt Learning,<br>Supervised Learning,<br>Transfer Learning,<br>Batch Learning |
| **Performance Highlights** | Overall average success rate improvement vs VLA baselines (%): 23.57,<br>Overall average compliance rate increase: 0.298,<br>Grasp Glass Rod SR (%): 55,<br>Grasp Glass Rod CR: 0.325,<br>Grasp Glass Rod SR (%): 20,<br>Grasp Glass Rod CR: 0.100,<br>Grasp Glass Rod SR (%): 40,<br>Grasp Glass Rod CR: 0.200,<br>Grasp Glass Rod SR (%): 85,<br>Grasp Glass Rod CR: 0.750,<br>Grasp Glass Rod SR (%): 95,<br>Grasp Glass Rod CR: 0.875,<br>Heat Platinum Wire SR (%): 20,<br>Heat Platinum Wire CR: 0.063,<br>Heat Platinum Wire SR (%): 60,<br>Heat Platinum Wire CR: 0.363,<br>Heat Platinum Wire SR (%): 55,<br>Heat Platinum Wire CR: 0.325,<br>Heat Platinum Wire SR (%): 70,<br>Heat Platinum Wire CR: 0.575,<br>Heat Platinum Wire SR (%): 90,<br>Heat Platinum Wire CR: 0.800,<br>Insert into Solution SR (%): 10,<br>Insert into Solution CR: 0.050,<br>Insert into Solution SR (%): 80,<br>Insert into Solution CR: 0.775,<br>Insert into Solution SR (%): 80,<br>Insert into Solution CR: 0.800,<br>Insert into Solution SR (%): 85,<br>Insert into Solution CR: 0.850,<br>Insert into Solution SR (%): 95,<br>Insert into Solution CR: 0.950,<br>Pour Liquid SR (%): 25,<br>Pour Liquid CR: 0.288,<br>Pour Liquid SR (%): 90,<br>Pour Liquid CR: 0.675,<br>Pour Liquid SR (%): 80,<br>Pour Liquid CR: 0.475,<br>Pour Liquid SR (%): 80,<br>Pour Liquid CR: 0.663,<br>Pour Liquid SR (%): 95,<br>Pour Liquid CR: 0.800,<br>Stir the Solution SR (%): 15,<br>Stir the Solution CR: 0.075,<br>Stir the Solution SR (%): 75,<br>Stir the Solution CR: 0.400,<br>Stir the Solution SR (%): 85,<br>Stir the Solution CR: 0.600,<br>Stir the Solution SR (%): 95,<br>Stir the Solution CR: 0.650,<br>Stir the Solution SR (%): 100,<br>Stir the Solution CR: 0.825,<br>Transfer the Solid SR (%): 15,<br>Transfer the Solid CR: 0.063,<br>Transfer the Solid SR (%): 75,<br>Transfer the Solid CR: 0.513,<br>Transfer the Solid SR (%): 80,<br>Transfer the Solid CR: 0.525,<br>Transfer the Solid SR (%): 85,<br>Transfer the Solid CR: 0.538,<br>Transfer the Solid SR (%): 95,<br>Transfer the Solid CR: 0.675,<br>Press the Button SR (%): 0,<br>Press the Button CR: 0.100,<br>Press the Button SR (%): 65,<br>Press the Button CR: 0.413,<br>Press the Button SR (%): 70,<br>Press the Button CR: 0.575,<br>Press the Button SR (%): 75,<br>Press the Button CR: 0.613,<br>Press the Button SR (%): 85,<br>Press the Button CR: 0.663,<br>Complete task: Mix NaCl and CuSO4 SR (%): 80,<br>Complete task: Mix NaCl and CuSO4 CR: 0.450,<br>Complete task: Mix NaCl and CuSO4 SR (%): 95,<br>Complete task: Mix NaCl and CuSO4 CR: 0.775,<br>Visual prompting comparison (Grasp Glass Rod) SR (π0) (%): 40,<br>Visual prompting comparison (Grasp Glass Rod) CR (π0): 0.200,<br>ReKep+π0 SR: 35,<br>ReKep+π0 CR: 0.200,<br>MOKA+π0 SR: 65,<br>MOKA+π0 CR: 0.350,<br>RoboChemist w/o CL SR: 85,<br>RoboChemist w/o CL CR: 0.750,<br>Config 1 average SR (%): 67.14,<br>Config 2 average SR (%): 70.00,<br>Config 3 average SR (%): 62.14,<br>Config 4 average SR (%): 22.14,<br>Pouring two cups (standalone VLA w/o visual prompt) success count: 16/20,<br>Pouring three cups (standalone VLA w/o visual prompt) success count: 5/20,<br>Pouring two cups (with visual prompt) success count: 19/20,<br>Pouring three cups (with visual prompt) success count: 17/20 |
| **Application Domains** | Robotic chemistry / laboratory automation,<br>Robotic manipulation (bimanual) in hazardous and deformable-material settings,<br>Vision-language-grounded robotic control,<br>Long-horizon safe procedural automation in chemistry experiments |

---


### [366. AI mirrors experimental science to uncover a mechanism of gene transfer crucial to bacterial evolution](https://doi.org/10.1016/j.cell.2025.08.018), Cell *(September 09, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | _None_ |
| **Models** | _None_ |
| **Tasks** | _None_ |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | unspecified / unclear (paper text is heavily corrupted/encoded and does not explicitly state an application domain),<br>possible domain-specific experimental science (terms such as FO!m{/�mtoz�t��, yomskzt�y, WOIO, SST appear repeatedly but do not match standard AI dataset/model/task names) |

---


### [365. SurFF: a foundation model for surface exposure and morphology across intermetallic crystals](https://doi.org/10.1038/s43588-025-00839-0), Nature Computational Science *(September 09, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Design space of generated intermetallic surfaces,<br>SurFF training dataset (active learning-generated),<br>Active learning test set,<br>In-distribution (ID) test set,<br>Out-of-distribution (OOD) test set,<br>Large-scale prediction dataset (predictions performed on catalytic materials),<br>Experimental validation dataset (literature + original experiments) |
| **Models** | Graph Neural Network,<br>Graph Convolutional Network,<br>Transformer,<br>Attention Mechanism,<br>Ensemble Learning |
| **Tasks** | Regression,<br>Optimization,<br>Multi-class Classification,<br>Representation Learning,<br>Clustering / Sampling (Active Learning selection - diversity sampling) |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Ensemble Learning,<br>Pre-training,<br>Fine-Tuning,<br>Transfer Learning,<br>Representation Learning |
| **Performance Highlights** | MAE_active_learning_test_set (surface energy, meV Å−2): 3.8,<br>MAE_ID_test_set (surface energy, meV Å−2): 3.0,<br>MAE_OOD_test_set (surface energy, meV Å−2): 10.5,<br>Structural_RMS_error_between_SurFF_and_DFT_relaxed_structures (Å): 0.109,<br>Speedup_vs_DFT: 105x,<br>Active_learning_dataset_size_generated (surface energy datapoints): 12,000 (final reported training set),<br>DFT_compute_cost_for_dataset (CPU-hours): 155,612,<br>Overall_synthesizability_accuracy (SurFF vs DFT / experimental context): 71.9%,<br>High_synthesizability_accuracy (SurFF, ID): 77.1%,<br>OOD_top-5_accuracy (surface synthesizability): 0.810,<br>OOD_high_accuracy: 0.744,<br>Experimental_facets_predicted_accuracy: 73.1%,<br>Improvement_single_point_energy_after_fine_tuning (%): 45,<br>Improvement_single_point_force_after_fine_tuning (%): 53,<br>Improvement_surface_energy_MAE_after_fine_tuning (%): 35,<br>Improvement_top-3_accuracy_after_fine_tuning (%): 5.1 |
| **Application Domains** | Heterogeneous catalysis / catalyst design,<br>Materials discovery and screening (intermetallic crystals),<br>Computational materials science / surface science,<br>Quantum chemistry acceleration (DFT surrogate),<br>Nanoparticle morphology prediction (Wulff construction) |

---


### [363. AI-driven protein design](https://doi.org/10.1038/s44222-025-00349-8), Nature Reviews Bioengineering *(September 08, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | UniProt,<br>Protein Data Bank (PDB),<br>AlphaFoldDB / AlphaFold-predicted structures,<br>ESM Metagenomic Atlas (ESM-predicted structures),<br>ESM-IF training set (AlphaFold2-predicted structures),<br>CodonTransformer training set,<br>AAV in silico library / experimental validation sets (AAV capsid case study),<br>De novo luciferase design library (NTF2-like scaffolds),<br>Variational Synthesis generative synthesis scale |
| **Models** | Transformer,<br>Recurrent Neural Network,<br>Convolutional Neural Network,<br>Graph Neural Network,<br>Message Passing Neural Network,<br>Diffusion Model,<br>Variational Autoencoder,<br>Denoising Diffusion Probabilistic Model,<br>Attention Mechanism / Self-Attention Network,<br>Graph Convolutional Network,<br>Geometric 3D networks |
| **Tasks** | Language Modeling,<br>Clustering,<br>Ranking,<br>Binary Classification,<br>Regression,<br>Data Generation,<br>Synthetic Data Generation |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Reinforcement Learning,<br>Transfer Learning,<br>Zero-Shot Learning,<br>Few-Shot Learning,<br>Active Learning,<br>Pre-training,<br>Fine-Tuning,<br>Contrastive Learning |
| **Performance Highlights** | sequence_recovery: from 41.2% to >50.0%,<br>experimental_success: high success rates across diverse, experimentally validated design settings (qualitative),<br>antibody_evolution_binding_improvement: up to 160-fold (some immature antibodies); up to 7-fold (four mature antibodies),<br>filtered_candidates: 201,426 from ~1e10 virtual library,<br>experimental_viable_count: 110,689,<br>viability_rate: 58.1%,<br>activity_improvement: up to 100-fold improvement in protein activity across diverse targets (EVOLVEpro study referenced),<br>binding_affinity_increase: up to 37-fold (rational antibody optimization using ESM-IF),<br>binder_design_affinity: nanomolar affinities reported in de novo binder designs (Gainza et al.; qualitative),<br>MaSIF speedup: searches 20–200 times faster than conventional docking (MaSIF description) |
| **Application Domains** | Protein engineering / protein design,<br>Drug discovery (therapeutic protein and binder design),<br>Biotechnology (industrial enzymes, developability improvement),<br>Synthetic biology (design of novel proteins and biological systems),<br>Gene therapy (AAV capsid engineering),<br>Antibody therapeutics (optimization, de-immunization, humanization),<br>Structural biology (structure prediction and complex modelling),<br>DNA synthesis and experimental protocol optimization |

---


### [362. Accelerating protein engineering with fitness landscape modelling and reinforcement learning](https://doi.org/10.1038/s42256-025-01103-w), Nature Machine Intelligence *(September 08, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | UniRef50 (UR50),<br>ProteinGym v0.1 (substitution),<br>ProteinGym Indel,<br>FLIP (including FLIP AAV VP1 splits),<br>FLEXS benchmark (five ground-truth landscapes),<br>TEM-1 single-mutant DMS (Stiffler et al. ref. 45),<br>Curated ESBL dataset (clinical variants),<br>TEM-1 wet-laboratory validation set (this study),<br>One-to-multi evaluation datasets (GB1, TEM-1, Pab1, PSD-95, GRB2, APP, GFP, YAP1, VP1) |
| **Models** | μFormer,<br>μSearch,<br>Transformer,<br>Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>Multi-Head Attention / Self-Attention Network / Attention Mechanism,<br>Ridge (one-hot-embedding-based),<br>ECNet / DeepSequence / EVmutation / ESM / Tranception / ProteinNPT / ConFit / Augmented DeepSequence |
| **Tasks** | Regression,<br>Ranking,<br>Binary Classification,<br>Optimization,<br>Policy Learning,<br>Out-of-Distribution Learning,<br>Decision Making |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Supervised Learning,<br>Transfer Learning,<br>Reinforcement Learning,<br>Policy Gradient,<br>Actor-Critic,<br>Representation Learning |
| **Performance Highlights** | Spearman_rho_>0.7_percentage: over 50% of ProteinGym datasets,<br>Spearman_rho_>0.9_datasets: 6 datasets exceeded 0.9,<br>indel_Spearman_rho: above 0.8 for Mut-Des and Des-Mut settings on FLIP AAV / ProteinGym Indel,<br>top-100_recall_average: 0.165 (single-to-multi setting across nine proteins),<br>top-500_recall_average: 0.341 (single-to-multi setting across nine proteins),<br>auPRC_examples: auPRC used for ultrahigh-value mutants (GB1 n=681, GRB2 n=426, YAP1 n=488, VP1 n=131) — μFormer outperforms alternatives,<br>Spearman_MIC_correlation_TEM-1: reported values: ρ = 0.94 (text) and ρ = 0.92 (figure caption) with MIC measurements,<br>oracle_queries_for_superior_scores: μSearch reached fitness levels unreachable by other algorithms by ~50,000 queries (single-round TEM-1 with μFormer oracle); comparative experiments with 250,000 queries per method,<br>high_score_sequences_identified: for predicted score >0.3, μSearch identified over 2,000 sequences that others failed to detect,<br>sample_efficiency: μSearch demonstrated superior sample efficiency on several protein landscapes in FLEXS benchmarks (multi-round setting),<br>computational_budget_setting: multi-round simulated design: 10 rounds × 100 candidates per round, budget 5,000 local approximate model calls,<br>computational_screened: 1,000,000 mutant sequences screened across six μSearch runs,<br>candidates_selected_for_experiment: top-200 variants selected for E. coli growth assay,<br>experimental_success_rate: 47 distinct RL-designed variants (23.5% of top-200 tested) exhibited superior growth on cefotaxime relative to wild type; random baseline: 12% improved,<br>growth_fold_change: E. coli harbouring certain variants exhibited growth rates up to 2,000-fold higher than wild type,<br>novel_high_activity_variant: G236S;T261V surpassed activity level of known quadruple mutant A40G;E102K;M180T;G236S |
| **Application Domains** | Protein engineering,<br>Enzyme optimization (e.g., TEM-1 β-lactamase activity against cefotaxime),<br>Antibody design,<br>Drug-resistance prediction,<br>Machine-guided directed evolution / sequence design |

---


### [361. AI-Driven Defect Engineering for Advanced Thermoelectric Materials](https://doi.org/10.1002/adma.202505642), Advanced Materials *(September 04, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Gaultois et al. dataset,<br>Na et al. dataset,<br>Sierepeklis / ChemDataExtractor auto-generated TE dataset,<br>SpringerMaterials TE dataset,<br>Itani et al. (LLM-extracted) TE dataset,<br>Chen et al. high-throughput DFT TE dataset,<br>Petretto et al. phonon database,<br>Ricci et al. electronic transport dataset,<br>Toher et al. phonon thermal conductivity dataset,<br>Open Quantum Materials Database (OQMD),<br>Materials Project (MP) |
| **Models** | Gradient Boosting Tree,<br>XGBoost,<br>LightGBM,<br>Gaussian Process,<br>Support Vector Machine,<br>Random Forest,<br>Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>ResNet,<br>Transformer,<br>Attention Mechanism,<br>Variational Autoencoder,<br>Generative Adversarial Network,<br>Diffusion Model,<br>BERT,<br>GPT,<br>Graph Neural Network,<br>Message Passing Neural Network,<br>Graph Convolutional Network,<br>Gaussian Process,<br>Variational Autoencoder,<br>Feedforward Neural Network |
| **Tasks** | Regression,<br>Feature Selection,<br>Clustering,<br>Representation Learning,<br>Data Generation,<br>Synthetic Data Generation,<br>Active Learning,<br>Clustering,<br>Hyperparameter Optimization / Optimization,<br>Clustering |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Self-Supervised Learning,<br>Active Learning,<br>Transfer Learning,<br>Generative Learning,<br>Representation Learning,<br>Feature Selection,<br>Fine-Tuning |
| **Performance Highlights** | MAE_zT: 0.06,<br>MAE_Seebeck_μV_per_K: 49,<br>zT_enhancement_percent: up to 104%,<br>thermal_conductivity_change_percent: ≈14% (reported in specific Pb–Se–Te–S local-order study) |
| **Application Domains** | Thermoelectrics,<br>Energy materials,<br>Materials science and engineering,<br>Computational materials discovery / high-throughput screening,<br>Atomistic simulation (DFT, MD) accelerated by ML,<br>Defect engineering (point defects, dislocations, grain boundaries, interfaces),<br>High-entropy alloys / high-entropy thermoelectrics,<br>Sustainability-aware materials design |

---


### [359. Supervised learning in DNA neural networks](https://doi.org/10.1038/s41586-025-09479-w), Nature *(September 03, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Modified National Institute of Standards and Technology (MNIST) — subsets converted to 100-bit binary patterns |
| **Models** | Feedforward Neural Network |
| **Tasks** | Binary Classification,<br>Image Classification,<br>Clustering |
| **Learning Methods** | Supervised Learning,<br>Batch Learning |
| **Performance Highlights** | activated_memory_classification_correctness: Fluorescence kinetics experiments confirmed correct outputs for all selected test patterns on activated memories (12 tests per class) (Fig. 3),<br>representative_tests_after_training: 72 representative tests classified successfully after 3 distinct training processes (Fig. 6d; simulations and experiments shown),<br>lower_bound_accuracy_estimates_on_full_MNIST_subsets: examples: 56% for threes, 46% for fours, 56% for sixes, 71% for sevens (expected lower bounds for full dataset given 20% diagonal margin); Extended Data Fig.1: 53% for '0' and 83% for '1' as lower bounds,<br>classification_time_scale: Typical testing fluorescence kinetics monitored up to 8 hours; reporting often shown at 4–8 h endpoints,<br>system_size: 100-bit, 2-memory network involved >700 distinct species in a single test tube and >1,200 unique strands across learning and testing,<br>learned_weight_readout_noise: Before training, background signals in memories 1 and 2 were 0.4 ± 0.4% and 0.7 ± 0.3% of total signal; after learning, pixels with 0 values in all 10 training patterns showed low signals (0.5 ± 0.3% for '1' memory, 1.1 ± 0.4% for '0'),<br>learned_signal_enrichment: Other pixels showed up to 13-fold signal increases over background in learned weights (endpoint values),<br>successful_memory_integration: Learned weights consistently stored across different training orders; memory integration robust |
| **Application Domains** | Molecular computation (DNA strand-displacement circuits),<br>Synthetic biology / DNA nanotechnology,<br>Bioengineering (cell-free molecular devices),<br>Diagnostics (molecular classifiers for disease biomarkers, potential future applications),<br>Programmable soft materials and active materials (materials that adapt using DNA circuits) |

---


### [358. Machine learning in X-ray diffraction for materials discovery and characterization](https://doi.org/10.1016/j.matt.2025.102272), Matter *(September 03, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | _None_ |
| **Models** | _None_ |
| **Tasks** | _None_ |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | _None_ |

---


### [357. A generalizable pathology foundation model using a unified knowledge distillation pretraining framework](https://doi.org/10.1038/s41551-025-01488-4), Nature Biomedical Engineering *(September 02, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | GPFM pretraining collection (56 sources),<br>TCGA (multiple cohorts, e.g., TCGA-BRCA, TCGA-LUAD, TCGA-GBMLGG, etc.),<br>CPTAC (e.g., CPTAC-LUAD),<br>PANDA,<br>CAMELYON16 + CAMELYON17 (CAMELYON),<br>CRC-100K (NCT-CRC-HE-100K + CRC-VAL-HE-7K),<br>CRC-MSI,<br>CCRCC-TCGA-HEL,<br>PanCancer-TCGA (PanCancer-TCGA images),<br>PanCancer-TIL,<br>PCAM,<br>WSSS4LUAD,<br>BACH,<br>BreakHis,<br>UniToPatho,<br>PathVQA,<br>WSI-VQA,<br>TCGA WSI-Report,<br>PatchGastricADC22,<br>UBC-OCEAN,<br>GasHisDB,<br>CHA / ESCA dataset (esophageal carcinoma subtyping),<br>IMP-CRS (colon lesion grading),<br>HANCOCK,<br>EBRAINS (Digital Tumour Atlas),<br>Center-1 to Center-5 cohorts |
| **Models** | Vision Transformer,<br>ResNet,<br>BERT,<br>Transformer,<br>Attention Mechanism,<br>Linear Model,<br>Encoder-Decoder |
| **Tasks** | Image Classification,<br>Survival Analysis,<br>Image Retrieval,<br>Question Answering,<br>Text Generation,<br>Binary Classification,<br>Multi-class Classification |
| **Learning Methods** | Self-Supervised Learning,<br>Knowledge Distillation,<br>Pre-training,<br>Fine-Tuning,<br>Multi-Instance Learning,<br>Contrastive Learning,<br>Transfer Learning,<br>Representation Learning |
| **Performance Highlights** | average_rank_WSIs_across_36_tasks: 1.22,<br>AUC_overall: 0.891,<br>balanced_accuracy: 0.752,<br>weighted_F1: 0.736,<br>AUC_internal: 0.938,<br>AUC_external: 0.832,<br>average_rank_survival_15_tasks: 2.1,<br>average_C-index: 0.665,<br>average_rank_ROI_16_tasks: 1.88,<br>average_AUC: 0.946,<br>weighted_F1: 0.865,<br>balanced_accuracy: 0.866,<br>top-1_accuracy: 0.906,<br>top-3_accuracy: 0.993,<br>top-5_accuracy: 0.995,<br>patch_level_rank: second-best (close to CONCH),<br>WSI_level_metrics: best or second-best across 6 out of 7 metrics (detailed per-metric results in Supplementary Table 39),<br>ranking: Phikon best; GPFM second-best across BLEU/METEOR/ROUGE-L metrics on TCGA WSI-Report and PatchGastricADC22,<br>TP53_AUC_LUAD: 0.855,<br>IDH1_AUC_internal: 0.986,<br>IDH1_AUC_external: 0.943,<br>average_performance_across_72_tasks: 0.749 |
| **Application Domains** | Computational Pathology,<br>Digital Histopathology,<br>Cancer Diagnosis and Subtyping,<br>Mutation Prediction from Histology,<br>Survival Prognosis Prediction,<br>Medical Image Retrieval,<br>Pathology Visual Question Answering (clinical decision support),<br>Automated Pathology Report Generation |

---


### [356. PXDesign: Fast, Modular, and Accurate De Novo Design of Protein Binders](https://doi.org/10.1101/2025.08.15.670450), Preprint *(September 02, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Cao data,<br>RFDiffusion wet-lab set,<br>EGFR challenge / EGFR competition dataset,<br>SKEMPI subset (filtered),<br>PDB subset (curated up to May 1, 2021) + AFDB + MGnify distillation,<br>AlphaProteo (published baselines / datasets referenced),<br>RFpeptides / cyclic peptide targets (benchmarks) |
| **Models** | Diffusion Model,<br>Denoising Diffusion Probabilistic Model,<br>Transformer,<br>Multi-Layer Perceptron |
| **Tasks** | Synthetic Data Generation,<br>Binary Classification,<br>Regression,<br>Ranking,<br>Clustering,<br>Data Generation |
| **Learning Methods** | Generative Learning,<br>End-to-End Learning,<br>Backpropagation,<br>Gradient Descent,<br>Ensemble Learning,<br>Pre-training,<br>Fine-Tuning,<br>Hyperparameter Optimization,<br>End-to-End Learning |
| **Performance Highlights** | nanomolar_hit_rates_by_target_Table1: {'PD-L1': '72.7%', 'VEGF-A': '47.1%', 'SC2RBD': '50.0%', 'TrkA': '20.0%', 'TNF-α': '0.0%'},<br>summary: nanomolar binder hit rates of 20–73% across five of six targets (PXDesign overall),<br>in_silico_comparative: PXDesign-h shows strong in silico performance and is competitive with hallucination baselines; slower but superior on specific cyclic peptide targets (e.g., MDM2, MCL1, IL17A, TNF-α).,<br>24h_yield_comparison: PXDesign-d delivers more successful designs within 24h than any hallucination method (PXDesign-h, BindCraft, BoltzDesign1) due to faster generation and higher pass rates (no exact numeric given).,<br>RFDiffusion_re-ranking: Re-ranking the 95 RFdiffusion designs by Protenix ipTM (top10/top15) substantially increases observed success rates (Figure 1d).,<br>AUC_AP_filtering: Protenix-derived scores outperform AF2-IG across most targets in AUC and Average Precision on Cao data (Figure 8); Protenix matches/exceeds AF3 on SKEMPI subset (Figure 1f).,<br>sequence_design_protocol: ProteinMPNN-CA used for sequence design with default settings; for diffusion designs one sequence per backbone; for hallucination designs 8 sequences per structure.,<br>impact_on_success: Design considered successful if at least one sequence meets filter criteria; protocol differences accounted for in comparisons (no single numeric). |
| **Application Domains** | protein–protein binder design (de novo binder design),<br>unconditional protein monomer design (de novo protein backbone generation),<br>cyclic peptide binder design,<br>nucleic acid (RNA/DNA) binder design (case studies; qualitative),<br>small-molecule binder design (case studies; qualitative),<br>post-translationally modified protein design (case studies; qualitative) |

---


### [355. Developing machine learning for heterogeneous catalysis with experimental and computational data](https://doi.org/10.1038/s41570-025-00740-4), Nature Reviews Chemistry *(September 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Open Catalyst (OC20 / OC22 / OCx24),<br>CatalysisHub / IoChem-BD / NOMAD / AFLOW / OQMD / Materials Project / BEAST DB,<br>Gregoire scanning droplet / combinatorial experimental screening datasets,<br>Taniike high-throughput OCM screening,<br>Yildirim methane reforming compilations,<br>Zavyalova OCM literature database,<br>Perovskite / pseudo-quaternary combinatorial OER datasets,<br>Large computational screening datasets (examples reported),<br>IrO2 / IrO3 polymorph DFT dataset (Flores et al.) |
| **Models** | Linear Model,<br>Decision Tree,<br>Random Forest,<br>Gradient Boosting Tree,<br>XGBoost,<br>LightGBM,<br>Support Vector Machine,<br>Gaussian Process,<br>Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>Graph Neural Network,<br>K-means,<br>Principal Component Analysis,<br>Random Forest |
| **Tasks** | Regression,<br>Classification,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Extraction,<br>Clustering,<br>Optimization,<br>Experimental Design,<br>Clustering,<br>Representation Learning |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Active Learning,<br>Transfer Learning,<br>Semi-Supervised Learning,<br>Reinforcement Learning,<br>Unsupervised Learning |
| **Performance Highlights** | acceleration_vs_random: up to 20-fold |
| **Application Domains** | Heterogeneous catalysis,<br>Electrocatalysis (water splitting, OER, HER, ORR),<br>CO and CO2 conversion and reduction,<br>Methane reforming and methane reactivity (oxidative coupling of methane, methanation),<br>Nitrogen reactivity (N2 reduction, NO decomposition, ammonia synthesis),<br>High-throughput experimental catalyst screening (combinatorial materials),<br>Computational high-throughput DFT screening / materials discovery |

---


### [354. Protein evolution as a complex system](https://doi.org/10.1038/s41589-025-01977-2), Nature Chemical Biology *(September 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | vast quantities of protein sequence data (unspecified),<br>synthetic datasets generated by ancestral sequence reconstruction,<br>in silico-evolved variant populations |
| **Models** | Graph Neural Network,<br>Autoencoder,<br>Variational Autoencoder,<br>Transformer,<br>BERT |
| **Tasks** | Language Modeling,<br>Regression,<br>Sequence-to-Sequence,<br>Dimensionality Reduction,<br>Optimization,<br>Decision Making,<br>Data Generation,<br>Clustering,<br>Representation Learning |
| **Learning Methods** | Self-Supervised Learning,<br>Reinforcement Learning,<br>Representation Learning,<br>Manifold Learning,<br>Pre-training,<br>Generative Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | protein evolution,<br>protein engineering and design,<br>molecular evolution,<br>directed evolution,<br>synthetic biology,<br>complex systems modeling,<br>in silico evolutionary simulation,<br>fitness prediction / protein fitness landscapes |

---


### [353. Robot-assisted mapping of chemical reaction hyperspaces and networks](https://doi.org/10.1038/s41586-025-09490-1), Nature *(September 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Full robot-generated spectral dataset (all automated reactions),<br>E1 hyperspace cube,<br>SN1 hyperspace cube (9-butyl-9H-fluoren-9-ol),<br>SN1 hyperspace cube (15a substrate with anomaly example),<br>Ugi-type four-component hyperspace,<br>Hantzsch reaction hyperspace,<br>Prussian blue analogue (PBA) catalyst composition space |
| **Models** | Linear Model,<br>Radial Basis Function Network |
| **Tasks** | Feature Extraction,<br>Anomaly Detection,<br>Regression,<br>Optimization,<br>Clustering |
| **Learning Methods** | Maximum Likelihood Estimation |
| **Performance Highlights** | yield_estimate_uncertainty_example: ±1% absolute (20% yield reported spread to 19–21%),<br>optical_vs_purified_R2: 0.96,<br>optical_repeatability_RSD: 2% (n=54, same crude mixture),<br>workflow_repeatability_RSD: 5% (n=27, entire workflow),<br>SN1_thermodynamic_parameters_estimates: ΔH = -30.7 ± 1.4 kJ mol−1 (SN1); other parameters for E1: values reported in Methods,<br>fit_quality_examples: Kinetic models fitted closely to experimental hyperspace data (Fig. 2e,f; Fig. 4e–h) |
| **Application Domains** | organic chemistry / synthetic reaction discovery,<br>reaction optimization,<br>analytical spectroscopy (UV-Vis spectral analysis),<br>catalysis (Prussian blue analogues for styrene epoxidation),<br>materials chemistry (PBA composition screening),<br>chemical reaction network reconstruction and mechanistic studies |

---


### [352. Electron flow matching for generative reaction mechanism prediction](https://doi.org/10.1038/s41586-025-09426-9), Nature *(September 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | USPTO-full (processed),<br>FlowER mechanistic dataset (authors' curated),<br>Pistachio (subset of patent reactions not assigned a reaction class),<br>RMechDB (supplemental),<br>PMechDB (supplemental) |
| **Models** | Transformer,<br>Graph Neural Network,<br>Radial Basis Function Network,<br>Encoder-Decoder |
| **Tasks** | Sequence-to-Sequence,<br>Structured Prediction,<br>Graph Generation |
| **Learning Methods** | Generative Learning,<br>Denoising Diffusion Probabilistic Model,<br>Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Representation Learning |
| **Performance Highlights** | validity_rate: ≈95% (valid intermediate or product SMILES strings generated in approximately 95% of test reactions),<br>model_size_default_params: 7 million parameters (default FlowER),<br>model_size_large_params: 16 million parameters (FlowER-large),<br>SMILES_validity: 70.2%,<br>heavy_atom_conservation: 39.1%,<br>cumulative_conservation_(heavy_atom+proton+electron): 33.0%,<br>model_size: 12 million parameters,<br>SMILES_validity_G2S: 76.3%,<br>SMILES_validity_G2S+H: 78.8%,<br>heavy_atom_conservation_G2S: 30.7%,<br>heavy_atom_conservation_G2S+H: 27.7%,<br>cumulative_conservation_G2S: 17.2%,<br>cumulative_conservation_G2S+H: 19.0%,<br>model_size: 18 million parameters (G2S),<br>few_shot_fine_tune_500_steps_top1_step_accuracy_FlowER: ≈35%,<br>few_shot_fine_tune_500_steps_top10_step_accuracy_FlowER: ≈40%,<br>G2S_performance_on_500_steps: near-zero (reported as near-zero performance by G2S),<br>fine_tune_32_examples_top1_pathway_accuracy_>=65%_count: 9 out of 12 unseen reaction types,<br>recovered_products_from_Pistachio_22k: 351 products recovered from 22,000 unrecognized reactions |
| **Application Domains** | medicinal chemistry / synthetic planning,<br>materials discovery,<br>combustion chemistry,<br>atmospheric chemistry,<br>electrochemical systems,<br>automated quantum chemical calculations / thermodynamic and kinetic feasibility estimation,<br>reaction design and predictive chemistry |

---


### [351. The Biodiversity Cell Atlas: mapping the tree of life at cellular resolution](https://doi.org/10.1038/s41586-025-09312-4), Nature *(September 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Earth BioGenome Project genomes,<br>Biodiversity Cell Atlas (BCA) single-cell datasets (planned/curated by this paper),<br>Human Cell Atlas,<br>Tabula Muris (single-cell transcriptomics of 20 mouse organs),<br>Fly Cell Atlas,<br>Malaria Cell Atlas,<br>Stony coral cell atlas,<br>GoaT (Genomes on a Tree) metadata,<br>TranscriptFormer preprint / cross-species generative cell atlas (preprint dataset/model) |
| **Models** | Convolutional Neural Network,<br>Transformer,<br>Variational Autoencoder,<br>Graph Neural Network |
| **Tasks** | Regression,<br>Language Modeling,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Extraction,<br>Clustering,<br>Embedding Learning,<br>Generative Learning |
| **Learning Methods** | Supervised Learning,<br>Pre-training,<br>Transfer Learning,<br>Fine-Tuning,<br>Self-Supervised Learning,<br>Generative Learning,<br>Representation Learning,<br>Embedding Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | Comparative genomics and evolutionary biology,<br>Single-cell biology and cell atlas construction,<br>Biotechnology,<br>Biomedicine (disease variant prediction, functional genomics),<br>Environmental science and biomonitoring,<br>Synthetic biology (design of regulatory sequences and circuits),<br>Computational method and software infrastructure (data standards, pipelines) |

---


### [350. Generalizable descriptors for automatic titanium alloys design by learning from texts via large language model](https://doi.org/10.1016/j.actamat.2025.121275), Acta Materialia *(September 01, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Data S1 (corpus of abstracts),<br>Data S2,<br>Data S3,<br>Data S4,<br>Unexplored candidate space (generated search set),<br>GitHub repository data and code |
| **Models** | Transformer,<br>BERT,<br>Linear Model,<br>Random Forest,<br>Gradient Boosting Tree,<br>Support Vector Machine,<br>Gaussian Process,<br>Multi-Layer Perceptron,<br>Genetic Algorithm |
| **Tasks** | Language Modeling,<br>Representation Learning,<br>Regression,<br>Dimensionality Reduction,<br>Clustering,<br>Binary Classification |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Supervised Learning,<br>Transfer Learning,<br>Contrastive Learning,<br>Evolutionary Learning,<br>Mini-Batch Learning,<br>Representation Learning,<br>Batch Learning |
| **Performance Highlights** | pretraining_loss: 0.48,<br>vocabulary_size: 4462 tokens,<br>training_samples: >780,000,<br>improvement_over_conventional_descriptors_tensile_strength_R2: 8.04% (GBR improvement reported),<br>improvement_over_conventional_descriptors_yield_strength_R2: 5.91% (GBR improvement reported),<br>improvement_over_conventional_descriptors_elongation_R2: 8.24% (GBR improvement reported),<br>improvement_over_conventional_descriptors_tensile_strength_R2: 44.2%,<br>improvement_over_conventional_descriptors_yield_strength_R2: 25.3%,<br>improvement_over_conventional_descriptors_elongation_R2: 29.8%,<br>improvement_over_conventional_descriptors_yield_strength_R2: 19.3%,<br>improvement_over_conventional_descriptors_elastic_modulus_R2: 7.25%,<br>evaluation_metrics_used: R2, MAE, RMSE (five-fold cross-validation),<br>evaluation_metrics_used: R2, MAE, RMSE (five-fold cross-validation),<br>evaluation_metrics_used: R2, MAE, RMSE (five-fold cross-validation),<br>relative_performance: Paper reports that their model's embeddings generally outperform SciBERT, MatBERT, MatSciBERT across tested cases (see Table S5 for details),<br>GA_parameters: population=800, generations=450, crossover_rate=80%, mutation_rate=5%,<br>result: Dimensionality reduced to 13 optimized descriptors; fitness (R2) converged by generation 450 for GBR |
| **Application Domains** | Materials science (titanium alloys design),<br>Metallurgy,<br>Aerospace materials (undercarriage, structural applications),<br>Biomedical implants (low-modulus titanium alloys),<br>Materials informatics / text-mining for materials design |

---


### [349. From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery](https://doi.org/10.48550/arXiv.2505.13259), Preprint *(August 30, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | LitQA / PaperQA,<br>ArxivDIGESTables,<br>arXiv2Table,<br>Text-Tuple-Table,<br>TKGT,<br>IdeaBench / AI Idea Bench 2025 / LiveIdeaBench,<br>MOOSE-Chem,<br>BioPlanner benchmark,<br>ARCADE,<br>DS-1000,<br>MLE-Bench / MLE-Bench (MLE-Bench),<br>AIDE,<br>Chain-of-Table,<br>TableBench,<br>ChartQA,<br>CharXiv / ChartX & ChartVLM,<br>AutomaTikZ,<br>Text2Chart31,<br>ClaimCheck,<br>SciReplicate-Bench,<br>LLM-SRBench / LLM-SR,<br>Gravity-Bench-v1,<br>InfiAgent-DABench,<br>BLADE / DiscoveryBench / DiscoveryWorld / ScienceAgentBench / CURIE / EAIRA / ResearchBench |
| **Models** | Transformer,<br>Vision Transformer,<br>Graph Neural Network,<br>Multi-Layer Perceptron |
| **Tasks** | Information Retrieval,<br>Text Summarization,<br>Text Generation,<br>Planning,<br>Question Answering,<br>Regression,<br>AutoML,<br>Image Generation,<br>Survival Analysis,<br>Language Modeling,<br>Hyperparameter Optimization |
| **Learning Methods** | Reinforcement Learning,<br>Fine-Tuning,<br>Zero-Shot Learning,<br>Continual Learning,<br>Online Learning,<br>Multi-Agent Learning,<br>Supervised Learning,<br>Instruction Tuning |
| **Performance Highlights** | _None_ |
| **Application Domains** | General scientific literature review and knowledge synthesis,<br>Artificial intelligence / machine learning research,<br>Biomedicine / biology (protocol planning, hypothesis generation, drug discovery),<br>Chemistry / materials science (hypothesis discovery, experimental planning, robotic chemistry),<br>Physics (function discovery, gravitational physics benchmarks),<br>Data science (code generation, data analysis, tabular reasoning),<br>Cross-domain agentic research / autonomous scientific discovery platforms |

---


### [348. A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers](https://doi.org/10.48550/arXiv.2508.21148), Preprint *(August 28, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Intern-S1 continual pre-training corpus (scientific subset),<br>Galactica pre-training corpus,<br>MedPaLM-2 training/eval datasets (medical-domain datasets / USMLE-style),<br>ProteinChat dataset (protein prompt-answer triplets),<br>LLaMA-Gene instruction corpus (DNA/protein tasks),<br>HuatuoGPT-II corpus,<br>NatureLM corpus (multidisciplinary biomedical + materials),<br>MIMIC-CXR,<br>PMC-OA / PMC-CaseReport / PMC figures datasets,<br>ZINC,<br>ChEMBL,<br>USPTO reaction dataset,<br>Materials Project,<br>MatBench,<br>UniProtKB / Swiss-Prot,<br>PDB (Protein Data Bank),<br>UniRef / UniRef50 / UniRef90 (protein sequence clusters),<br>MD17 / ISO17 (molecular dynamics),<br>Kepler mission data / TESS / ZTF mentioned,<br>LIGO/Virgo GWTC catalogs,<br>ERA5 / WeatherBench / WEATHER-5K,<br>RS5M / GeoLLaVA-8K / GeoPixel / LHRS datasets (remote sensing corpora),<br>MMSci / MaCBench (materials multimodal benchmarks),<br>ScienceQA,<br>MMLU-Pro,<br>ResearchBench,<br>HLE (Humanity’s Last Exam),<br>SFE (Scientists’ First Exam) |
| **Models** | BERT,<br>GPT,<br>Transformer,<br>Vision Transformer,<br>CLIP,<br>U-Net,<br>Attention Mechanism,<br>Self-Attention Network,<br>Multi-Head Attention,<br>Graph Neural Network,<br>Message Passing Neural Network,<br>Convolutional Neural Network |
| **Tasks** | Question Answering,<br>Text Generation,<br>Language Modeling,<br>Image-to-Image Translation,<br>Image Classification,<br>Regression,<br>Classification,<br>Time Series Forecasting,<br>Sequence-to-Sequence,<br>Image Generation,<br>Clustering,<br>Feature Extraction,<br>Anomaly Detection,<br>Symbolic Regression,<br>Molecular Property Prediction (mapped to Regression/Classification),<br>Image Captioning,<br>Information Retrieval,<br>Hyperparameter Optimization / Architecture Search |
| **Learning Methods** | Transfer Learning,<br>Pre-training,<br>Fine-Tuning,<br>Reinforcement Learning,<br>Self-Supervised Learning,<br>Continual Learning,<br>Supervised Learning,<br>Prompt Learning,<br>Few-Shot Learning,<br>Zero-Shot Learning,<br>Contrastive Learning,<br>Test-Time Learning,<br>Representation Learning |
| **Performance Highlights** | accuracy: >85% (MedPaLM-2 on USMLE-style questions, as reported),<br>benchmarks: state-of-the-art on PubMedQA and MedMCQA-dev (Galactica reported at release),<br>sample_efficiency: Matches FNO performance with only 20 samples vs FNO needing 1024 samples,<br>accuracy: 86.2% (AstroSage-LLaMA-3.1 on AstroMLab-1 benchmark, reported),<br>accuracy: ≈98.6% (CSLLM on synthesizability prediction task),<br>metastable_generation_rate: ≈49% materials generated metastable (CrystaLLM reported),<br>HLE_performance: Closed-source models show steep drops on HLE (frontier scientific benchmark); Grok 4 reported 50.7% on HLE in paper,<br>claimed: Intern-S1 'surpasses existing closed-source state-of-the-art models in professional tasks such as molecular synthesis, reaction condition prediction, and crystalline thermodynamic stability prediction' (no numeric values reported in paper),<br>win_or_draw_rate_vs_vicuna: 95% (Xiwu vs Vicuna-13B reported win-or-draw rate),<br>code_generation_performance: Surpasses GPT-4 on certain HEP code generation tasks (qualitative claim),<br>improvement: Test-time training improves chemical exploration but diminishing returns beyond long test-time durations (cited study results qualitative) |
| **Application Domains** | Physics,<br>Chemistry,<br>Materials Science,<br>Life Sciences / Biology / Multi-omics,<br>Healthcare / Medical Science,<br>Astronomy,<br>Earth Science / Remote Sensing / Climate,<br>Agriculture,<br>Neuroscience,<br>Pharmacy / Drug Discovery |

---


### [346. Graph attention networks decode conductive network mechanism and accelerate design of polymer nanocomposites](https://doi.org/10.1038/s41524-025-01773-5), npj Computational Materials *(August 28, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | hPF-MD simulated CNT/homopolymer dataset (CP1-CP8),<br>Additional CNT variants (four other CNTs) (supplementary) |
| **Models** | Graph Neural Network,<br>Graph Convolutional Network,<br>Attention Mechanism,<br>Multi-Head Attention |
| **Tasks** | Regression,<br>Dimensionality Reduction,<br>Clustering,<br>Feature Extraction,<br>Representation Learning |
| **Learning Methods** | Supervised Learning,<br>Incremental Learning,<br>Fine-Tuning,<br>Pre-training,<br>Backpropagation,<br>Gradient Descent,<br>Representation Learning,<br>End-to-End Learning |
| **Performance Highlights** | CP1_RMSE_GAT: 0.00022,<br>CP1_MAE_GAT: 0.00022,<br>CP2_RMSE_GAT: 0.00025,<br>CP2_MAE_GAT: 0.00025,<br>CP3_RMSE_GAT: 0.00016,<br>CP3_MAE_GAT: 0.00016,<br>CP4_RMSE_GAT: 0.00023,<br>CP4_MAE_GAT: 0.00023,<br>CP5_RMSE_GAT: 0.00017,<br>CP5_MAE_GAT: 0.00017,<br>CP6_RMSE_GAT: 0.00012,<br>CP6_MAE_GAT: 0.00012,<br>CP7_RMSE_GAT: 9e-05,<br>CP7_MAE_GAT: 9e-05,<br>CP8_RMSE_GAT: 0.00021,<br>CP8_MAE_GAT: 0.00021,<br>aggregate_observation: prediction errors reported in Table 1 (RMSE and MAE) for GAT are approximately in the 1e-4 range per concentration,<br>CP1_RMSE_GCN: 0.01762,<br>CP1_MAE_GCN: 0.01762,<br>CP2_RMSE_GCN: 0.01872,<br>CP2_MAE_GCN: 0.01872,<br>CP3_RMSE_GCN: 0.01208,<br>CP3_MAE_GCN: 0.01208,<br>CP4_RMSE_GCN: 0.01588,<br>CP4_MAE_GCN: 0.01588,<br>CP5_RMSE_GCN: 0.01586,<br>CP5_MAE_GCN: 0.01586,<br>CP6_RMSE_GCN: 0.01472,<br>CP6_MAE_GCN: 0.01472,<br>CP7_RMSE_GCN: 0.01224,<br>CP7_MAE_GCN: 0.01224,<br>CP8_RMSE_GCN: 0.01211,<br>CP8_MAE_GCN: 0.01211,<br>aggregate_observation: GCN baseline RMSE/MAE are on the order of 1e-2 per Table 1 |
| **Application Domains** | Conductive polymer nanocomposites (CPNs),<br>Materials science,<br>Predictive materials design,<br>Wearable electronics / flexible electronics,<br>Strain sensors and electronic skin,<br>Soft robotics |

---


### [345. Deep generative models design mRNA sequences with enhanced translational capacity and stability](https://doi.org/10.1126/science.adr8470), Science *(August 28, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | GEMORNA-CDS training dataset (natural protein sequences and corresponding CDSs),<br>GEMORNA-UTR pre-training dataset (5' and 3' UTRs),<br>GEMORNA-UTR fine-tuning datasets,<br>Pred-5UTR training dataset (mean ribosome load labels),<br>Pred-3UTR training dataset (stability labels),<br>Bicknell et al. dataset (m1Ψ-modified exogenous mRNAs),<br>Leppek et al. dataset (unmodified exogenous mRNAs),<br>In-house m1Ψ-modified Fluc2P dataset,<br>Experimental benchmarking datasets (in vitro / in vivo assays used for performance evaluation) |
| **Models** | Transformer,<br>Seq2Seq,<br>Encoder-Decoder,<br>Bidirectional LSTM,<br>Conditional Random Field,<br>Recurrent Neural Network,<br>Convolutional Neural Network |
| **Tasks** | Sequence-to-Sequence,<br>Language Modeling,<br>Data Generation,<br>Regression,<br>Synthetic Data Generation |
| **Learning Methods** | Unsupervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Supervised Learning,<br>Zero-Shot Learning |
| **Performance Highlights** | improvement_over_biLSTM-CRF_at_48h_Fluc: up to 20-fold,<br>improvement_over_pGL4.11_at_48h_Fluc: 4.8-fold,<br>5prime_UTRs_similar_or_higher_than_BNT162b2: 5 GEMORNA 5' UTRs exhibited similar or higher Fluc activities compared with BNT162b2 5' UTR,<br>group_level_improvement_over_BNT162b2_UTR_pairs: up to 7-fold increase in Fluc activity compared to the benchmark BNT162b2 UTRs,<br>HEK293T_vs_HepG2_correlation: r^2 = 0.92,<br>UTR_pair_cross_target_correlation: r^2 = 0.096,<br>max_fold_increase_Fluc_vs_Benchmark-FL1_in_vitro: 41-fold,<br>fold_increase_Fluc_vs_Benchmark-FL2_in_vitro: 8.2-fold,<br>fold_increase_Fluc_in_HepG2_vs_Benchmark-FL2: 15.9-fold,<br>antibody_titers_vs_BNT162b2_and_LinearDesign: GEMORNA-derived full-length mRNA induced higher antibody titers than both the BNT162b2 and the LinearDesign mRNAs at multiple time points (no numeric titer provided in text),<br>NanoLuc_improvement_vs_benchmark: GEMORNA-designed mRNAs exhibited higher expression than a strong benchmark across 24-72 hours (no single fold-number provided in main text),<br>EPO_in_vitro: 6 of 7 GEMORNA designs achieved enhanced EPO activities in vitro; selected designs showed longer durability,<br>EPO_in_vivo_best_fold_increase: GMR-EPO-F7 achieved 15-fold increase in expression compared to the benchmark at 24 hours,<br>circRNA_in_vitro_EPO_accumulated_expression_vs_benchmark: 13.8-fold (best GEMORNA design vs Chen et al. benchmark, Fig. 5F),<br>circRNA_in_vivo_EPO_best_fold_increase_at_24h: 121-fold increase vs benchmark (Fig. 5H),<br>circRNA_expression_longevity_ratio_best: 46.5% (144h/24h) vs benchmark 2.5% (Fig. 5G),<br>NanoLuc_circRNA_long_durability: GMR-NL3 maintained high NanoLuc activity over 144 hours, and achieved a 3-fold increase at 72h vs 24h (Fig. 5C-E),<br>training_dataset_size: 166,530 sequences,<br>data_split: 0.7/0.15/0.15 (train/val/test),<br>reported_performance: described as 'high accuracy' in selection of 5' UTRs for fine-tuning (no numeric accuracy provided in text),<br>training_dataset_size: 90,000 sequences,<br>data_split: 0.7/0.15/0.15 (train/val/test),<br>reported_performance: used to identify stable 3' UTRs for fine-tuning (no numeric metric in main text),<br>correlation_with_Bicknell_invitro_half-life: r = 0.92,<br>correlation_with_Bicknell_invivio_expression: r = 0.96,<br>relative_performance: GEMORNA CDSs up to 20-fold improvement over biLSTM-CRF at 48h Fluc |
| **Application Domains** | mRNA therapeutics and vaccines,<br>synthetic biology (RNA design),<br>protein replacement therapy (e.g., EPO),<br>vaccine antigen design (COVID-19 spike protein),<br>non-viral CAR-T cell therapy (CD19 CAR via circRNA),<br>circular RNA (circRNA) therapeutics,<br>gene expression optimization in mammalian cells |

---


### [344. One-shot design of functional protein binders with BindCraft](https://doi.org/10.1038/s41586-025-09429-6), Nature *(August 27, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | PD-1 designs (this work),<br>PD-L1 designs (this work),<br>IFNAR2 designs (this work),<br>CD45 designs (this work),<br>CLDN1 (claudin 1) soluble-analogue designs (this work),<br>BBF-14 designs (de novo beta-barrel target, this work),<br>SAS-6 designs (this work),<br>Der f7 designs (dust mite allergen; this work),<br>Der f21 designs (dust mite allergen; this work),<br>Bet v1 designs (birch allergen; this work),<br>SpCas9 (CRISPR–Cas9) designs (this work),<br>CbAgo (Clostridium butyricum Argonaute) designs (this work),<br>HER2 and PD-L1 targeted AAV transduction screens (this work),<br>PDB target inputs used for design (templates) |
| **Models** | Transformer,<br>Message Passing Neural Network,<br>Diffusion Model,<br>Ensemble (multiple model weights),<br>Rosetta (physics-based scoring) (NOT in provided model list) |
| **Tasks** | Structured Prediction,<br>Data Generation,<br>Binary Classification,<br>Ranking,<br>Regression |
| **Learning Methods** | Backpropagation,<br>Stochastic Gradient Descent,<br>Ensemble Learning,<br>Pre-training,<br>Representation Learning,<br>Gradient Descent |
| **Performance Highlights** | in_silico_initial_pass_rate_range: 16.8%–62.7%,<br>mpnnsol_filtered_pass_rate_range: 0.6%–65.9%,<br>experimental_success_rate_range_per_target: 10%–100%,<br>average_experimental_success_rate: 46.3%,<br>mpnnsol_sequence_design_limit: Only two MPNNsol sequences per AF2 trajectory allowed to pass filters (to promote interface diversity),<br>mpnnsol_contribution: Improved expression/stability in many designs (qualitative; pass rates reflected in filtered designs),<br>generation_time_comparison: BindCraft yields similar success rates in terms of generation time across several targets and binder lengths compared with RFdiffusion,<br>amino_acid_distribution_difference: RFdiffusion-generated designs underrepresent bulky amino acids at binder interface (qualitative),<br>i_pTM_as_binary_predictor: AF2 i_pTM effectively discriminates on-target vs off-target interactions (qualitative/ROC-like behaviour reported); i_pTM does not correlate with interaction affinity,<br>example: i_pTM used to rank designs; top designs showed experimental binding |
| **Application Domains** | Therapeutics (binder therapeutics, allergy neutralization),<br>Biotechnology (AAV retargeting for gene delivery),<br>Structural biology (de novo protein design and structural validation),<br>Molecular biology (modulation of nucleases such as SpCas9 and CbAgo),<br>Immunology (immune checkpoint receptor binders, allergen neutralization) |

---


### [343. Digital Twin for Chemical Science: a case study on water interactions on the Ag(111) surface](https://doi.org/10.1038/s43588-025-00857-y), Nature Computational Science *(August 27, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Ambient-pressure X-ray photoelectron spectroscopy (APXPS) spectra of Ag(111) interacting with H2O (Ag/H2O system) |
| **Models** | Gaussian Process,<br>Hidden Markov Model,<br>Markov Random Field |
| **Tasks** | Regression,<br>Time Series Forecasting,<br>Optimization,<br>Simulation (mapped to 'Time Series Forecasting' and 'Regression' tasks) |
| **Learning Methods** | Supervised Learning,<br>Stochastic Learning,<br>Model-Based Learning |
| **Performance Highlights** | iterations: 1000,<br>relative_error_cutoff: 0.5 (unitless),<br>qualitative_accuracy: Improved with additional spectra; with more input spectra GP performance approaches basin hopping but was slower in convergence in reported tests,<br>qualitative_match: Surface CRN showed slightly better match to experimental spectra than bulk CRN; surface CRN required more time to reach equilibrium,<br>runtime_iterations: Not applicable (simulation-based forward solver); Gillespie stochastic runs used,<br>qualitative: Bulk CRN outputs smooth concentration profiles; surface CRN outputs stochastic concentration profiles showing uncertainty due to Markov chain randomness |
| **Application Domains** | Chemical characterization / spectroscopy (APXPS),<br>Surface science (metal surface — Ag(111)),<br>Catalysis and electrocatalysis,<br>Corrosion,<br>Battery interfaces / electrode–electrolyte interfaces,<br>Autonomous / on-the-fly experimental planning and decision support |

---


### [342. Target-aware 3D molecular generation based on guided equivariant diffusion](https://doi.org/10.1038/s41467-025-63245-0), Nature Communications *(August 25, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | PDBbind (2020),<br>CrossDocked (refined subset) |
| **Models** | Denoising Diffusion Probabilistic Model,<br>Graph Neural Network,<br>Multi-Layer Perceptron |
| **Tasks** | Graph Generation,<br>Synthetic Data Generation,<br>Optimization,<br>Regression,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Representation Learning,<br>Backpropagation,<br>Stochastic Gradient Descent,<br>End-to-End Learning |
| **Performance Highlights** | JS_C-C_2A_PDBbind: 0.1815,<br>JS_all-atom_12A_PDBbind: 0.0486,<br>JS_bonds_mean_PDBbind: 0.363 ± 0.15,<br>JS_angles_mean_PDBbind: 0.211 ± 0.08,<br>JS_dihedrals_mean_PDBbind: 0.361 ± 0.10,<br>JS_bonds_mean_CrossDocked: 0.392 ± 0.18,<br>JS_angles_mean_CrossDocked: 0.198 ± 0.05,<br>JS_dihedrals_mean_CrossDocked: 0.423 ± 0.13,<br>RMSD_median_approx: ~1.0 Å (DiffGui consistently achieves ~1 Å RMSD across scenarios),<br>Vina_Score_mean_PDBbind: -6.700 ± 2.55,<br>Vina_Min_mean_PDBbind: -7.655 ± 2.51,<br>Vina_Dock_mean_PDBbind: -8.448 ± 2.24,<br>QED_mean_PDBbind: 0.631 ± 0.21,<br>SA_mean_PDBbind: 0.678 ± 0.15,<br>LogP_mean_PDBbind: 1.977 ± 3.01,<br>TPSA_mean_PDBbind: 100.49 ± 62.97,<br>Validity_DiffGui-nolab: 0.9427 (higher validity noted),<br>Ablation_effects: Removal of bond diffusion or property guidance leads to deteriorated JS divergences, Vina scores, and QED; removing both results in larger performance drop,<br>LeadOptimization_examples: Generated candidates show Vina/QED/SA comparable or superior to reference ligands in case studies (e.g., PDBid 3l13, 6e23, DHODH experiments).,<br>WetLab_IC50_RSK4: Compound 1 IC50 ≈ 215.0 nM; Compound 2 IC50 ≈ 111.1 nM,<br>WetLab_IC50_DHODH: Compound 3 IC50 improved from 8.02 μM to 4.27 μM; Compound 4 IC50 improved from 32.20 nM to 10.45 nM |
| **Application Domains** | Structure-based drug design (SBDD),<br>De novo drug design,<br>Lead optimization (fragment-based design, scaffold hopping),<br>Protein-ligand 3D molecular generation,<br>Computational medicinal chemistry / virtual screening |

---


### [341. A versatile multimodal learning framework bridging multiscale knowledge for material design](https://doi.org/10.1038/s41524-025-01767-3), npj Computational Materials *(August 25, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Multimodal electrospun nanofibers dataset,<br>Nanoﬁber-reinforced composite dataset |
| **Models** | Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>ResNet,<br>Transformer,<br>Vision Transformer,<br>Diffusion Model,<br>Denoising Diffusion Probabilistic Model |
| **Tasks** | Regression,<br>Information Retrieval,<br>Image Generation,<br>Optimization,<br>Representation Learning,<br>Multi-task Learning |
| **Learning Methods** | Self-Supervised Learning,<br>Contrastive Learning,<br>Pre-training,<br>Fine-Tuning,<br>Transfer Learning,<br>Multi-Task Learning,<br>Representation Learning,<br>Classifier-Free Guidance |
| **Performance Highlights** | R2: MatMCL (conditions) and MatMCL (fusion) reported higher R2 vs conventional models without SGPT (exact numeric R2 values not reported in main text figures),<br>RMSE: MatMCL (conditions) shows significant improvement (lower RMSE) vs conventional models without SGPT (exact numeric RMSE values not reported in main text figures),<br>R2/RMSE: Transformer-style MatMCL evaluated and shows decreasing multimodal contrastive loss during training and competitive test performance; architecture choice has little impact on retrieval and both architectures evaluated for regression (exact numeric metrics not provided in text),<br>Feature_correlations: Orientation seen: 0.968; Orientation unseen: 0.926; Diameter seen: 0.954; Diameter unseen: 0.947; Pore size seen: 0.882; Pore size unseen: 0.604,<br>FID: FID heatmap reported showing much lower FID on diagonal (generated vs real for same condition); exact numeric FID per-condition values not listed in main text,<br>RMSE: Multi-stage learning (MSL) achieves much lower prediction error on composite test set compared to models trained from random initialization; exact numeric RMSE values not provided in main text,<br>Prediction accuracy: Predicted fracture strengths closely match measured values for both nanofibers and composites; prediction errors for all validated samples remained within a narrow range (exact values in Supplementary / figures),<br>Top-k retrieval accuracy: MatMCL achieves significantly higher retrieval accuracy compared to random and a similarity-based baseline; exact top-1/top-3/top-5/top-10 numeric values are shown in Fig. 3 but not explicitly enumerated in main text,<br>KNN on representations (structural feature prediction): Representations of MatMCL (fusion) achieve highest KNN regression performance for predicting structural features; MatMCL (conditions) next; original processing conditions worst (Fig. 2g). |
| **Application Domains** | Materials science,<br>Electrospun nanofiber design and characterization,<br>Nanoﬁber-reinforced composite design,<br>Multimodal materials databases (cross-modal retrieval/generation),<br>Inverse materials design / process optimization |

---


### [340. GraphVelo allows for accurate inference of multimodal velocities and molecular mechanisms for single cells](https://doi.org/10.1038/s41467-025-62784-w), Nature Communications *(August 22, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | dyngen simulated datasets (linear, cyclic, bifurcating),<br>Analytical 3D bifurcation system constrained to 2D manifold,<br>FUCCI cell cycle dataset,<br>Pancreatic endocrinogenesis dataset,<br>Dentate gyrus dataset,<br>Intestinal organoid dataset,<br>Hematopoiesis dataset,<br>Mouse erythroid maturation (mouse gastrulation -> erythroid lineage subset),<br>Human bone marrow development dataset,<br>Mouse coronal hemibrain spatial transcriptomics (binned; bin size 60),<br>HCMV-infected monocyte-derived dendritic cells (moDCs) dataset,<br>SARS-CoV-2 infected Calu-3 cells (Perturb-seq dataset subset),<br>SHARE-seq mouse hair follicle dataset (multi-omics; transcriptome + ATAC),<br>Developing human cortex 10x Multiome multi-omics dataset,<br>A549 sci-fate metabolic-labeling cell cycle dataset |
| **Models** | Variational Autoencoder |
| **Tasks** | Time Series Forecasting,<br>Regression,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Extraction,<br>Causal Inference,<br>Representation Learning,<br>Clustering |
| **Learning Methods** | Unsupervised Learning,<br>Representation Learning,<br>Dimensionality Reduction,<br>Gradient Descent,<br>Representation Learning |
| **Performance Highlights** | cosine_similarity: GraphVelo preserves both direction and magnitude vs cosine kernel (figures show significant improvements),<br>RMSE: GraphVelo shows lower RMSE vs cosine kernel in simulated tests (Fig.2b, f–h),<br>accuracy: GraphVelo shows higher accuracy in sign prediction vs cosine kernel and random predictor (Fig.2f–h),<br>CBC_score: GraphVelo achieved noticeably improved cross-boundary correctness (CBC) score against input velocity and other advanced methods (Supplementary Fig. 2–4),<br>Spearman_pseudotime_vs_embryo_time: ρ = 0.831 (GraphVelo vector field-based pseudotime vs embryo time for erythroid lineage; Fig.3d),<br>Spearman_pseudotime_vs_viral_RNA%: ρ = 0.980 (GraphVelo pseudotime vs % viral RNA in HCMV dataset; Fig.4b); scVelo baseline: ρ = 0.601 (p = 2.79E-143),<br>Jacobian-based_regulatory_maps: Recovered sequential activation (Gata2 -> Gata1 -> Klf1) in erythropoiesis and identified viral factors inhibiting host pathways (HCMV analyses); in silico knockouts indicate UL123 as top viral factor reducing total viral RNA,<br>in_silico_knockout_screen_samples: n = 1454 (virtual perturbation screen in HCMV; Fig.4j) |
| **Application Domains** | single-cell transcriptomics (developmental biology, cell differentiation),<br>single-cell multi-omics (scRNA + scATAC; chromatin dynamics),<br>spatial transcriptomics (mouse brain spatial datasets),<br>host-pathogen interactions / infection dynamics (HCMV, SARS-CoV-2),<br>simulated single-cell data for benchmarking (dyngen, analytic dynamical systems) |

---


### [339. Capturing short-range order in high-entropy alloys with machine learning potentials](https://doi.org/10.1038/s41524-025-01722-2), npj Computational Materials *(August 21, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | TS-0,<br>TS-1,<br>TS-2,<br>TS-3,<br>TS-4,<br>TS-5,<br>TS-f (final training set),<br>Test set (from ref. 23) |
| **Models** | Radial Basis Function Network |
| **Tasks** | Regression,<br>Distribution Estimation,<br>Clustering |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Batch Learning |
| **Performance Highlights** | energy_RMSE_at_1800K_TS-0: 5.3 meV/atom,<br>energy_RMSE_at_1800K_TS-3: 5.6 meV/atom,<br>energy_RMSE_at_2684K_TS-0: 7.6 meV/atom,<br>energy_RMSE_at_2684K_TS-3: 6.6 meV/atom,<br>ε_SRO_relative_error_TS-2_vs_DFT: TS-2 reproduces DFT WC parameters within statistical accuracy (qualitative),<br>ensemble_std_at_levmax_20_TS-2: 3%,<br>ensemble_std_at_levmax_20_TS-0: 29%,<br>ensemble_std_at_levmax_20_TS-1: 13%,<br>pRDF_first_shell_peak_height_agreement_TS-3_vs_DFT: within ±4.9%,<br>pRDF_first_shell_peak_location_discrepancy_Cr-Cr: -0.12 Å,<br>ε_pRDF_relative_error_TS-3_vs_DFT: TS-3 better than TS-0 (specific numeric values shown in Fig. 2c),<br>TS-4_failure_to_reproduce_γ_sf_and_ΔE: TS-4 (fcc-only) is not capable of reproducing γ_sf and ΔE to DFT accuracy (Fig. 3 shows mismatch),<br>TS-5_success_reproduce_SRO_effects: TS-5 (includes hcp) captures correct SRO and its effects on γ_sf and ΔE (qualitative/within DFT statistical accuracy as shown in Fig. 3),<br>melting_temperature_TS-f: 1661 K,<br>experimental_melting_temperature: ~1690 K,<br>note: TS-f melting temperature shows excellent agreement with experiment; improvement over EAM (EAM result not reliably parsed from main text formatting). |
| **Application Domains** | Atomistic simulations / computational materials science,<br>High-entropy alloys (CrCoNi) and metallic alloys,<br>Prediction of materials properties: stacking-fault energy, phase stability, melting temperature,<br>Study of chemical short-range order (SRO) in crystal and liquid phases |

---


### [338. Machine learning-assisted Ru-N bond regulation for ammonia synthesis](https://doi.org/10.1038/s41467-025-63064-3), Nature Communications *(August 21, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | 201 Ru-based binary intermetallics curated from ICSD,<br>High-throughput DFT adsorption dataset (subset used as labels) |
| **Models** | XGBoost,<br>Multi-Layer Perceptron,<br>Gradient Boosting Tree,<br>Support Vector Machine |
| **Tasks** | Regression,<br>Ranking,<br>Feature Selection,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning |
| **Performance Highlights** | R2_EN_train: 0.98,<br>R2_EN_test: 0.92,<br>R2_EN2_train: 0.94,<br>R2_EN2_test: 0.82 |
| **Application Domains** | Ammonia synthesis catalysis,<br>Materials informatics / computational materials discovery,<br>Catalyst screening and design (Ru-based intermetallic compounds),<br>DFT-driven materials modeling combined with ML |

---


### [337. Revealing nanostructures in high-entropy alloys via machine-learning accelerated scalable Monte Carlo simulation](https://doi.org/10.1038/s41524-025-01762-8), npj Computational Materials *(August 20, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | FeCoNiAlTi DFT dataset (this work),<br>MoNbTaW DFT dataset (from ref. 33, used here),<br>FeCoNiAlTi MC simulation configurations (this work),<br>MoNbTaW MC simulation configurations (this work),<br>Simulated APT specimen configurations (this work) |
| **Models** | Graph Neural Network,<br>Multi-Layer Perceptron,<br>Generalized Linear Model,<br>Polynomial Model |
| **Tasks** | Regression,<br>Clustering,<br>Feature Extraction,<br>Hyperparameter Optimization |
| **Learning Methods** | Supervised Learning,<br>Backpropagation,<br>Stochastic Gradient Descent |
| **Performance Highlights** | validation_RMSE: 0.1819 mRy/atom (~2.5 meV/atom),<br>R^2: >0.995,<br>validation_RMSE: 0.1643 mRy/atom,<br>R^2: >0.995 |
| **Application Domains** | Computational materials science,<br>Atomistic simulations of high-entropy alloys,<br>Nanostructure evolution and nanoparticle morphology,<br>Thermodynamic finite-temperature simulations (order-disorder transitions),<br>Data-driven Monte Carlo simulations accelerated by machine-learned energy models,<br>Comparison with experimental characterization (APT, TEM) |

---


### [336. Bayesian learning-assisted catalyst discovery for efficient iridium utilization in electrochemical water splitting](https://doi.org/10.1126/sciadv.adw0894), Science Advances *(August 20, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | DFT screening dataset of 66 binary oxides (OER overpotential),<br>IrTiO2 Bayesian-optimization dataset (ΔGO − ΔGOH),<br>IrTiO2−x oxygen-vacancy dataset (ΔGO − ΔGOH),<br>Formation energy / stability dataset for IrxTi1−xO2 configurations,<br>Materials Project rutile oxide unit cells,<br>Zenodo repository (trained ML models & datasets) |
| **Models** | Gaussian Process,<br>Radial Basis Function Network,<br>Multi-Layer Perceptron |
| **Tasks** | Regression,<br>Regression,<br>Regression,<br>Optimization,<br>Experimental Design,<br>Feature Extraction,<br>Dimensionality Reduction |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Batch Learning,<br>Feature Learning |
| **Performance Highlights** | MAE_ΔG(ΔGO−ΔGOH): 0.06 eV,<br>MAE_formation_energy: 0.004 eV/atom,<br>MAE_ΔG_IrTiO2−x: 0.10 eV,<br>Predicted_mass_activity_enhancement: >40x (predicted at 12.5% Ir surface ratio with oxygen vacancies) |
| **Application Domains** | Electrocatalysis (Oxygen Evolution Reaction),<br>Materials discovery / computational materials science,<br>Catalyst design and synthesis,<br>Hydrogen production / renewable energy (PEM water electrolysis) |

---


### [335. A unified pre-trained deep learning framework for cross-task reaction performance prediction and synthesis planning](https://doi.org/10.1038/s42256-025-01098-4), Nature Machine Intelligence *(August 19, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Pre-training dataset (13 million chemical reactions; 6.8 million high-quality),<br>USPTO-50k,<br>USPTO-full,<br>USPTO-480k,<br>USPTO-STEREO,<br>Buchwald–Hartwig reaction dataset (dataset 1),<br>Suzuki–Miyaura reaction dataset (dataset 2),<br>Radical C–H functionalization dataset (dataset 3),<br>Asymmetric thiol addition dataset (dataset 4),<br>External validation dataset 1 (Ni-catalysed C–O coupling / NiCOlit),<br>External validation dataset 2 (asymmetric hydrogenation of olefins),<br>External validation dataset 3 (pallada-electrocatalysed C–H activation),<br>Related datasets referenced in external validation splitting |
| **Models** | Graph Neural Network,<br>Transformer,<br>Graph Convolutional Network,<br>Multi-Layer Perceptron,<br>Seq2Seq,<br>Random Forest,<br>BERT,<br>Graph Neural Network,<br>Transformer,<br>Graph Attention Network |
| **Tasks** | Regression,<br>Classification,<br>Sequence-to-Sequence,<br>Clustering,<br>Representation Learning |
| **Learning Methods** | Pre-training,<br>Contrastive Learning,<br>Fine-Tuning,<br>Supervised Learning,<br>Representation Learning,<br>Contrastive Learning |
| **Performance Highlights** | dataset1_R2: 0.971,<br>dataset1_m.a.e._yield_%: 2.98,<br>dataset2_R2: 0.876,<br>dataset2_m.a.e._yield_%: 6.30,<br>dataset3_R2: 0.992,<br>dataset3_m.a.e._∆G_kcal/mol: 0.266,<br>dataset4_R2: 0.915,<br>dataset4_m.a.e._∆∆G_kcal/mol: 0.134,<br>component_combination_test_m.a.e._yield_%: 10.12,<br>external_dataset1_m.a.e._yield_%: 21.76,<br>external_dataset1_R2: 0.309,<br>external_dataset1_binary_precision_high-yield: 0.793,<br>external_dataset1_binary_accuracy_high-yield: 0.732,<br>external_dataset2_R2: 0.832,<br>external_dataset2_m.a.e._∆∆G_kcal/mol: 0.371,<br>external_dataset3_R2: 0.924,<br>external_dataset3_m.a.e._∆∆G_kcal/mol: 0.211,<br>USPTO-50k_top-1_accuracy_retrosynthesis_%: 51.0,<br>USPTO-50k_top-3_accuracy_retrosynthesis_%: 69.0,<br>USPTO-50k_top-5_accuracy_retrosynthesis_%: 74.2,<br>USPTO-50k_top-10_accuracy_retrosynthesis_%: 79.2,<br>USPTO-full_top-1_accuracy_retrosynthesis_%: 47.4,<br>USPTO-full_top-3_accuracy_retrosynthesis_%: 63.0,<br>USPTO-full_top-5_accuracy_retrosynthesis_%: 67.4,<br>USPTO-full_top-10_accuracy_retrosynthesis_%: 71.6,<br>USPTO-480k_top-1_accuracy_forward_%: 90.6,<br>USPTO-480k_top-3_accuracy_forward_%: 94.3,<br>USPTO-480k_top-5_accuracy_forward_%: 94.9,<br>USPTO-480k_top-10_accuracy_forward_%: 95.5,<br>USPTO-STEREO_top-1_accuracy_forward_%: 78.2,<br>USPTO-STEREO_top-3_accuracy_forward_%: 85.1,<br>USPTO-STEREO_top-5_accuracy_forward_%: 86.5,<br>USPTO-STEREO_top-10_accuracy_forward_%: 87.8 |
| **Application Domains** | Organic synthesis,<br>Chemical reaction prediction (reactivity, yield),<br>Selectivity prediction (regioselectivity, enantioselectivity),<br>Computer-aided synthesis planning (retrosynthesis and forward synthesis),<br>Computational chemistry / catalysis,<br>Data-driven reaction discovery and reaction space exploration |

---


### [334. Boosting the predictive power of protein representations with a corpus of text annotations](https://doi.org/10.1038/s42256-025-01088-6), Nature Machine Intelligence *(August 18, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Swiss-Prot (UniProtKB Swiss-Prot subset, 2023-02),<br>Temporal test set (Swiss-Prot entries added 2023-02 to 2024-01),<br>Binary localization,<br>Subcellular localization,<br>Fold,<br>DAVIS,<br>BindingDB,<br>UniRef50 / BFD100 / UniParc (pretraining corpora referenced) |
| **Models** | Transformer,<br>Encoder-Decoder,<br>Seq2Seq,<br>Attention Mechanism,<br>Cross-Attention,<br>Multi-Head Attention,<br>Self-Attention Network,<br>BERT,<br>T5,<br>GPT,<br>Linear Model |
| **Tasks** | Representation Learning,<br>Multi-class Classification,<br>Multi-label Classification,<br>Binary Classification,<br>Sequence Labeling,<br>Regression,<br>Information Retrieval,<br>Few-Shot Learning,<br>Language Modeling |
| **Learning Methods** | Fine-Tuning,<br>Pre-training,<br>Self-Supervised Learning,<br>Language Modeling,<br>Few-Shot Learning,<br>Contrastive Learning,<br>Supervised Learning,<br>Transfer Learning |
| **Performance Highlights** | average_improvement_over_base_family_%: 10.89,<br>average_improvement_over_base_name_%: 12.53,<br>average_improvement_over_base_domain_%: 10.55,<br>average_improvement_over_base_binding_site_%: 8.49,<br>average_improvement_over_base_active_site_%: 9.84,<br>improvement_family_%: 15.10,<br>improvement_name_%: 13.56,<br>improvement_domain_%: 17.31,<br>improvement_binding_site_%: 13.55,<br>improvement_active_site_%: 12.41,<br>improvement_over_base_ESM2-650M_%: 19.39,<br>improvement_over_base_ESM1b_%: 7.47,<br>improvement_over_base_ProtT5_%: 11.89,<br>average_improvement_GO-CC_%: 7.08,<br>average_improvement_GO-MF_%: 12.12,<br>average_improvement_GO-BP_%: 5.84,<br>Pearson_correlation_improvement: positive (exact values in Supplementary Table B; reported qualitatively as improved across models),<br>tasks_outperforming_BLAST_count: 6_of_9,<br>tasks_with_comparable_performance: 3_of_9,<br>one_shot_accuracy_PAIR_embeddings_%: 87.1 ± 0.8,<br>one_shot_accuracy_baselines_range_%: 67–77,<br>low_resource_one_shot_average_accuracy_PAIR_%: ≈85 (P < 0.02 vs baseline) |
| **Application Domains** | Protein function prediction / protein annotation,<br>Protein representation learning (bioinformatics),<br>Enzyme function classification (EC prediction),<br>Subcellular localization prediction,<br>Protein fold / remote homology detection,<br>Drug–target interaction (binding affinity prediction),<br>Retrieval of annotated proteins (bioinformatics sequence search and retrieval) |

---


### [333. Steering towards safe self-driving laboratories](https://doi.org/10.1038/s41570-025-00747-x), Nature Reviews Chemistry *(August 18, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Syn-TODD,<br>Vector-LabPics,<br>TransProteus CGI dataset,<br>Solution-based inorganic materials synthesis procedures dataset,<br>ChemDataExtractor-generated datasets (examples: perovskite and dye-sensitized solar-cell device databases),<br>HeinSight / HeinSight2.0 datasets (implied) |
| **Models** | Convolutional Neural Network,<br>Transformer,<br>GPT,<br>Diffusion Model,<br>Multi-Layer Perceptron |
| **Tasks** | Instance Segmentation,<br>Depth Estimation,<br>Object Localization,<br>Image Classification,<br>Anomaly Detection,<br>Language Modeling,<br>Text Generation,<br>Named Entity Recognition,<br>Experimental Design,<br>Optimization,<br>Sequence Labeling |
| **Learning Methods** | Active Learning,<br>Reinforcement Learning,<br>Imitation Learning,<br>End-to-End Learning,<br>Fine-Tuning,<br>Pre-training,<br>Multi-Agent Learning,<br>Supervised Learning |
| **Performance Highlights** | success_rate: 71% |
| **Application Domains** | chemistry (autonomous chemical experimentation, synthesis, reaction optimization),<br>materials science (materials discovery, thin-film materials, solid-state synthesis),<br>robotics (laboratory robotic manipulation, mobile robots, motion planning),<br>computer vision (laboratory perception, transparent object detection, liquid monitoring),<br>natural language processing (protocol extraction, named entity recognition, LLM-based protocol conversion),<br>autonomous vehicles (comparison and safety lessons drawn from AV domain),<br>biological SDLs and space-based SDLs (discussed as cross-domain applicability) |

---


### [332. An automated framework for exploring and learning potential-energy surfaces](https://doi.org/10.1038/s41467-025-62510-6), Nature Communications *(August 18, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | GAP-RSS dataset (silicon),<br>GAP-RSS dataset (TiO2, Ti–O binary system),<br>GAP-RSS dataset (SiO2 @ PBE and @SCAN),<br>GAP-RSS dataset (liquid water / ice polymorphs) @ revPBE-D3(zero),<br>GAP-RSS dataset (phase-change materials: Ge1Sb2Te4 and In3Sb1Te2),<br>Materials Project relaxation trajectories (referenced as common dataset for foundational MLIPs) |
| **Models** | Gaussian Process,<br>Graph Neural Network,<br>Message Passing Neural Network,<br>Feedforward Neural Network |
| **Tasks** | Regression,<br>Data Generation,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Pre-training,<br>Fine-Tuning,<br>Transfer Learning,<br>Incremental Learning |
| **Performance Highlights** | target_accuracy: 0.01 eV per atom (10 meV/atom),<br>silicon_diamond_beta-tin: ≈0.01 eV/atom achieved with ≈500 DFT single-point evaluations,<br>silicon_oS24: ≈0.01 eV/atom achieved within 'a few thousand' DFT single-point evaluations,<br>RMSE_meV_per_atom_TiO2_Anatase (GAP-RSS TiO2-only): 0.1,<br>RMSE_meV_per_atom_TiO2_Anatase (GAP-RSS Full Ti–O system): 0.7,<br>RMSE_meV_per_atom_TiO2_Baddeleyite (TiO2-only): 1.1,<br>RMSE_meV_per_atom_TiO2_Baddeleyite (Full Ti–O): 28,<br>RMSE_meV_per_atom_TiO2_Brookite: 10 (TiO2-only) ; 8.2 (Full Ti–O),<br>RMSE_meV_per_atom_TiO2_Columbite: 1.0 ; 0.9,<br>RMSE_meV_per_atom_TiO2_Rutile: 0.2 ; 1.8,<br>RMSE_meV_per_atom_TiO2-B: 24 ; 20,<br>RMSE_meV_per_atom_Ti3O5 (TiO2-only trained model): 105 ; Full Ti–O: 19,<br>notable_errors: Errors >1 eV/atom for some off-stoichiometry attempts when model trained only on TiO2 (reported as >1 eV/atom and not tabulated numerically).,<br>ΔE_meV_per_atom_coesite (PBE DFT): 30 (DFT), 31 (GAP-RSS),<br>ΔE_meV_per_atom_stishovite (PBE DFT): 186 (DFT), 185 (GAP-RSS),<br>ΔE_meV_per_atom_α-cristobalite (PBE DFT): −7.9 (DFT) ; GAP-RSS −7.5 (PBE) — note PBE erroneously predicts cristobalite more stable than α-quartz,<br>ΔE_meV_per_atom_moganite (PBE DFT): −0.4 (DFT) ; GAP-RSS −3.5 (PBE) — PBE inaccuracies noted,<br>GAP@SCAN vs GAP@PBE: GAP@SCAN yields qualitatively correct sign for α-quartz vs α-cristobalite (ΔE>0), whereas GAP@PBE gives ΔE<0 (incorrect ordering).,<br>liquid_water_hydrogen_bond_number_NequIP: ≈3.5 (average predicted by NequIP fit to GAP-RSS dataset),<br>experimental_hydrogen_bond_range_reference: 3.48 to 3.84 (literature range cited),<br>ice_structures_energy_prediction: NequIP shows 'much improved predictive accuracy' versus GAP when predicting energies of 54 ice structures (qualitative improvement; numeric errors not tabulated in main text),<br>Ge1Sb2Te4_atomic_environments_reduction: 46% fewer atomic environments used in GAP-RSS-derived model compared to hand-crafted training dataset of ref.26 (which had 49,056 environments) — implies ≈26,500 atomic environments (approximate),<br>crystallisation_simulation_time_GAP: crystallisation simulation essentially completed after 350 ps (GAP-driven MD),<br>qualitative_performance: RDFs and ring statistics show encouraging agreement with AIMD references (Fig.5c–f), but exact numeric RMSEs not provided in main text |
| **Application Domains** | Computational materials science,<br>Inorganic solids (silicon, TiO2, Ti–O binaries, SiO2 polymorphs),<br>Condensed-phase molecular systems (liquid water, ice polymorphs),<br>Phase-change memory materials (Ge1Sb2Te4, In3Sb1Te2),<br>High-throughput ML-driven materials exploration / potential-energy surface exploration |

---


### [330. Accelerated design of gold nanoparticles with enhanced plasmonic performance](https://doi.org/10.1126/sciadv.adx2299), Science Advances *(August 15, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Electrodynamics simulation results (BEM) for AuBP parameter sweeps,<br>Experimental characterization dataset of gold bipyramids (AuBPs) |
| **Models** | Gaussian Process,<br>Gaussian Process |
| **Tasks** | Optimization,<br>Hyperparameter Optimization,<br>Experimental Design |
| **Learning Methods** | Bayesian Optimization,<br>Multi-Objective Learning,<br>Batch / Quasi-Random Sampling (initialization technique) |
| **Performance Highlights** | TMax_K_at_λ_exc_1050nm: 679,<br>Optimal_dimensions_length_nm: 137,<br>Optimal_dimensions_width_nm: 27,<br>Absorption_to_extinction_ratio: ≈0.80,<br>EFSERS_at_λ_exc_1050nm: 9.3e7,<br>EFSERS_local_maxima_950nm_values: ∼8.4e7 and ∼7.9e7,<br>Optimal_dimensions_length_nm: 140,<br>Optimal_dimensions_width_nm: 29,<br>Optimal_analyte_distance_nm: 2,<br>PLEF_max_at_λems_950nm: 5965,<br>Quantum_yield_enhancement_max_at_λexc_850nm: 0.8,<br>Optimal_dimensions_length_nm: 140,<br>Optimal_dimensions_width_nm: 35,<br>Optimal_emitter_distance_nm: 2,<br>Rabi_splitting_range_meV: 150-200,<br>Example_ΩR_meV_at_λems_650nm: 195,<br>Optimal_dimensions_length_nm: 66,<br>Optimal_dimensions_width_nm: 31,<br>Optimal_excitonic_layer_thickness_nm: 5,<br>ΔE94_max: ≈4.79,<br>Optimal_dimensions_length_nm: 58,<br>Optimal_dimensions_width_nm: 40,<br>Optimal_interparticle_gap_nm: 2,<br>ΔH_max_percent: ≈86,<br>Pareto_optimization_example_objectives: maximize PLEF while minimizing δT (λ = 950 nm),<br>Human_in_loop_steps_equivalent: BO pipeline accelerates optimization ~600× relative to canonical human-in-the-loop OFAT (estimated OFAT ~31,000 steps to search 3D parameter space) |
| **Application Domains** | Materials Science,<br>Nanophotonics,<br>Plasmonics,<br>Sensing / Colorimetric Sensing,<br>Surface-Enhanced Raman Spectroscopy (SERS) / Molecular Sensing,<br>Biomedicine (photothermal therapy, bioimaging),<br>Optoelectronics / Quantum technologies (strong plexcitonic coupling) |

---


### [329. Chem3DLLM: 3D Multimodal Large Language Models for Chemistry](https://doi.org/10.48550/arXiv.2508.10696), Preprint *(August 14, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | QM9,<br>Cross-Docked (CrossDocked) dataset |
| **Models** | Transformer,<br>Multi-Layer Perceptron,<br>Graph Neural Network,<br>Normalizing Flow,<br>Diffusion Model,<br>Convolutional Neural Network,<br>Transformer |
| **Tasks** | Molecular Conformation Generation,<br>Structure-Based Drug Design |
| **Learning Methods** | Supervised Learning,<br>Fine-Tuning,<br>Reinforcement Learning,<br>Policy Gradient,<br>Pre-training,<br>Multi-Task Learning,<br>Representation Learning |
| **Performance Highlights** | Atom Stability (%): 99.45,<br>Mol Stability (%): 95.00,<br>Valid (%): 100.00,<br>Unique (%): 100.00,<br>Vina Score Avg. (lower better): -7.03,<br>Vina Med. (median): -7.15,<br>Vina Min. (best min after optimization, joint training): -12.30,<br>Vina Score Avg. (joint multi-task Chem3DLLM†): -7.21,<br>Vina Score Avg. (w/o RLSF / SFT only): -7.03,<br>Vina Min. (w/o RLSF / SFT only): -12.20,<br>Vina Score Avg. (w/o RCMT): -1.82,<br>Vina Min. (w/o RCMT): -4.70,<br>Atom Stability (%): 85.0,<br>Mol Stability (%): 4.9,<br>Valid (%): 40.2,<br>Unique (%): 39.4,<br>Atom Stability (%): 95.7,<br>Mol Stability (%): 68.1,<br>Valid (%): 85.5,<br>Unique (%): 80.3,<br>GDM Atom Stability (%): 97.0,<br>GDM Mol Stability (%): 63.2,<br>GDM-AUG Atom Stability (%): 97.6,<br>GDM-AUG Mol Stability (%): 71.6,<br>EDM Atom Stability (%): 98.7,<br>EDM Mol Stability (%): 82.0,<br>EDM-Bridge Atom Stability (%): 98.8,<br>EDM-Bridge Mol Stability (%): 84.6,<br>GeoLDM Atom Stability (%): 98.9,<br>GeoLDM Mol Stability (%): 89.4,<br>Atom Stability (%): 98.25,<br>Mol Stability (%): 86.87,<br>Valid (%): 100.0,<br>Unique (%): 100.0,<br>Reference Vina Avg.: -6.36,<br>Reference Vina Med.: -6.46,<br>AR Vina Avg.: -5.75,<br>AR Vina Med.: -5.64,<br>Pocket2Mol Vina Avg.: -5.14,<br>Pocket2Mol Vina Med.: -4.70,<br>FLAG Vina Avg.: 16.48,<br>FLAG Vina Med.: 4.53,<br>TargetDiff Vina Avg.: -5.47,<br>TargetDiff Vina Med.: -6.30,<br>Decomp-R Vina Avg.: -5.19,<br>Decomp-R Vina Med.: -5.27,<br>Decomp-O Vina Avg.: -5.67,<br>Decomp-O Vina Med.: -6.04,<br>MolCRAFT Vina Avg.: -6.59,<br>MolCRAFT Vina Med.: -7.04 |
| **Application Domains** | Computational chemistry,<br>Drug discovery / Structure-based drug design (SBDD),<br>Molecular modeling and conformation generation,<br>Materials science (3D molecular structure modeling),<br>Protein-ligand interaction modeling |

---


### [327. SAGERank: inductive learning of protein–protein interaction from antibody–antigen recognition](https://doi.org/10.1039/D5SC03707G), Chemical Science *(August 12, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Antibody–Antigen docking dataset (authors' training set),<br>Cognate antibody–antigen pairing dataset (shuffled pairings),<br>TCR–pMHC dataset (STCRDab),<br>Small protein–protein docking decoy set (authors),<br>Expanded protein–protein docking test set (authors),<br>DC dataset (biological vs crystal interfaces),<br>Cancer target epitope benchmark (IEDB-annotated),<br>Molecular glue ternary complex experiments (FKBP12–rapamycin–FRAP),<br>Nanobody–protein recognition test set (DeepConformer / AlphaFold3 comparison),<br>Reference / prior antibody–antigen benchmarks (cited) |
| **Models** | Graph Neural Network,<br>Graph Convolutional Network |
| **Tasks** | Ranking,<br>Binary Classification,<br>Structured Prediction |
| **Learning Methods** | Supervised Learning,<br>End-to-End Learning,<br>Inductive Learning,<br>Representation Learning |
| **Performance Highlights** | training_set_size_complexes: 287 complexes,<br>total_decoys: 455,420,<br>comparison: outperforms ZRANK, PISA, FoldX and Rosetta on Ab–Ag docking decoy set (see Fig. 2B),<br>AUC_test_set: 0.82,<br>average_score_positive: 0.57,<br>average_score_negative: 0.20,<br>F1_max_at_threshold_0.3: 0.74,<br>confusion_matrix_test_counts: negatives: 9813/11307 correctly classified; positives: 3685/4642 correctly classified,<br>ROCAUC_residue_level_GCa: 0.6467,<br>PRAUC_residue_level_GCa: 0.6739,<br>small_PPI_set_performance: SAGERank competitive with or slightly outperforming PISA on some aspects (see Fig. 2B),<br>expanded_PPI_test_set: on 80 complexes (62,220 structures) SAGERank significantly surpassed success rates of three other scoring methods and was on par with PISA,<br>binding_site_prediction_proteins: 8 out of 10 protein cases correct (80% accuracy),<br>epitope_prediction_antigens: 3 out of 5 antigen cases correct (60% accuracy),<br>DC_dataset_accuracy_SAGERank: 80%,<br>DC_dataset_accuracy_PISA: 79%,<br>DC_dataset_accuracy_PRODIGY: 74%,<br>DC_dataset_accuracy_Deep-Rank: 86%,<br>molecular_glue_positive_total_native_in_top10_SAGERank: 30 native ternary complexes identified in top-10 selections (across positive conformations aggregated as 'All' in Table 4),<br>molecular_glue_positive_total_native_in_top10_Pisa: 10,<br>molecular_glue_negative_total_native_in_top10_SAGERank: 2,<br>molecular_glue_negative_total_native_in_top10_Pisa: 6,<br>mean_min_iRMSD_sagerank: 6.62 Å,<br>mean_min_iRMSD_af3_score: 6.88 Å,<br>median_min_iRMSD_sagerank: 5.75 Å,<br>median_min_iRMSD_af3_score: 5.32 Å,<br>near_native_rate_iRMSD<=2.0_sagerank: 25.6%,<br>near_native_rate_iRMSD<=2.0_af3_score: 27.9%,<br>correlation_between_methods: strong positive correlation (r > 0.7) between iRMSD values from both methods |
| **Application Domains** | antibody–antigen recognition and antibody design,<br>structural immunology (epitope prediction, TCR–pMHC specificity),<br>protein–protein interaction prediction and docking,<br>molecular glue / ternary complex screening (protein + small molecule),<br>cancer antigen epitope identification,<br>general computational structural biology / bioinformatics |

---


### [326. Probing the limitations of multimodal language models for chemistry and materials research](https://doi.org/10.1038/s43588-025-00836-3), Nature Computational Science *(August 11, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | MaCBench (v1.0.0) |
| **Models** | Transformer,<br>Vision Transformer,<br>GPT,<br>BERT |
| **Tasks** | Classification,<br>Multi-class Classification,<br>Regression,<br>Image Classification,<br>Feature Extraction,<br>Sequence-to-Sequence,<br>Text Generation,<br>Binary Classification |
| **Learning Methods** | Prompt Learning,<br>Fine-Tuning,<br>In-Context Learning,<br>Pre-training |
| **Performance Highlights** | equipment_identification_accuracy: 0.77,<br>table_composition_extraction_accuracy: 0.53,<br>hand_drawn_to_SMILES_accuracy: 0.8,<br>isomer_relationship_naming_accuracy: 0.24,<br>stereochemistry_assignment_accuracy: 0.24,<br>baseline_accuracy: 0.22,<br>crystal_system_assignment_accuracy: 0.55,<br>space_group_assignment_accuracy: 0.45,<br>atomic_species_counting_accuracy: 0.85,<br>capacity_values_interpretation_accuracy: 0.59,<br>Henry_constants_comparison_accuracy: 0.83,<br>XRD_amorphous_vs_crystalline_accuracy: 0.69,<br>AFM_interpretation_accuracy: 0.24,<br>MS_NMR_interpretation_accuracy: 0.35,<br>XRD_highest_peak_identification_accuracy: 0.74,<br>XRD_relative_intensity_ranking_accuracy: 0.28,<br>performance_dependency_on_internet_presence: positive_correlation (visualized in Fig. 5) |
| **Application Domains** | chemistry (organic chemistry, spectroscopy, NMR, mass spectrometry),<br>materials science (crystallography, MOF isotherms, electronic structure, AFM),<br>laboratory experiment understanding and safety assessment,<br>in silico experiments and materials characterization,<br>scientific literature information extraction and data curation |

---


### [325. Unconditional latent diffusion models memorize patient imaging data](https://doi.org/10.1038/s41551-025-01468-8), Nature Biomedical Engineering *(August 11, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | PCCTA (in-house photon-counting coronary CT angiography sub-volumes),<br>MRNet,<br>fastMRI,<br>X-ray (ChestX-ray8 subset) |
| **Models** | Denoising Diffusion Probabilistic Model,<br>Diffusion Model,<br>Variational Autoencoder,<br>Generative Adversarial Network,<br>Transformer,<br>Autoencoder |
| **Tasks** | Image Generation,<br>Synthetic Data Generation,<br>Novelty Detection,<br>Data Augmentation |
| **Learning Methods** | Self-Supervised Learning,<br>Contrastive Learning,<br>Adversarial Training,<br>Variational Inference,<br>Generative Learning,<br>Representation Learning |
| **Performance Highlights** | aggregate_memorization_pct: approx. 37.2% (abstract reported overall across datasets),<br>aggregate_synthetic_copies_pct: approx. 68.7% (abstract reported overall across datasets),<br>PCCTA_Nmem_pct: 43.8%,<br>PCCTA_Ncopies_pct: 91.7%,<br>MRNet_Nmem_pct: 40.2%,<br>MRNet_Ncopies_pct: 76.1%,<br>fastMRI_Nmem_pct: 24.8%,<br>fastMRI_Ncopies_pct: 37.3%,<br>PCCTA_Nmem_pct: 40.5%,<br>PCCTA_Ncopies_pct: 83.1%,<br>MRNet_Nmem_pct: 48.2%,<br>MRNet_Ncopies_pct: 87.4%,<br>fastMRI_Nmem_pct: 30.8%,<br>fastMRI_Ncopies_pct: 51.0%,<br>Xray_Nmem_pct: 32.6% (MONAI-2D),<br>Xray_Ncopies_pct: 54.5% (MONAI-2D),<br>VQVAE-Trans_PCCTA_Nmem_pct: 49.6%,<br>VQVAE-Trans_MRNet_Nmem_pct: 58.3%,<br>VQVAE-Trans_fastMRI_Nmem_pct: 40.2%,<br>VQVAE-Trans_PCCTA_Ncopies_pct: 66.1%,<br>VQVAE-Trans_MRNet_Ncopies_pct: 83.3%,<br>VQVAE-Trans_fastMRI_Ncopies_pct: 57.5%,<br>CCE-GAN_overall: Detected some copies but generally unable to synthesize realistic samples in 3D (many detected copies shared little global info),<br>proj-GAN_2D: Synthesized reasonable quality images in 2D but contained no copies (only very small number of false positives),<br>PCCTA_MedDiff_sensitivity: 97.6%,<br>PCCTA_MedDiff_specificity: 93.1%,<br>PCCTA_MONAI_sensitivity: 88.4%,<br>PCCTA_MONAI_specificity: 94.7%,<br>MRNet_MedDiff_sensitivity: 95.4%,<br>MRNet_MedDiff_specificity: 90.6%,<br>MRNet_MONAI_sensitivity: 100%,<br>MRNet_MONAI_specificity: 85.5%,<br>fastMRI_MONAI_sensitivity: 80.6%,<br>fastMRI_MONAI_specificity: 94.3%,<br>Xray_MONAI-2D_sensitivity: 83.3%,<br>Xray_MONAI-2D_specificity: 94.2%,<br>PCCTA_MedDiffAug_Nmem_pct: 40.1%,<br>PCCTA_MONAIAug_Nmem_pct: 36.0%,<br>PCCTA_MedDiffAug_Ncopies_pct: 72.7%,<br>PCCTA_MONAIAug_Ncopies_pct: 76.3%,<br>MRNet_MedDiffAug_Nmem_pct: 27.7%,<br>MRNet_MONAIAug_Nmem_pct: 27.1%,<br>MRNet_MedDiffAug_Ncopies_pct: 36.0%,<br>MRNet_MONAIAug_Ncopies_pct: 61.5%,<br>fastMRI_MedDiffAug_Nmem_pct: 8.7%,<br>fastMRI_MONAIAug_Nmem_pct: 6.3%,<br>fastMRI_MedDiffAug_Ncopies_pct: 9.3%,<br>fastMRI_MONAIAug_Ncopies_pct: 6.4%,<br>Xray_MONAIAug_Nmem_pct: 5.6%,<br>Xray_MONAIAug_Ncopies_pct: 7.3%,<br>architecture_params_MONAI_2D: 25m (small), 171m (medium), 270m (large),<br>architecture_params_MONAI_3D: 68m (small), 191m (medium), 442m (large),<br>general_observation: small architecture had lower memorization across datasets (except PCCTA); medium and large had higher but similar memorization |
| **Application Domains** | Medical imaging (MRI, CT, chest X-ray),<br>Synthetic data generation / open-data sharing in healthcare,<br>Patient privacy and re-identification risk assessment,<br>Data augmentation and dataset expansion for downstream AI tasks,<br>Evaluation of generative model memorization in 2D and 3D medical images |

---


### [324. Observation of dendrite formation at Li metal-electrolyte interface by a machine-learning enhanced constant potential framework](https://doi.org/10.1038/s41467-025-62824-5), Nature Communications *(August 11, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | superlattice model (used for cell relaxation),<br>Li/[EC + LiPF6] interfacial model (initial training dataset),<br>single-interface model (used in configurational sampling),<br>double-interface model (pair of counter electrodes) (used in configurational sampling and production MD),<br>training and testing dataset (DFT-labeled; full dataset archive) |
| **Models** | Multi-Layer Perceptron,<br>Feedforward Neural Network |
| **Tasks** | Regression,<br>Data Generation |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Stochastic Gradient Descent,<br>Representation Learning |
| **Performance Highlights** | full_DP_energy_RMSE_test: 3.04e-3 eV/atom,<br>full_DP_force_RMSE_test: 1.54e-1 eV/Å,<br>DP-QEq_ConstQ_energy_RMSE_test: 1.31e-3 eV/atom,<br>DP-QEq_ConstQ_force_RMSE_test: 1.63e-1 eV/Å,<br>DP-QEq_ConstP_energy_RMSE_test: 1.00e-2 eV/atom,<br>DP-QEq_ConstP_force_RMSE_test: 2.27e-1 eV/Å,<br>Li_atoms_force_RMSE_in_dendrite_regions: 0.07–0.09 eV/Å |
| **Application Domains** | Li metal batteries (Li metal anodes, solid electrolyte interphase),<br>Electrochemical interface modeling,<br>Atomistic simulations of dendrite nucleation,<br>All-solid-state batteries (suggested broader application),<br>Electrocatalytic surface corrosion modeling (suggested broader application) |

---


### [323. Designing Pb-Free High-Entropy Relaxor Ferroelectrics with Machine Learning Assistance for High Energy Storage](https://doi.org/10.1021/jacs.5c07213), Journal of the American Chemical Society *(August 06, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | dataset of 141 titanate-based compositions |
| **Models** | Random Forest |
| **Tasks** | Regression,<br>Feature Selection,<br>Dimensionality Reduction,<br>Hyperparameter Optimization,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning |
| **Performance Highlights** | test_R2: 0.81,<br>MAE: 0.7 J cm^-3,<br>experimental_Wrec_of_ML-designed_composition: 17.2 J cm^-3,<br>efficiency_eta: 87%,<br>breakdown_strength_EB: 79 kV mm^-1 |
| **Application Domains** | Materials Science,<br>Dielectric Energy Storage,<br>Relaxor Ferroelectrics,<br>High-Entropy Materials,<br>Pb-free ceramic capacitors,<br>Pulsed-power electronic systems |

---


### [321. Quantifying large language model usage in scientific papers](https://doi.org/10.1038/s41562-025-02273-8), Nature Human Behaviour *(August 04, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | arXiv,<br>bioRxiv,<br>Nature portfolio (15 journals),<br>Validation set (pre-ChatGPT) |
| **Models** | GPT,<br>Transformer,<br>Language Modeling |
| **Tasks** | Text Summarization,<br>Text Generation,<br>Distribution Estimation,<br>Binary Classification,<br>Text Classification,<br>Information Retrieval,<br>Feature Extraction,<br>Language Modeling |
| **Learning Methods** | Maximum Likelihood Estimation,<br>Prompt Learning,<br>Zero-Shot Learning,<br>Fine-Tuning,<br>Embedding Learning,<br>Adversarial Training,<br>Contrastive Learning,<br>Feature Extraction |
| **Performance Highlights** | population_level_prediction_error: < 3.5%,<br>computer_science_abstracts_alpha_Sept_2024: 22.5% (bootstrapped 95% CI (21.7%, 23.3%)),<br>computer_science_introductions_alpha_Sept_2024: 19.6% (bootstrapped 95% CI (19.2%, 20.0%)),<br>electrical_engineering_abstracts_alpha_Sept_2024: 18.0% (bootstrapped 95% CI (16.7%, 19.3%)),<br>electrical_engineering_introductions_alpha_Sept_2024: 18.4% (bootstrapped 95% CI (17.8%, 19.0%)),<br>mathematics_abstracts_alpha_Sept_2024: 7.7% (bootstrapped 95% CI (7.1%, 8.3%)),<br>nature_portfolio_abstracts_alpha_Sept_2024: 8.9% (bootstrapped 95% CI (8.2%, 9.6%)),<br>validation_set_size_sentences_per_ground_truth_alpha: n = 30,000 sentences; validation range alpha = 0% to 25% in 5% increments,<br>abstracts_more_similar_alpha_Sept_2024: 23.0% (bootstrapped 95% CI (22.3%, 23.7%)),<br>abstracts_less_similar_alpha_Sept_2024: 18.7% (bootstrapped 95% CI (18.0%, 19.4%)),<br>robustness_to_proofreading_estimated_increase: approx. +1% in estimated alpha after LLM 'proofreading' (minor edits),<br>validation_role: Used to generate realistic LLM-produced training data which contributed to validation error < 3.5% (see above) |
| **Application Domains** | scientific publishing / academic writing,<br>computer science research papers (arXiv),<br>electrical engineering and systems science (arXiv),<br>mathematics (arXiv),<br>physics and statistics (arXiv),<br>biology (bioRxiv),<br>journal articles in Nature portfolio (multidisciplinary journals) |

---


### [320. Modeling protein conformational ensembles by guiding AlphaFold2 with Double Electron Electron Resonance (DEER) distance distributions](https://doi.org/10.1038/s41467-025-62582-4), Nature Communications *(August 02, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | OpenFold training subset (fine-tuning set),<br>OpenFold larger training set,<br>PDBe-KB validation dataset,<br>Benchmark set of 29 targets (59 conformations),<br>PfMATE experimental DEER distance distributions,<br>LmrP experimental DEER distance distributions,<br>Pgp (ABCB1) experimental DEER distance distributions,<br>Simulated DEER datasets (Kazmier method) - Simulation 1,<br>Simulated DEER datasets (Kazmier method) - Simulation 2,<br>Zenodo repository of training targets and benchmarking results |
| **Models** | Transformer,<br>Attention Mechanism |
| **Tasks** | Structured Prediction,<br>Clustering,<br>Dimensionality Reduction |
| **Learning Methods** | Fine-Tuning,<br>Pre-training,<br>Transfer Learning,<br>Representation Learning |
| **Performance Highlights** | PfMATE_unconstrained_Neff=5_median_RMSD_to_OF: 0.87 Å,<br>PfMATE_unconstrained_Neff=5_median_RMSD_to_IF: 5.19 Å,<br>PfMATE_Experiment1_Neff=5_median_RMSD_to_IF: 2.11 Å,<br>PfMATE_Experiment1_Neff=5_median_RMSD_to_OF: 3.53 Å,<br>PfMATE_Experiment2_Neff=5_median_RMSD_to_IF: 1.32 Å,<br>PfMATE_Experiment2_Neff=5_median_RMSD_to_OF: 4.71 Å,<br>PfMATE_Simulation1_Neff=5_median_RMSD_to_IF: 1.19 Å,<br>PfMATE_Simulation2_Neff=5_median_RMSD_to_IF: 1.22 Å,<br>PfMATE_TM-score_Experiment1_full_MSA_%predictions_TM>0.9_to_IF: 93%,<br>LmrP_unconstrained_Neff=5_median_RMSD_to_IF: 5.56 Å (initial unconstrained set),<br>LmrP_Experiment1_Neff=5_median_RMSD_to_IF: 1.52 Å,<br>LmrP_Experiment2_Neff=5_median_RMSD_to_IF: 1.19 Å,<br>LmrP_Simulation1_Neff=5_median_RMSD_to_IF: 1.28 Å,<br>LmrP_Simulation2_2constraints_#models_RMSD<3Å: 58 out of 100 models,<br>Pgp_unconstrained_median_RMSD_to_IF_narrow: 3.41 Å,<br>Pgp_Experiment1_ADP-Vi_median_RMSD_to_OF_references: 3.25 Å and 3.30 Å,<br>Pgp_Simulation2_fullset_54pairs_success_rate_TM>=0.9_to_7ZK7: 100% (all predictions transitioned),<br>Pgp_Simulation2_random_8_constraints_#models_switched_to_target_7ZK7_TM>=0.9: 293 out of 500 (58.6%),<br>Benchmark_29_targets_constrained_average_RMSD_to_target: 3.11 Å (average constrained predictions),<br>Benchmark_29_targets_constrained_average_TM-score: 0.91 (average constrained predictions),<br>Benchmark_paired_t-test_RMSD_improvement: p = 1.38E-05,<br>Benchmark_success_rate_over_29_targets: >70% for all 29 targets; 20 targets achieved 100%,<br>EMD_correlation_with_TM-scores: Lower EMD distances consistently correspond to higher TM-scores and better match to target conformations (examples: LmrP, PfMATE, Pgp cluster centroids RMSD/TM reported),<br>LmrP_cluster_centroids_RMSD_to_OF: 0.81 Å (yellow centroid),<br>LmrP_blue_centroid_RMSD_to_IF: 1.73 Å, TM-score = 0.96,<br>PfMATE_blue_centroid_RMSD_to_IF: 2.03 Å, TM-score = 0.93,<br>Pgp_PC1_cluster_series_RMSD_range_to_IF: RMSD decreased from 5.38 Å to 2.93 Å with TM-score increasing from 0.84 to 0.95 |
| **Application Domains** | Protein structure prediction,<br>Structural biology,<br>Protein conformational dynamics,<br>Membrane transporter biology,<br>Biophysical spectroscopy (DEER/EPR) integration with ML,<br>Model-guided experimental design (spin-label pair optimization),<br>Computational structural biophysics |

---


### [319. Navigating protein landscapes with a machine-learned transferable coarse-grained model](https://doi.org/10.1038/s41557-025-01874-0), Nature Chemistry *(August 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | CATH domains training set (50 protein domains),<br>Dimer mono-/dipeptide dataset (~1,200 / 1,245 dimers),<br>Decoy/augmented frames (noisy frames),<br>Test proteins (unseen) — set of small peptides and proteins with PDB codes,<br>PUMA–MCL-1 and PUMA–Ubiquitin systems (case studies),<br>Ubiquitin mutational benchmark (experimental ΔΔG from Went & Jackson 2005) |
| **Models** | Graph Neural Network,<br>Graph Neural Network |
| **Tasks** | Distribution Estimation,<br>Clustering,<br>Dimensionality Reduction,<br>Regression,<br>Feature Extraction,<br>Clustering |
| **Learning Methods** | Supervised Learning,<br>Representation Learning |
| **Performance Highlights** | qualitative_match_of_free_energy_landscapes: CG free energy landscapes reproduce folded/unfolded/metastable basins comparable to all-atom reference for many test proteins,<br>extrapolation_sequence_similarity_range: test proteins have low sequence similarity to training set (16–40% for many test proteins; Table 1 lists specific percents),<br>Pearson_correlation_coefficient_r: 0.63,<br>MAE: 1.25 kcal mol^-1,<br>homeodomain_mean_RMSD_to_crystal: ~0.5 nm,<br>homeodomain_fraction_native_contacts_Q: ~0.75,<br>PUMA_induced_folding_r.m.s.d.: ~2.5 Å (0.25 nm threshold referenced),<br>landscape_visualization: free energy landscapes plotted on TICA coordinates show CGSchNet captures many metastable states similar to all-atom reference (qualitative),<br>comparison_to_other_CG_models: CGSchNet explores much of the all-atom free energy landscape; AWSEM/UNRES/Martini often stabilize a single metastable state,<br>transferability: model extrapolates to proteins with low sequence similarity (examples in Table 1), and stabilizes folded states of larger proteins that were withheld from training,<br>speedup: orders of magnitude faster than all-atom MD (qualitative; Supplementary Table 9 referenced) |
| **Application Domains** | protein folding and conformational dynamics,<br>molecular dynamics / computational chemistry,<br>structural biology,<br>protein stability and mutational effect prediction,<br>protein–peptide binding and folding-upon-binding studies,<br>coarse-grained modeling and force-field development |

---


### [318. Data-driven de novo design of super-adhesive hydrogels](https://doi.org/10.1038/s41586-025-09269-4), Nature *(August 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | NCBI adhesive protein dataset (adhesive proteins),<br>Resilin protein dataset,<br>DM-driven hydrogel dataset (initial experimental dataset),<br>Expanded hydrogel dataset (training + validation after ML rounds) |
| **Models** | Gaussian Process,<br>Random Forest,<br>Gradient Boosting Tree,<br>XGBoost,<br>Support Vector Machine,<br>Linear Model |
| **Tasks** | Regression,<br>Optimization,<br>Feature Extraction,<br>Dimensionality Reduction,<br>Experimental Design,<br>Feature Selection,<br>Synthetic Data Generation |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Batch Learning |
| **Performance Highlights** | model_selection: GP and RFR achieved the lowest RMSE among nine models (exact RMSE values not specified in main text),<br>role_in_BO: GP_KB and RFR-GP were top-performing SMBO methods; GP_KB identified high-Fa formulations,<br>model_selection: RFR was runner-up to GP in RMSE; selected as an effective base model,<br>SMBO_best: RFR-GP produced the highest Fa overall among SMBO methods; warm-start variant RFR-GP* exhibited the highest Fa among all models,<br>usage: GBM used as one of EI maximizers in batched SMBO (RFR-GBM) but RFR-GP and GP_KB outperformed other combinations,<br>benchmarking: XGBoost (XGB) included among benchmarked non-linear models; not reported as top performer vs GP/RFR,<br>benchmarking: Support vector regression (SVR) tested among non-linear models but did not achieve lowest RMSE,<br>benchmarking: Lasso and Ridge linear regressions used as linear baselines; inferior to non-linear models (GP, RFR) in RMSE,<br>experimental_dataset_growth: Initial dataset 180 → after round 1 added 109 validated points (total 289) → rounds 2 and 3 added 27 and 25 (final 341),<br>hydrogel_performance: ML-driven hydrogels achieved Fa exceeding 1 MPa (R1-max); best DM-driven hydrogel (G-max) had Fa = 147 kPa; among 180 DM-driven gels 16 had Fa > 100 kPa and 83 had Fa > 46 kPa,<br>durability: R1-max maintained robust adhesion over 200 attachment–detachment cycles; R1-max sustained 1-kg shear load for over 1 year |
| **Application Domains** | Soft materials design,<br>Adhesive hydrogels / biomaterials,<br>Biomedical engineering (implantation, wound sealing),<br>Marine environments / deep-sea exploration,<br>Marine farming and seawater applications,<br>Polymer chemistry and materials discovery |

---


### [317. Electron-density-informed effective and reliable de novo molecular design and optimization with ED2Mol](https://doi.org/10.1038/s42256-025-01095-7), Nature Machine Intelligence *(August 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | DUD-E,<br>ASB-E (AlloSteric Benchmark—Enhanced),<br>PDB-derived pocket–ligand training set (processed subset),<br>GP dataset (Growth-Point dataset),<br>TA dataset (Torsion-Angle dataset),<br>ZINC subset (drug-like molecules),<br>Real-world target case sets (FGFR3, CDC42, GCK, GPRC5A) |
| **Models** | Variational Autoencoder,<br>Graph Neural Network,<br>Convolutional Neural Network,<br>Multi-Layer Perceptron,<br>Diffusion Model,<br>Graph Neural Network (baseline mappings) |
| **Tasks** | Graph Generation,<br>Optimization,<br>Binary Classification,<br>Multi-class Classification,<br>Generative Learning,<br>Representation Learning,<br>Regression,<br>Ranking |
| **Learning Methods** | Supervised Learning,<br>Generative Learning,<br>Representation Learning,<br>Fine-Tuning,<br>Classification,<br>Adversarial Training |
| **Performance Highlights** | GP_dataset_size: 2,986,278 samples (from 94,519 molecules),<br>TA_dataset_size: 17,978,368 samples (from 4,497,302 molecules),<br>Generation_success_rate_DUD-E: 67.3%,<br>Generation_success_rate_ASB-E: 68.9%,<br>PB-valid (PoseBusters) percentage: 97.3 ± 0.4%,<br>Occupancy_ranking_score: 28.2 (ED2Mol) > TargetDiff 26.2 > Pocket2Mol 25.6 > FLAG 15.0 > ResGen 7.9 > GraphBP 4.5,<br>Occupancy_coverage: 84% (ED2Mol) vs 74% (TargetDiff),<br>Reliable_success_rate: 34.4 ± 0.5% (ED2Mol) vs TargetDiff 17.1 ± 1.0% and Pocket2Mol 15.0 ± 2.0%,<br>Re-docking_stability_reference: reference co-crystal ligands recovered 72% (RMSD < 2 Å),<br>Re-docking_stability_notes: ED2Mol showed the highest fidelity; '12.7% more stable poses than the second-best model, Pocket2Mol, on DUD-E (34.8% total)' and '15.9% better stability than the second-best model, TargetDiff, on ASB-E (29.0% total)'.,<br>PB-valid (PoseBusters) percentages: TargetDiff 58.1 ± 0.5% (PB-valid),<br>Reliable_success_rate (reported baseline): TargetDiff 17.1 ± 1.0% (reliable success rate),<br>PB-valid percentages: Pocket2Mol 63.1 ± 0.4%; FLAG 59.4 ± 1.4%; ResGen 34.1 ± 3.9%; GraphBP 32.2 ± 1.2%,<br>Generation_success_rate_notes: ResGen and GraphBP produce poor binding molecules scored higher than zero (qualitative remark).,<br>Reliable_success_rate_notes: Pocket2Mol 15.0 ± 2.0%; FLAG 6.7 ± 0.1%; ResGen 4.5 ± 1.1%,<br>FGFR3: F4 KD = 599.0 μM (SPR); F42 KD = 61.4 μM (SPR), 9.8-fold improvement,<br>Brr2 (hit optimization): Recovered inhibitor 9; docking RMSD = 0.56 Å (ED2Mol-generated vs re-docked lead),<br>PPARγ (fragment growth): Recovered activator 3 with RMSD = 0.95 Å,<br>CDC42: C1 IC50 = 47.58 ± 3.71 μM (in vivo BRET assay); C11 IC50 = 111.63 ± 0.90 μM; C1 KD = 5.35 μM (SPR),<br>GCK: G1 EC50 = 290 nM (6.1-fold over G0); G11 EC50 = 150 nM (11.9-fold improvement),<br>GPRC5A: A4 EC50 = 8.94 ± 2.30 μM (PRESTO-Tango assay); ED2Mol vs AlphaFold top-ranked holo structure RMSD = 1.48 Å |
| **Application Domains** | Structure-based drug discovery,<br>De novo molecular design,<br>Hit identification and lead optimization,<br>Protein–ligand binding / computational structural biology,<br>Allosteric modulator discovery,<br>Cheminformatics / fragment-based molecular generation,<br>Experimental validation (biophysical and cellular assays) |

---


### [316. Kolmogorov–Arnold graph neural networks for molecular property prediction](https://doi.org/10.1038/s42256-025-01087-7), Nature Machine Intelligence *(August 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | BACE,<br>BBBP,<br>ClinTox,<br>SIDER,<br>Tox21,<br>HIV,<br>MUV |
| **Models** | Graph Neural Network,<br>Graph Convolutional Network,<br>Graph Attention Network,<br>Multi-Layer Perceptron |
| **Tasks** | Graph Classification,<br>Binary Classification,<br>Multi-label Classification,<br>Regression |
| **Learning Methods** | Supervised Learning,<br>End-to-End Learning,<br>Backpropagation,<br>Pre-training,<br>Contrastive Learning |
| **Performance Highlights** | BACE (ROC-AUC): 0.890(0.014),<br>BBBP (ROC-AUC): 0.787(0.014),<br>ClinTox (ROC-AUC): 0.992(0.005),<br>SIDER (ROC-AUC): 0.842(0.001),<br>Tox21 (ROC-AUC): 0.799(0.005),<br>HIV (ROC-AUC): 0.821(0.005),<br>MUV (ROC-AUC): 0.834(0.009),<br>BACE (ROC-AUC): 0.884(0.004),<br>BBBP (ROC-AUC): 0.785(0.021),<br>ClinTox (ROC-AUC): 0.991(0.005),<br>SIDER (ROC-AUC): 0.847(0.002),<br>Tox21 (ROC-AUC): 0.800(0.006),<br>HIV (ROC-AUC): 0.823(0.002),<br>MUV (ROC-AUC): 0.834(0.010),<br>BACE (ROC-AUC): 0.835(0.014),<br>BBBP (ROC-AUC): 0.735(0.011),<br>ClinTox (ROC-AUC): 0.979(0.004),<br>SIDER (ROC-AUC): 0.834(0.001),<br>Tox21 (ROC-AUC): 0.747(0.006),<br>HIV (ROC-AUC): 0.762(0.005),<br>MUV (ROC-AUC): 0.741(0.006),<br>BACE (ROC-AUC): 0.834(0.012),<br>BBBP (ROC-AUC): 0.707(0.007),<br>ClinTox (ROC-AUC): 0.983(0.006),<br>SIDER (ROC-AUC): 0.836(0.002),<br>Tox21 (ROC-AUC): 0.751(0.007),<br>HIV (ROC-AUC): 0.761(0.003),<br>MUV (ROC-AUC): 0.784(0.019),<br>BACE (ROC-AUC): 0.890(0.014),<br>BBBP (ROC-AUC): 0.787(0.014),<br>ClinTox (ROC-AUC): 0.992(0.005),<br>SIDER (ROC-AUC): 0.842(0.001),<br>Tox21 (ROC-AUC): 0.799(0.005),<br>HIV (ROC-AUC): 0.821(0.005),<br>MUV (ROC-AUC): 0.834(0.009),<br>BACE (ROC-AUC): 0.853(0.027),<br>BBBP (ROC-AUC): 0.654(0.009),<br>ClinTox (ROC-AUC): 0.981(0.004),<br>SIDER (ROC-AUC): 0.832(0.006),<br>Tox21 (ROC-AUC): 0.715(0.004),<br>HIV (ROC-AUC): 0.804(0.010),<br>MUV (ROC-AUC): 0.787(0.012),<br>BACE (ROC-AUC): 0.771(0.012),<br>BBBP (ROC-AUC): 0.723(0.008),<br>ClinTox (ROC-AUC): 0.973(0.006),<br>SIDER (ROC-AUC): 0.824(0.003),<br>Tox21 (ROC-AUC): 0.724(0.005),<br>HIV (ROC-AUC): 0.753(0.007),<br>MUV (ROC-AUC): 0.638(0.008),<br>BACE (ROC-AUC): 0.884(0.004),<br>BBBP (ROC-AUC): 0.785(0.021),<br>ClinTox (ROC-AUC): 0.991(0.005),<br>SIDER (ROC-AUC): 0.847(0.002),<br>Tox21 (ROC-AUC): 0.800(0.006),<br>HIV (ROC-AUC): 0.823(0.002),<br>MUV (ROC-AUC): 0.834(0.010),<br>BACE (ROC-AUC): 0.8319(0.007),<br>BBBP (ROC-AUC): 0.708(0.005),<br>ClinTox (ROC-AUC): 0.983(0.003),<br>SIDER (ROC-AUC): 0.836(0.001),<br>Tox21 (ROC-AUC): 0.753(0.004),<br>HIV (ROC-AUC): 0.818(0.006),<br>MUV (ROC-AUC): 0.797(0.011),<br>BACE (ROC-AUC): 0.808(0.009),<br>BBBP (ROC-AUC): 0.657(0.004),<br>ClinTox (ROC-AUC): 0.948(0.003),<br>SIDER (ROC-AUC): 0.825(0.004),<br>Tox21 (ROC-AUC): 0.731(0.012),<br>HIV (ROC-AUC): 0.744(0.018),<br>MUV (ROC-AUC): 0.792(0.013),<br>Qualitative comparison: Fourier-based KAN demonstrates superior approximation capability compared to standard two-layer MLP across six representative functions (see Supplementary Fig. 1). |
| **Application Domains** | Molecular property prediction,<br>Drug discovery,<br>Computational chemistry,<br>Biophysics,<br>Physiology,<br>Geometric deep learning for non-Euclidean data,<br>General molecular data modelling |

---


### [315. An actor–critic algorithm to maximize the power delivered from direct methanol fuel cells](https://doi.org/10.1038/s41560-025-01804-x), Nature Energy *(August 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Three-electrode chronoamperometry (CA) current–time trajectories (experimental),<br>DMFC device long-term operation measurements (MEA experiments),<br>Augmented dataset (synthetic adjacency-based augmentation) |
| **Models** | Convolutional Neural Network,<br>Multi-Layer Perceptron,<br>Feedforward Neural Network |
| **Tasks** | Control,<br>Policy Learning,<br>Decision Making,<br>Regression,<br>Data Augmentation,<br>Optimization,<br>Decision Making |
| **Learning Methods** | Reinforcement Learning,<br>Policy Gradient,<br>Temporal Difference Learning,<br>Supervised Learning,<br>Stochastic Gradient Descent,<br>Gradient Descent |
| **Performance Highlights** | validation_MAE_mW: 0.011,<br>Pearson_r: 0.969,<br>alternate_plot_MAE_mW: 0.013,<br>alternate_plot_r: 0.965,<br>training_epochs_to_converge: 300,<br>4h_produced_power_mW_alphaFC: 0.284 ± 0.013,<br>4h_multiplier_vs_Co-Pt-Ru/NC_constant: 2.15×,<br>4h_multiplier_vs_PtRu/C_constant: 4.64×,<br>12h_time_averaged_power_vs_constant_percent: 153%,<br>12h_increase_vs_switching_strategy_percent: 30.4%,<br>12h_increase_vs_constant_strategies_percent: 185.2% and 486.1% (context-specific comparisons),<br>90h_average_power_multiplier_vs_constant: 4.86× (≈486%),<br>long_term_stable_hours: >250 hours,<br>actor_action_selection_time_CPU_s: ≈0.3,<br>comparison_with_GA: GBO consistently achieves superior predicted produced power across batch sizes and run-time constraints (fig. 3c),<br>runtime_tradeoff_note: Gradient tracking takes roughly three times more computations (justifying GA comparison),<br>PID_power_fraction_of_alphaFC: ≈50%,<br>MPC_note: MPC reliability degraded when simulation inaccurate; not numerically quantified here |
| **Application Domains** | Electrochemistry / Fuel cells (Direct Methanol Fuel Cells, DMFCs),<br>Experimental device control / Real-world control systems,<br>Energy devices: maximizing power delivery and prolonging catalyst life,<br>Edge artificial intelligence for laboratory/device controllers,<br>Potential generalization domains mentioned: battery formation/charging protocols, electrodeposition, temperature/fluid flow control in reactors |

---


### [313. Deep learning for property prediction of natural fiber polymer composites](https://doi.org/10.1038/s41598-025-10841-1), Scientific Reports *(July 30, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | This study experimental natural-fiber composite dataset (augmented),<br>Li et al. microstructure dataset (stochastic microstructures),<br>MPOB degradable plastics dataset (Bakar et al.),<br>Volgin synthetic polyimide dataset + experimental PIs,<br>Gurnani et al. polymer dataset,<br>polyBERT pretraining dataset and downstream dataset,<br>Jung et al. optical property dataset,<br>Aldeghi et al. polymer dataset (wD-MPNN),<br>Xue et al. CFRP-wrapped RC columns dataset,<br>Zhu et al. hydrogel SAW dataset,<br>Bradford et al. SPE experimental dataset (ChemArr),<br>QM9 and other referenced datasets |
| **Models** | Deep Neural Network,<br>Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>3D Convolutional Neural Network,<br>Graph Neural Network,<br>Graph Convolutional Network,<br>Message Passing Neural Network,<br>Graph Attention Network,<br>Long Short-Term Memory,<br>Transformer,<br>Random Forest,<br>Gradient Boosting Tree,<br>Linear Model,<br>Deep Residual Convolutional Neural Network |
| **Tasks** | Regression,<br>Regression,<br>Sequence-to-Sequence,<br>Regression,<br>Regression,<br>Representation Learning / Feature Extraction,<br>Hyperparameter Optimization,<br>Multitask Learning |
| **Learning Methods** | Supervised Learning,<br>Transfer Learning,<br>Pre-training,<br>Fine-Tuning,<br>Multi-Task Learning,<br>Data Augmentation,<br>Ensemble Learning,<br>Hyperparameter Optimization,<br>Feature Selection,<br>AutoML,<br>Representation Learning |
| **Performance Highlights** | Tensile Strength R2: 0.89 ± 0.01,<br>Tensile Strength MAE: 2.1 ± 0.2 MPa,<br>Young's Modulus R2: 0.87 ± 0.02,<br>Young's Modulus MAE: 105 ± 7 MPa,<br>Elongation at Break R2: 0.83 ± 0.02,<br>Elongation at Break MAE: 1.3 ± 0.1 %,<br>Impact Strength R2: 0.85 ± 0.02,<br>Impact Strength MAE: 0.35 ± 0.03 kJ/m2,<br>Tensile Strength R2: 0.88 ± 0.01,<br>Tensile Strength MAE: 2.3 ± 0.2 MPa,<br>Young's Modulus R2: 0.84 ± 0.02,<br>Young's Modulus MAE: 115 ± 8 MPa,<br>Elongation at Break R2: 0.81 ± 0.02,<br>Elongation at Break MAE: 1.5 ± 0.1 %,<br>Impact Strength R2: 0.83 ± 0.02,<br>Impact Strength MAE: 0.4 ± 0.03 kJ/m2,<br>Tensile Strength R2: 0.85 ± 0.02,<br>Tensile Strength MAE: 2.7 ± 0.3 MPa,<br>Young's Modulus R2: 0.82 ± 0.03,<br>Young's Modulus MAE: 130 ± 10 MPa,<br>Elongation at Break R2: 0.78 ± 0.03,<br>Elongation at Break MAE: 1.8 ± 0.2 %,<br>Impact Strength R2: 0.80 ± 0.03,<br>Impact Strength MAE: 0.5 ± 0.05 kJ/m2,<br>Tensile Strength R2: 0.72 ± 0.03,<br>Tensile Strength MAE: 4.5 ± 0.4 MPa,<br>Young's Modulus R2: 0.65 ± 0.04,<br>Young's Modulus MAE: 210 ± 15 MPa,<br>Elongation at Break R2: 0.60 ± 0.05,<br>Elongation at Break MAE: 3.2 ± 0.3 %,<br>Impact Strength R2: 0.68 ± 0.04,<br>Impact Strength MAE: 0.9 ± 0.1 kJ/m2,<br>Longitudinal modulus R2: 0.991,<br>Transverse modulus R2: 0.969,<br>In-plane shear modulus R2: 0.984,<br>Major Poisson's ratio R2: 0.903,<br>Out-of-plane shear modulus R2: 0.955,<br>Tg MAE: 20 K,<br>Stress–strain curve prediction speed: seconds vs days for simulations,<br>accuracy: high (qualitative; reported as fast, accurate predictions),<br>Outperformance vs baselines: combined model outperformed both graph-only and sequence-only baselines (no single-number metric provided),<br>λmax prediction improvement: reduced systematic errors for certain classes (qualitative); no single-number metric provided in text,<br>Relative performance: outperformed Chemprop and XGBoost in both absolute error and ranking accuracy (no numeric values provided),<br>Improved accuracy: significant improvement over unweighted MPNNs (no numeric metric provided) |
| **Application Domains** | Natural-fiber polymer composites (mechanical property prediction),<br>Polymer informatics (polymer property prediction and fingerprinting),<br>Soft materials and hydrogels (mechanical response prediction),<br>Solid polymer electrolytes (ionic conductivity prediction for batteries),<br>Molecular design for solar cells and optical properties (λmax prediction),<br>Geopolymer concrete compressive strength prediction,<br>Coarse-grained polymer modeling and accelerated molecular simulation,<br>Structural engineering (CFRP-wrapped RC columns, lateral confinement coefficient) |

---


### [312. Accelerating primer design for amplicon sequencing using large language model-powered agents](https://doi.org/10.1038/s41551-025-01455-z), Nature Biomedical Engineering *(July 30, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | SARS-CoV-2 synthetic RNA standards (Wuhan-01),<br>Human genomic DNA standard NA12878,<br>Mycobacterium tuberculosis (MTB) DNA standards and MTB culture isolates,<br>Plasmid pools for enzyme mutant sequencing (Luc, KODm, Cid1, TdT),<br>Curated microbial reference database (PrimeGen),<br>ClinVar, OMIM, COSMIC, UniProt, CARD and WHO clinical-annotation datasets,<br>SARS-CoV-2 variant file (derived) |
| **Models** | GPT,<br>Transformer,<br>Vision Transformer |
| **Tasks** | Information Retrieval,<br>Optimization,<br>Text Generation,<br>Anomaly Detection,<br>Classification,<br>Embedding Learning,<br>Pre-training,<br>Sequence-to-Sequence |
| **Learning Methods** | Fine-Tuning,<br>Prompt Learning,<br>Pre-training,<br>Representation Learning,<br>Embedding Learning,<br>Supervised Learning,<br>In-Context Learning |
| **Performance Highlights** | sequence_search_success_rate_%: GPT-4o: 86; GPT-4: 84; Qwen-Max: 89; Qwen2.5-72B-Instruct: 88; GPT-3.5: 70; GLM-4-plus: 85,<br>panel_optimization_reduction_in_loss_relative_per_method: LLM optimizer stabilizes loss comparable to stochastic greedy in 12-plex; in 78-plex LLM optimizer outperforms AdaLead and GA after ~750 iterations (no absolute loss value provided),<br>code_modification_accuracy_%: GPT-4o: 100; GPT-4: 100; Qwen-Max: 100; Qwen2.5-72B-Instruct: 100; GPT-3.5: 88.75; GLM-4-plus: 85.2,<br>Qwen2VL-7B_average_accuracy: 0.87,<br>Qwen2VL-7B_slot-layout_accuracy: 0.79,<br>Qwen2VL-7B_pipette-side_precision: 0.94,<br>Qwen2VL-7B_well-plate-side_precision: 0.88,<br>Qwen2VL-7B_BLEU: 0.20,<br>Qwen2VL-7B_ROUGE-L: 0.50,<br>Qwen2VL-7B_GPT4-Score: 4.25,<br>retrieval_top_k: top-5 candidate code blocks retrieved via embedding similarity before final selection by GPT-4o |
| **Application Domains** | Biomedical research,<br>Targeted next-generation sequencing (tNGS) and amplicon sequencing,<br>Viral genomics (SARS-CoV-2 sequencing and variant surveillance),<br>Clinical genetics (expanded carrier screening),<br>Microbial pathogen detection and drug-resistance mutation detection (Mycobacterium tuberculosis),<br>Protein engineering and directed evolution (plasmid mutant sequencing),<br>Laboratory automation / self-driving laboratories / liquid-handling robotics,<br>AI-driven protocol generation and code automation,<br>Vision-based laboratory anomaly detection |

---


### [309. Biomimetic Intelligent Thermal Management Materials: From Nature-Inspired Design to Machine-Learning-Driven Discovery](https://doi.org/10.1002/adma.202503140), Advanced Materials *(July 29, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | 119-compound DFT/phonon dataset,<br>911 datapoints from 25 studies (nanomaterial-enhanced PCM dataset),<br>Molecular dynamics (MD) heat-flux dataset (training data for GAN),<br>Polymer repeat-unit / MD-augmented polymer dataset (for CNN),<br>260 MD simulation results (for XGBoost seawater-evaporation model),<br>Inverse-design / metasurface spectral response dataset (FDTD-simulated spectra),<br>SETC performance dataset (solar absorption / storage metrics) |
| **Models** | Random Forest,<br>Support Vector Machine,<br>Gaussian Process,<br>Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>Generative Adversarial Network,<br>Conditional GAN,<br>Diffusion Model,<br>Graph Neural Network,<br>Long Short-Term Memory,<br>Bidirectional LSTM,<br>XGBoost,<br>Gradient Boosting Tree,<br>Feedforward Neural Network,<br>Gaussian Mixture Model,<br>Convolutional Neural Network (metasurface forward model) |
| **Tasks** | Regression,<br>Time Series Forecasting,<br>Optimization,<br>Synthetic Data Generation,<br>Ranking,<br>Clustering,<br>Feature Extraction,<br>Image-to-Image (interpreted as structure→spectrum mapping / inverse design) |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Reinforcement Learning,<br>Transfer Learning,<br>Active Learning,<br>Generative Learning,<br>Adversarial Training,<br>Evolutionary Learning,<br>Pre-training,<br>Fine-Tuning |
| **Performance Highlights** | dataset_size: 119 compounds,<br>features: 57 features from crystal structure and composition,<br>R2: 0.93,<br>thermal_conductivity_Al: ~7 W m^-1 K^-1,<br>thermal_conductivity_Cu: 13–14 W m^-1 K^-1,<br>SEA_enhancement: 6–36x (initial SEA enhancement),<br>recoverability_SEA_improvement: 0.2–1.5x,<br>predicted_cooling_theoretical: 8.5 °C,<br>experimental_cooling: 8.3 °C,<br>reflectance: >95%,<br>training_MD_samples: 260 MD results,<br>candidate_materials_screened: 38,142 2D materials,<br>validation_metrics: low mean absolute error and high coefficient of determination (reported qualitatively) |
| **Application Domains** | Electronics (device thermal management, semiconductor cooling),<br>Aerospace (thermal protection and management),<br>Biomedical (wearable thermal textiles, personal thermal management),<br>Buildings (energy-efficient facades, smart windows, passive radiative cooling),<br>Robotics (biomimetic robot thermal management),<br>Photovoltaics (PV-leaf transpiration cooling, PV temperature reduction),<br>Desalination / water harvesting (solar-driven atmospheric water harvesting, evaporation-based cooling),<br>Energy storage (phase-change materials, solar-thermal storage),<br>Metasurfaces / photonics (spectral regulation, infrared stealth) |

---


### [308. Atomistic Generative Diffusion for Materials Modeling](https://doi.org/10.48550/arXiv.2507.18314), Preprint *(July 24, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Quantum Cluster Database (QCD),<br>Computational 2D Materials Database (C2DB) |
| **Models** | Denoising Diffusion Probabilistic Model,<br>Graph Neural Network,<br>Variational Autoencoder |
| **Tasks** | Data Generation,<br>Synthetic Data Generation,<br>Graph Generation |
| **Learning Methods** | Unsupervised Learning,<br>Self-Supervised Learning,<br>Representation Learning |
| **Performance Highlights** | Precision-Recall AUC (QCD GM clusters model): AUC value shown on PR curve in Fig.3(b) (higher than the larger ΔE<200 meV model); exact numeric AUCs are reported in the figure panels.,<br>Precision-Recall AUC (QCD ΔE<200 meV model): Lower AUC / lower recall compared to GM clusters model (value shown in Fig.3(b)).,<br>Precision-Recall AUC (C2DB model): PR curve and area under curve values shown in Fig.5(b) (figure labels show multiple baseline and model AUC values; model exhibits strong precision and recall).,<br>Symmetry accuracy vs guidance scale: Symmetry accuracy peaks at moderate guidance scales (w ≈ 0.5–0.75) as shown in Fig.6(a); increasing guidance beyond w=1 degrades symmetry preservation.,<br>Qualitative plausibility of interpolated bimetallic clusters: t-SNE visualizations (Fig.4) show interpolated Pt–Cu and Pd–Ag clusters spanning configuration space between mono-metallic endpoints; sampled structures 'appear visually similar to known bimetallic motifs'.,<br>Baseline PR curves (synthetic perturbations): Multiple synthetic baselines generated by adding Gaussian noise (σ varied) and subsampling (e.g., 100% and 50% coverage) produce PR curves shown in Fig.3(b) and Fig.5(b); these baselines are used as reference anchors. |
| **Application Domains** | Materials discovery,<br>Nanoclusters / nanomaterials,<br>Two-dimensional materials,<br>Catalysis (mentioned as application domain),<br>Energy storage materials,<br>Crystal structure prediction (cited as potential application / future work),<br>Inverse materials design |

---


### [307. Decoding nature’s grammar with DNA language models](https://doi.org/10.1073/pnas.2512889122), Proceedings of the National Academy of Sciences *(July 22, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | 16 plant genomes (pretraining set),<br>Maize genomes / maize variant calls (transfer evaluation),<br>Arabidopsis annotated sites (benchmarks: transcription initiation and termination sites),<br>Splice donor and acceptor site annotations (benchmarks),<br>Site frequency spectrum / population variant frequency data,<br>Sweet corn causal mutation (well-studied causal mutation),<br>GERP++ constraint scores / alignment-based constraint datasets |
| **Models** | BERT,<br>Transformer,<br>Autoencoder |
| **Tasks** | Language Modeling,<br>Sequence Labeling,<br>Ranking,<br>Anomaly Detection,<br>Transfer Learning |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Transfer Learning,<br>Backpropagation,<br>Cross-Entropy,<br>Generative Learning,<br>Unsupervised Learning |
| **Performance Highlights** | loss_metric: cross-entropy (perplexity used for evaluation),<br>comparison: PlantCaduceus (state-space-based language model) performs as well as or better than prior alignment-based approaches; new SOTA baseline claimed (no numeric metrics reported in commentary) |
| **Application Domains** | Plant genomics,<br>Variant effect prediction / prioritization,<br>Annotation of noncoding genomic regions,<br>Comparative genomics / evolutionary constraint analysis,<br>Maize genetics (transfer application),<br>Arabidopsis functional site annotation |

---


### [306. Uni-Electrolyte: An Artificial Intelligence Platform for Designing Electrolyte Molecules for Rechargeable Batteries](https://doi.org/10.1002/ange.202503105), Angewandte Chemie *(July 21, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | DFT and MD database (electrolyte dataset),<br>Embedded electrolyte database (EMolCurator),<br>LiBE dataset (entire LiBE electrolyte database),<br>USPTO chemical reaction database (pretraining data),<br>QM9 (pretraining reference for Uni-Mol baseline),<br>Reaxys reactions subset (augmentation) |
| **Models** | Graph Neural Network,<br>Message Passing Neural Network,<br>Diffusion Model,<br>Denoising Diffusion Probabilistic Model,<br>Encoder-Decoder,<br>Ensemble (EMol-QSPR) |
| **Tasks** | Regression,<br>Data Generation,<br>Information Retrieval,<br>Structured Prediction,<br>Graph Generation |
| **Learning Methods** | Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Ensemble Learning,<br>Representation Learning,<br>Transfer Learning |
| **Performance Highlights** | LEFTNet_OOD_MAE_dielectric_constant: 3.27,<br>LEFTNet_OOD_MAE_viscosity_mPa_s: 12.97,<br>G2GT_relative_error_vs_UniMol: 0.97,<br>UniMol_relative_error_baseline: 1.0,<br>G2GT_OOD_MAE_dielectric_constant: 3.31,<br>G2GT_OOD_MAE_viscosity_mPa_s: 13.28,<br>EMol-QSPR_relative_error_vs_baseline: 0.94,<br>EMol-QSPR_OOD_MAE_dielectric_constant: 3.17,<br>EMol-QSPR_OOD_MAE_viscosity_mPa_s: 12.83,<br>improvement_over_G2GT_percent: 3,<br>improvement_over_LEFTNet_percent: 4.4,<br>qualitative_performance: EDM outperformed cG-Schnet in targeted HOMO–LUMO generation; successfully generated molecules including DME in sparse region,<br>cG-LEFTNet_vs_cG-Schnet_Task2: comparable,<br>cG-LEFTNet_vs_cG-Schnet_Task3: superior,<br>G2GT_Top1_accuracy_one-step_retrosynthesis: 0.529,<br>Askcos_Top1_accuracy_one-step_retrosynthesis: 0.452,<br>G2GT-Askcos_number_of_retrosynthetic_routes_found: 17,<br>ASKCOS_number_of_routes_found: 8,<br>example_Gibbs_free_energy_ring_opening_eV: -4.98 |
| **Application Domains** | Rechargeable batteries (lithium-ion and lithium metal batteries),<br>Electrolyte molecular design,<br>Electrochemistry and interfacial (SEI) chemistry,<br>Computational chemistry / materials discovery,<br>Retrosynthetic planning for chemical synthesis |

---


### [305. AutoMAT: A Hierarchical Framework for Autonomous Alloy Discovery](https://doi.org/10.48550/arXiv.2507.16005), Preprint *(July 21, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Properties Handbook: Titanium Alloys (handbook chapters parsed by LLM),<br>TCHEA7 thermodynamic database (used via Thermo-Calc),<br>Simulated candidate composition pools (AutoMAT-generated),<br>Experimental dataset: as-cast titanium alloys (this work),<br>Experimental dataset: as-cast HEAs (Al-Co-Cr-Fe-Ni) (this work) |
| **Models** | GPT,<br>Transformer |
| **Tasks** | Information Retrieval,<br>Synthetic Data Generation,<br>Regression,<br>Optimization,<br>Ranking |
| **Learning Methods** | Pre-training,<br>Model-Based Learning |
| **Performance Highlights** | latency: minutes,<br>cost: less than US$1 (for the ideation query via GPT-4o API),<br>throughput_examples: LLM completed alloy system identification, handbook analysis, and suggested candidate (Ti-185) within minutes,<br>predicted_final_density: 4.355 g/cm3,<br>predicted_final_yield_strength: 927.08 MPa,<br>evaluation_rate: >1,000 compositions per day (multi-threaded execution),<br>candidate_pool_reduction_Ti_case: from >43,000 potential compositions to 3,161 for detailed evaluation,<br>time_reduction_Ti_case: reduced manual CALPHAD effort equivalent from ~2 years (100 compositions/day) to under a week,<br>experimental_final_density: 4.32 g/cm3 (measured),<br>experimental_final_yield_strength: 829 MPa (measured),<br>density_reduction_vs_reference: 8.1% lower density vs reference Ti-185,<br>specific_strength: 202 × 10^3 Pa·m^3/kg (reported high specific strength),<br>predicted_HEA_final_yield_strength: 906.64 MPa (CALPHAD-predicted),<br>predicted_HEA_improvement_pct: 70.4% predicted improvement over initial HEA candidate (532.11 MPa → 906.64 MPa),<br>experimental_HEA_yield_strength_initial: 305 MPa (measured baseline),<br>experimental_HEA_yield_strength_optimized: 397 MPa (measured) — reported up to 28.2% improvement,<br>density_change_HEA: experimental density decreased from 7.33 g/cm3 to 7.17 g/cm3 |
| **Application Domains** | Materials science / alloy discovery,<br>Metallurgy (titanium alloys, high-entropy alloys),<br>Aerospace structural materials,<br>Automotive structural materials,<br>Biomedical (general potential — ideation layer applicability mentioned) |

---


### [304. DiffuMeta: Algebraic Language Models for Inverse Design of Metamaterials via Diffusion Transformers](https://doi.org/10.48550/arXiv.2507.15753), Preprint *(July 21, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Implicit-equation shell metamaterials dataset (D = {(Ψ(n), σ(n), C(n)) : n = 1 ... N}),<br>Evaluation sampling set (unconditional sampling),<br>Fabricated experimental samples |
| **Models** | Denoising Diffusion Probabilistic Model,<br>Transformer,<br>Attention Mechanism,<br>Self-Attention Network,<br>Multi-Head Attention |
| **Tasks** | Synthetic Data Generation,<br>Data Generation,<br>Optimization,<br>Representation Learning,<br>Regression |
| **Learning Methods** | Self-Supervised Learning,<br>End-to-End Learning,<br>Representation Learning,<br>Multi-Task Learning |
| **Performance Highlights** | validity: 74.0%,<br>novelty: 100%,<br>uniqueness: 100%,<br>NRMSE_in-distribution_examples: 3.3%–3.6% (Fig.3a), 4.6% (best in Fig.3b) with variations up to 8.5%,<br>comparison_best_training_match: 5.0% (closest existing design in training for Fig.3a), 7.7% (best match in training for Fig.3b),<br>NRMSE_multi_target_examples: 4.7%–10.4% (for combined stress-strain + targeted Poisson's ratios ν32 = -3.0, 0.0, 1.0),<br>training_best_match_NRMSE: 11.2% (closest in training for the multi-target example),<br>NRMSE_unseen_extreme_example: 2.2%–3.5% (for an unseen stress-strain + ν32 = -3.0 target that lies outside training distribution),<br>training_best_match_NRMSE_for_unseen: 22.1% (best training candidate for that unseen target),<br>NRMSE_unseen_cases: 7.2% and 7.0% (two unseen highly-nonlinear targets in Fig.4),<br>training_best_match_NRMSE: 22.1% and 24.1% (best matches from training dataset for those targets),<br>experimental_vs_FE_agreement: Qualitative/good agreement reported (no single aggregate numeric metric provided); representative stress-strain curves match key features (plateau, buckling-induced softening, contact-induced hardening) |
| **Application Domains** | Mechanical metamaterials / architected materials,<br>Metamaterial inverse design,<br>Materials discovery,<br>Additive manufacturing / 3D printing (fabrication and experimental validation),<br>Soft robotics (application example),<br>Energy-absorbing components (application example),<br>Protective gear / biomedical implants (multi-target design use-cases) |

---


### [302. Generative AI enables medical image segmentation in ultra low-data regimes](https://doi.org/10.1038/s41467-025-61754-6), Nature Communications *(July 14, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | ISIC2018 (ISIC),<br>PH2,<br>DermIS,<br>DermQuest,<br>JSRT,<br>NLM-MC,<br>NLM-SZ,<br>COVID-QU-Ex,<br>FPD,<br>FetReg,<br>KVASIR,<br>CVC-ClinicDB,<br>BUID,<br>FUSeg,<br>ICFluid,<br>ETAB,<br>MSD Task04 (Hippocampus),<br>MSD Task03 (Liver) |
| **Models** | U-Net,<br>Generative Adversarial Network,<br>Conditional GAN,<br>Variational Autoencoder,<br>Diffusion Model,<br>Denoising Diffusion Probabilistic Model,<br>Swin Transformer,<br>Neural Architecture Search |
| **Tasks** | Semantic Segmentation,<br>Semantic Segmentation,<br>Semantic Segmentation |
| **Learning Methods** | Adversarial Training,<br>End-to-End Learning,<br>Supervised Learning,<br>Semi-Supervised Learning,<br>Neural Architecture Search,<br>Gradient Descent |
| **Performance Highlights** | Jaccard_DermIS_GenSeg-UNet: 0.65,<br>Jaccard_DermIS_UNet_baseline: 0.41,<br>Jaccard_PH2_GenSeg-UNet: 0.77,<br>Jaccard_PH2_UNet_baseline: 0.56,<br>Dice_NLM-MC_GenSeg-UNet: 0.86,<br>Dice_NLM-MC_UNet_baseline: 0.77,<br>Dice_NLM-SZ_GenSeg-UNet: 0.93,<br>Dice_NLM-SZ_UNet_baseline: 0.82,<br>Jaccard_ISIC_GenSeg-SwinUnet: 0.62,<br>Jaccard_ISIC_SwinUnet_baseline: 0.55,<br>Jaccard_PH2_GenSeg-SwinUnet: 0.65,<br>Jaccard_PH2_SwinUnet_baseline: 0.56,<br>Jaccard_DermIS_GenSeg-SwinUnet: 0.62,<br>Jaccard_DermIS_SwinUnet_baseline: 0.38,<br>Absolute_gain_GenSeg-UNet_placental_vessels: 15%,<br>Absolute_gain_GenSeg-UNet_skin_lesions: 9.6%,<br>Absolute_gain_GenSeg-UNet_polyps: 11%,<br>Absolute_gain_GenSeg-UNet_intraretinal_cystoid: 6.9%,<br>Absolute_gain_GenSeg-UNet_foot_ulcers: 19%,<br>Absolute_gain_GenSeg-UNet_breast_cancer: 12.6%,<br>Placental_vessel_Dice_GenSeg-DeepLab_with_50_examples: 0.51 (GenSeg-DeepLab achieved similar to DeepLab with 500 examples per paper text),<br>Foot_ulcer_Dice_GenSeg-UNet_with_50_examples: 0.6 (approx as reported contextual example; GenSeg-UNet required 50 vs UNet 600 to reach ~0.6),<br>Lung_Dice_GenSeg-UNet_with_9_examples: 0.97 (paper states achieving Dice 0.97 required 175 examples for UNet, whereas GenSeg-UNet needed just 9 examples),<br>Comparison_BBDM_End2End_vs_Pix2Pix_End2End: BBDM (End2End) achieved the highest performance across datasets; Pix2Pix (End2End) and Soft-Intro VAE (End2End) comparable but slightly lower,<br>Computational_cost_BBDM_vs_Pix2Pix: BBDM incurs significantly higher computational cost and larger model size compared to Pix2Pix |
| **Application Domains** | Medical image analysis / medical imaging,<br>Dermatology (skin lesion segmentation from dermoscopy),<br>Pulmonology / Radiology (lung segmentation from chest X-ray),<br>Obstetrics / Fetoscopy (placental vessel segmentation),<br>Gastroenterology (polyp segmentation from colonoscopy),<br>Wound care / Dermatology (foot ulcer segmentation),<br>Ophthalmology (intraretinal cystoid fluid segmentation from OCT),<br>Cardiology (left ventricle and myocardial wall segmentation from echocardiography),<br>Breast imaging / Oncology (breast cancer segmentation from ultrasound),<br>Neuroimaging and abdominal imaging (3D hippocampus and liver segmentation from MR/CT) |

---


### [301. La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching](https://doi.org/10.48550/arXiv.2507.09466), Preprint *(July 13, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Foldseek Clustered AFDB (filtered),<br>Custom AFDB subset for long-length training,<br>PDB (reference set samples),<br>AFDB reference subsets for evaluation |
| **Models** | Variational Autoencoder,<br>Transformer,<br>Normalizing Flow,<br>Diffusion Model,<br>Encoder-Decoder,<br>Autoencoder |
| **Tasks** | Synthetic Data Generation,<br>Data Generation,<br>Structured Prediction,<br>Regression,<br>Multi-class Classification,<br>Clustering,<br>Synthetic Data Generation |
| **Learning Methods** | Unsupervised Learning,<br>Generative Learning,<br>Pre-training,<br>Representation Learning,<br>Batch Learning |
| **Performance Highlights** | sequence_recovery_rate: 1.0 (perfect),<br>average_all-atom_RMSD_reconstruction: ≈0.12 Å,<br>co-designability_all-atom_%: 68.4,<br>co-designability_alpha-carbon_%: 72.2,<br>diversity_structure_clusters: 206,<br>diversity_sequence_clusters: 216,<br>diversity_str+seq_clusters: 301,<br>novelty_TMScore_PDB: 0.75,<br>novelty_TMScore_AFDB: 0.82,<br>designability_MPNN-8_%: 93.8,<br>designability_MPNN-1_%: 82.6,<br>secondary_alpha_%: 72,<br>secondary_beta_%: 5,<br>co-designability_all-atom_%: 75.0,<br>co-designability_alpha-carbon_%: 78.2,<br>diversity_structure_clusters: 129,<br>diversity_sequence_clusters: 199,<br>diversity_str+seq_clusters: 247,<br>novelty_TMScore_PDB: 0.82,<br>novelty_TMScore_AFDB: 0.86,<br>designability_MPNN-8_%: 94.6,<br>designability_MPNN-1_%: 84.6,<br>secondary_alpha_%: 73,<br>secondary_beta_%: 6,<br>La-Proteina (η_x, η_z)=(0.2,0.1) co-designability_all-atom_%: 60.6,<br>La-Proteina (η_x, η_z)=(0.3,0.1) co-designability_all-atom_%: 53.8,<br>overall_unconditional_generation_up_to_length: co-designable proteins up to 800 residues (La-Proteina remains viable where baselines collapse),<br>co-designability_%: 21.2,<br>diversity_all-atom_clusters: 51,<br>diversity_seq_clusters: 105,<br>diversity_str+seq_clusters: 91,<br>KL=1e-3 co-designability_%: 65.2,<br>KL=1e-4 co-designability_%: 83.8,<br>KL=1e-5 co-designability_%: 82.4,<br>best_co-designability_% (exp/quad scheduling, 0.1/0.1 noise): 68.4,<br>other viable combinations co-designability_%: [60.6, 57.4, 59.2, 57.0, 54.0, 52.4, 50.6, 53.6, 55.4],<br>tasks_solved_by_La-Proteina_out_of_26: 21-25 (depending on setup: all-atom vs tip-atom, indexed vs unindexed),<br>tasks_solved_by_Protpardelle_out_of_26: 4,<br>inference_time_batchsize1_seconds_length100: 2.94,<br>inference_time_batchsize1_seconds_length200: 3.0,<br>inference_time_batchsize1_seconds_length300: 3.67,<br>inference_time_batchsize1_seconds_length400: 4.75,<br>inference_time_batchsize1_seconds_length500: 6.33,<br>inference_time_batchsize1_seconds_length600: 8.45,<br>inference_time_batchsize1_seconds_length700: 10.63,<br>inference_time_batchsize1_seconds_length800: 13.52,<br>max_batch_inference_time_per_sample_length100: 0.34,<br>max_batch_inference_time_per_sample_length800: 12.31 |
| **Application Domains** | Protein design / computational structural biology,<br>De novo protein structure generation,<br>Atomistic motif scaffolding (enzyme active site design, binder design implications),<br>Biophysical / structural validation (rotamer modeling, MolProbity assessment) |

---


### [300. AlphaGenome: advancing regulatory variant effect prediction with a unified DNA sequence model](https://doi.org/10.1101/2025.06.25.661532), Preprint *(July 11, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | ENCODE,<br>GTEx (via RECOUNT3),<br>FANTOM5 (CAGE),<br>4D Nucleome (contact maps / Hi-C / Micro-C),<br>PolyA_DB / Polyadenylation annotations,<br>ClinVar,<br>MFASS (Multiplexed Functional Assay of Splicing using Sort-seq),<br>CAGI5 MPRA saturation mutagenesis challenge,<br>Open Targets (GWAS credible sets),<br>eQTL Catalog / SuSiE fine-mapped eQTLs,<br>ChromBPNet benchmarks (caQTL/dsQTL/bQTL),<br>ENCODE-rE2G (CRISPRi enhancer-gene validation),<br>gnomAD common variants (chr22 subset) |
| **Models** | Transformer,<br>U-Net,<br>Convolutional Neural Network,<br>Multi-Layer Perceptron,<br>Multi-Head Attention,<br>Self-Attention Network |
| **Tasks** | Regression,<br>Binary Classification,<br>Sequence Labeling,<br>Structured Prediction,<br>Link Prediction,<br>Ranking,<br>Feature Extraction / Representation Learning |
| **Learning Methods** | Supervised Learning,<br>Pre-training,<br>Knowledge Distillation,<br>Ensemble Learning,<br>Multi-Task Learning,<br>Fine-Tuning,<br>Representation Learning,<br>Batch Learning,<br>Gradient Descent |
| **Performance Highlights** | genome_track_evaluations_outperform_count: AlphaGenome outperformed external models on 22 out of 24 genome track evaluations,<br>variant_effect_evaluations_outperform_count: AlphaGenome matched or outperformed external models on 24 out of 26 variant effect prediction evaluations,<br>gene_expression_LFC_rel_improvement_vs_Borzoi: +17.4% (relative improvement in cell type-specific gene-level expression LFC Pearson r vs Borzoi),<br>contact_maps_rel_improvement_vs_Orca_Pearson_r: +6.3% (Pearson r), +42.3% (cell type-specific differences),<br>ProCapNet_rel_improvement_total_counts_Pearson_r: +15% (vs ProCapNet),<br>ChromBPNet_rel_improvement_accessibility_Pearson_r: +8% ATAC, +19% DNase (total counts Pearson r),<br>splice_benchmarks_SOTA: AlphaGenome achieves SOTA on 6 out of 7 splicing VEP benchmarks,<br>ClinVar_deep_intronic_auPRC: 0.66 (AlphaGenome composite) vs 0.64 (Pangolin),<br>ClinVar_splice_region_auPRC: 0.57 (AlphaGenome) vs 0.55 (Pangolin),<br>ClinVar_missense_auPRC: 0.18 (AlphaGenome) vs 0.16 (DeltaSplice/Pangolin/DeltaSplice),<br>MFASS_auPRC: 0.54 (AlphaGenome) vs 0.51 (Pangolin); SpliceAI/DeltaSplice = 0.49,<br>Junctions_prediction_Pearson_r_examples: High correlations reported for junction counts across tissues (e.g., Pearson r ~0.75-0.76 in examples),<br>contact_map_Pearson_r_vs_Orca: +6.3% Pearson r improvement; cell type differential prediction improvement +42.3% (compared to Orca),<br>contact_map_examples_Pearson_r_values: Example intervals: AlphaGenome Pearson r ~0.79-0.86 vs ground truth maps (figure examples),<br>zero_shot_causality_auROC_comparable_to_Borzoi: AlphaGenome zero-shot causality comparable to Borzoi (mean auROC ~0.68),<br>supervised_RF_auROC: Random Forest using AlphaGenome multimodal features improved mean auROC from 0.68 (zero-shot) to 0.75, surpassing Borzoi supervised performance (mean auROC 0.71),<br>zero_shot_cell_type_matched_DNase_Pearson_r: 0.57 (AlphaGenome cell type-matched DNase predictions; comparable to ChromBPNet and Borzoi Ensemble),<br>LASSO_multi-celltype_DNase_Pearson_r: 0.63 (AlphaGenome with LASSO aggregation over all cell types),<br>LASSO_multimodal_Pearson_r: 0.65 (AlphaGenome integrating multiple modalities across cell types; SOTA on CAGI5 reported),<br>ENCODE-rE2G_zero_shot_auPRC: AlphaGenome outperformed Borzoi in identifying validated enhancer-gene links, particularly beyond 10 kb distance; zero-shot within 1% auPRC of ENCODE-rE2G-extended trained model,<br>supervised_integration_auPRC_improvement: Including AlphaGenome features into ENCODE-rE2G-extended model increased auPRC to new SOTA across distance bins (Fig.4j),<br>APA_Spearman_r: 0.894 (AlphaGenome) vs 0.790 (Borzoi) for APA prediction; reported as SOTA,<br>paQTL_auPRC_within_10kb: 0.629 (AlphaGenome) vs 0.621 (Borzoi),<br>paQTL_auPRC_proximal_50bp: 0.762 (AlphaGenome) vs 0.727 (Borzoi),<br>caQTL_African_coefficient_Pearson_r: 0.74 (AlphaGenome predicted vs observed effect sizes for causal caQTLs; DNase GM12878 track example),<br>SPI1_bQTL_coefficient_Pearson_r: 0.55 (AlphaGenome predicted vs observed SPI1 bQTLs),<br>caQTL_causality_AP_mean: AlphaGenome achieved higher Average Precision vs Borzoi and ChromBPNet across multiple ancestries and datasets (specific AP values shown in Supplementary/Extended Data; e.g., AP = 0.50-0.63 depending on dataset),<br>inference_speed: <1 second per variant on NVIDIA H100 (single student model), enabling fast large-scale scoring,<br>overall_variant_benchmarks_outperform_count: Matched or outperformed external SOTA on 24/26 variant effect prediction benchmarks (Fig.1e) |
| **Application Domains** | Regulatory genomics,<br>Variant effect prediction / clinical variant interpretation,<br>Splicing biology and splicing variant interpretation,<br>Gene expression regulation and eQTL interpretation,<br>Alternative polyadenylation (APA) and paQTLs,<br>Chromatin accessibility and TF binding QTL analysis,<br>3D genome architecture (contact map prediction),<br>Enhancer–gene linking and functional genomics perturbation interpretation,<br>Massively parallel reporter assay (MPRA) analysis,<br>GWAS interpretation and prioritization |

---


### [298. Artificial Intelligence Paradigms for Next-Generation Metal–Organic Framework Research](https://doi.org/10.1021/jacs.5c08214), Journal of the American Chemical Society *(July 09, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Cambridge Structural Database (CSD) - MOF subset,<br>Trillions of hypothetical MOF structures (unnamed hypothetical databases),<br>CoRE MOF (Computation-ready, experimental MOF database),<br>QMOF,<br>MOFkey,<br>DigiMOF,<br>MOSAEC-DB,<br>ARC-MOF,<br>MOFX-DB,<br>OpenDAC2023 dataset,<br>ImageNet,<br>MOFSimplify dataset (stability data) |
| **Models** | Transformer,<br>Multi-Layer Perceptron,<br>Recurrent Neural Network,<br>Graph Neural Network,<br>Graph Convolutional Network,<br>Message Passing Neural Network,<br>Convolutional Neural Network,<br>Generative Adversarial Network,<br>Variational Autoencoder,<br>Gaussian Process,<br>Attention Mechanism |
| **Tasks** | Regression,<br>Node Classification,<br>Graph Generation,<br>Text Summarization,<br>Text Classification,<br>Information Retrieval,<br>Hyperparameter Optimization,<br>Feature Extraction,<br>Representation Learning |
| **Learning Methods** | Supervised Learning,<br>Self-Supervised Learning,<br>Transfer Learning,<br>Fine-Tuning,<br>Reinforcement Learning,<br>Unsupervised Learning,<br>Representation Learning,<br>Embedding Learning,<br>Incremental Learning,<br>Few-Shot Learning |
| **Performance Highlights** | accuracy_description: high-fidelity partial charge assignment,<br>runtime: orders of magnitude shorter runtime compared to DFT,<br>application: virtual screening for toluene vapor adsorption,<br>hyperparameter_optimization_method: Bayesian optimization (mentioned),<br>qualitative: high-accuracy gas adsorption predictions reported in transformer-based approaches (refs),<br>examples: MOFormer (self-supervised), MOF-Transformer, multi-modal pre-training transformer for universal transfer learning,<br>application: prediction of adsorption properties via 3D voxelized potential-energy-surfaces and nanoporous material recognition,<br>qualitative: used for inverse design (property-to-structure) enabling targeted materials design,<br>contextual_note: no numeric performance metrics reported in text,<br>qualitative: facilitates inverse design; no explicit quantitative metrics reported in the perspective,<br>qualitative: Gaussian regression or kernel methods presented as approaches to map atomic positions to potential energy surfaces within ML potentials context,<br>application: MOF-GRU predicted gas separation performance (ref.72),<br>quantitative_metrics: not provided in text,<br>application: interpretable graph transformer network for predicting adsorption isotherms of MOFs (Ref.77),<br>quantitative_metrics: not stated in this perspective,<br>accuracy_description: near-quantum mechanical accuracy (qualitative),<br>scaling: enable simulation of experimental-size MOF membranes (up to 28.2 × 28.2 nm^2) with high quality,<br>application_example: predicted adsorption isotherm at 77 K via Grand Canonical Monte Carlo in excellent agreement with experimental data (case study) |
| **Application Domains** | metal-organic frameworks (MOF) materials discovery,<br>gas storage and separation (CO2 capture, methane, hydrogen, toluene vapor adsorption),<br>direct air capture (DAC) and sorbent discovery,<br>catalysis,<br>drug delivery / biomedical MOFs,<br>water purification,<br>environmental remediation,<br>renewable energy and energy storage (batteries),<br>materials synthesis optimization and autonomous laboratories,<br>computational materials databases and information retrieval |

---


### [297. Accelerated data-driven materials science with the Materials Project](https://doi.org/10.1038/s41563-025-02272-0), Nature Materials *(July 03, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project (MP) main database,<br>Matbench suite,<br>Open Catalyst 2020 (OC20),<br>Electronic charge density database (representation-independent),<br>X-ray absorption spectra (XANES / EXAFS) dataset in MP,<br>Ab initio non-crystalline structure database |
| **Models** | Graph Neural Network,<br>Message Passing Neural Network,<br>Random Forest,<br>Attention Mechanism,<br>Graph Neural Network |
| **Tasks** | Regression,<br>Classification,<br>Density Estimation,<br>Graph Generation |
| **Learning Methods** | Supervised Learning,<br>Semi-Supervised Learning,<br>Unsupervised Learning,<br>Active Learning,<br>Pre-training,<br>Contrastive Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | materials science (inorganic crystals),<br>energy storage / batteries (cathodes, solid electrolytes),<br>catalysis / surface chemistry,<br>optoelectronics / transparent conductors,<br>phosphors / lighting materials,<br>thermoelectrics,<br>piezoelectrics,<br>magnetocalorics,<br>carbon capture materials,<br>quantum materials / 2D systems / topological materials,<br>non-crystalline / amorphous materials |

---


### [296. Natural-Language-Interfaced Robotic Synthesis for AI-Copilot-Assisted Exploration of Inorganic Materials](https://doi.org/10.1021/jacs.5c05916), Journal of the American Chemical Society *(July 02, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | benchmark dataset of inorganic synthesis examples,<br>solution-based inorganic materials synthesis procedures dataset,<br>material characterization / crystallographic data |
| **Models** | GPT,<br>Transformer |
| **Tasks** | Sequence-to-Sequence,<br>Language Modeling,<br>Information Retrieval,<br>Decision Making,<br>Planning,<br>Experimental Design,<br>Optimization |
| **Learning Methods** | Pre-training,<br>Prompt Learning,<br>In-Context Learning |
| **Performance Highlights** | syntax_validation_success_rate: ≈97%,<br>semantic_fidelity_success_rate: ≈86%,<br>iterations_for_syntax_validation: within three iterations,<br>predefined_operations_coverage: 80% of procedures in inorganic synthesis dataset (ref. 54),<br>materials_discovery_outcomes: synthesized 13 compounds across four classes; discovered new Mn−W clusters (Mn4W18, Mn4W8, Mn8W26, Mn57W42, new morphology of Mn72W48),<br>human-AI_interaction_rounds_for_Mn-W_exploration: 65 rounds,<br>new_structures_discovered: 4 structurally related new Mn−W clusters and 1 new morphology (Mn4W18, Mn4W8, Mn8W26, Mn57W42, new morphology of Mn72W48) |
| **Application Domains** | inorganic materials synthesis,<br>chemical synthesis automation,<br>robotic laboratory automation,<br>materials discovery (polyoxometalates, metal−organic frameworks, nanoparticles, coordination complexes),<br>experimental design and planning,<br>human–AI collaborative research |

---


### [295. Self-Evolving Discovery of Carrier Biomaterials with Ultra-Low Nonspecific Protein Adsorption for Single Cell Analysis](https://doi.org/10.1002/adma.202506243), Advanced Materials *(July 02, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Experimental protein adsorption dataset (polyacrylamide copolymer formulations),<br>Combinatorial formulation search space (design space),<br>RDKit monomer descriptor dataset (computed descriptors for monomers),<br>Detection sensitivity datasets (ELISA on plates and beads),<br>Algorithm training / testing runs (SEBO training & tests) |
| **Models** | Random Forest |
| **Tasks** | Optimization,<br>Experimental Design,<br>Regression,<br>Feature Selection,<br>Hyperparameter Optimization |
| **Learning Methods** | Supervised Learning,<br>Evolutionary Learning,<br>Ensemble Learning,<br>Hyperparameter Optimization |
| **Performance Highlights** | training_fitness_convergence: 0.1,<br>training_datapoints_used_for_parameter_evolution: 10 |
| **Application Domains** | single-cell analysis,<br>biomaterials discovery,<br>protein analysis (ELISA detection sensitivity),<br>microfluidics (bead carriers and plate carriers),<br>automated experiments / autonomous laboratory workflows,<br>materials science (copolymer discovery) |

---


### [294. El Agente: An autonomous agent for quantum chemistry](https://doi.org/10.1016/j.matt.2025.102263), Matter *(July 02, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | SST |
| **Models** | _None_ |
| **Tasks** | Sentiment Analysis |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | Natural Language Processing,<br>Sentiment Analysis |

---


### [293. Zero shot molecular generation via similarity kernels](https://doi.org/10.1038/s41467-025-60963-3), Nature Communications *(July 01, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | QM9,<br>SPICE,<br>GEOM-Drug,<br>SiMGen reference subsets (QM9-derived),<br>Generated structures / supporting dataset |
| **Models** | Denoising Diffusion Probabilistic Model,<br>Graph Neural Network,<br>Message Passing Neural Network,<br>Radial Basis Function Network,<br>Multi-Layer Perceptron |
| **Tasks** | Data Generation,<br>Graph Generation,<br>Conditional generation,<br>Synthetic Data Generation |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Transfer Learning,<br>Evolutionary Learning |
| **Performance Highlights** | training_data: trained on 80% of QM9 for 300 epochs,<br>model_hyperparams: cutoff=10 Å, 16 radial basis functions, 2 interaction layers, 64 channels, message equivariance L=1, correlation ν=3,<br>reference_efficiency: SiMGen can match trained-model performance using just 256 reference molecules (see SiMGen / 256 row in Table 1),<br>MACE_pretraining_data: SPICE dataset (1 million molecules),<br>descriptor_cutoff: 5.0 Å,<br>MACE_descriptor_channels: first layer invariant scalar node features used,<br>penicillin_guidance_fold_change_β-lactam: ≈8× increase,<br>penicillin_guidance_fold_change_thiolane: ≈3× increase,<br>hydrogenation_validity_before: ≈99% valid atoms before hydrogen addition across sizes,<br>generation_scaling: SiMGen generated molecules with 5-50 heavy atoms; validity after hydrogenation decreases slowly with size; with open priors valid molecules ≈0.6 for 50 heavy atoms,<br>energy_based_time_encoding: time is positionally encoded and combined via MLP into node features for the energy-based MACE model |
| **Application Domains** | de novo molecular generation,<br>drug discovery (fragment-based design, linker design, binder design),<br>materials design / crystal structure generation (discussion and motivation),<br>molecular force-field modelling and force prediction,<br>interactive molecular design tools (ZnDraw web tool) |

---


### [292. Enabling large language models for real-world materials discovery](https://doi.org/10.1038/s42256-025-01058-y), Nature Machine Intelligence *(July 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Battery Device QA,<br>MaScQA,<br>MatSciNLP,<br>OpticalTable / OpticalTable-SQA,<br>SustainableConcrete,<br>NanoMine,<br>ChatExtract,<br>Structured Information Extraction,<br>MatText,<br>MaCBench,<br>LitQA,<br>RedPajama (training corpus reference),<br>Materials Project,<br>DPA-2 (large atomic model dataset/model),<br>Open Reaction Database,<br>Polymer nanocomposite data (ACS Macro Lett.) |
| **Models** | BERT,<br>GPT,<br>Transformer,<br>Vision Transformer,<br>Graph Neural Network,<br>Diffusion Model,<br>Denoising Diffusion Probabilistic Model,<br>Multi-Layer Perceptron |
| **Tasks** | Named Entity Recognition,<br>Question Answering,<br>Text Classification,<br>Sequence-to-Sequence,<br>Regression,<br>Image Generation,<br>Graph Generation,<br>Planning,<br>Decision Making,<br>Information Retrieval,<br>Data Generation |
| **Learning Methods** | Pre-training,<br>Fine-Tuning,<br>In-Context Learning,<br>Prompt Learning,<br>Transfer Learning,<br>Multi-Agent Learning,<br>Reinforcement Learning,<br>Representation Learning |
| **Performance Highlights** | questions_examined: 650 |
| **Application Domains** | Materials science (general),<br>Chemistry (adjacent domain and source of methods),<br>Batteries / energy materials,<br>Optical materials,<br>Concrete / civil materials,<br>Nanomaterials,<br>Computational materials science (in silico evaluation),<br>Automated experimentation / laboratory robotics,<br>Sustainable materials and manufacturing,<br>Bio-inspired materials / biological materials (briefly referenced) |

---


### [291. A framework for evaluating the chemical knowledge and reasoning abilities of large language models against the expertise of chemists](https://doi.org/10.1038/s41557-025-01815-x), Nature Chemistry *(July 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | ChemBench (full corpus),<br>ChemBench-Mini,<br>Chemical preference dataset (Choung et al. 2023) — sampled subset,<br>MoleculeNet (referenced),<br>Therapeutics Data Commons (referenced),<br>MatBench (referenced),<br>PubChem and Gestis (referenced as specialized databases) |
| **Models** | Transformer,<br>GPT |
| **Tasks** | Question Answering,<br>Multi-class Classification,<br>Text Generation,<br>Regression,<br>Classification,<br>Ranking,<br>Uncertainty / Confidence estimation |
| **Learning Methods** | Self-Supervised Learning,<br>Fine-Tuning,<br>Few-Shot Learning,<br>Prompt Learning,<br>Pre-training |
| **Performance Highlights** | fraction_correct: 0.71,<br>fraction_correct: 0.22,<br>alignment_with_experts: often indistinguishable from random guessing,<br>example_confidence_reported: GPT-4: reported 1.0 for one correct answer and 4.0 for six incorrect answers (on 1–5 scale),<br>note: PaperQA2 (agentic system) included in evaluation; relative performance shown in Fig. 3 (no single-number reported in main text) |
| **Application Domains** | chemistry,<br>analytical chemistry,<br>organic chemistry,<br>inorganic chemistry,<br>physical chemistry,<br>technical chemistry,<br>materials science,<br>medicinal chemistry / drug discovery,<br>chemical safety / toxicity assessment,<br>chemical education / assessment |

---


### [290. A generalized platform for artificial intelligence-powered autonomous enzyme engineering](https://doi.org/10.1038/s41467-025-61209-y), Nature Communications *(July 01, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | AtHMT variant screening dataset (this study, Supplementary Data 3),<br>YmPhytase variant screening dataset (this study, Supplementary Data 4),<br>Initial variant libraries (this study),<br>ESM-2 pretraining corpus (referenced),<br>Multiple sequence alignments / homologous sequences used by EVmutation |
| **Models** | Transformer,<br>Markov Random Field,<br>GPT,<br>Supervised Learning |
| **Tasks** | Regression,<br>Data Generation,<br>Optimization,<br>Experimental Design,<br>Sequence-based prediction / zero-shot fitness prediction (mapped to Regression/Zero-Shot Learning) |
| **Learning Methods** | Unsupervised Learning,<br>Zero-Shot Learning,<br>Supervised Learning,<br>Self-Supervised Learning,<br>Prompt Learning,<br>Representation Learning |
| **Performance Highlights** | best_single_mutant_fold_change_AtHMT: 2.1,<br>best_single_mutant_fold_change_YmPhytase: 2.6,<br>initial_library_percent_variants_above_wt_AtHMT: 59.6%,<br>initial_library_percent_variants_above_wt_YmPhytase: 55%,<br>initial_library_percent_significantly_better_AtHMT: 50% (of above-wt?),<br>initial_library_percent_significantly_better_YmPhytase: 23% (two-tailed Student's t-test p<0.05),<br>relative_performance_vs_ESM2: EVmutation performed better overall than ESM-2 on initial predictions (qualitative statement),<br>overlap_between_models_predictions: substantial overlap (qualitative),<br>screened_variants_per_round_constructed: top 96 predicted mutants constructed (90 used for screening),<br>third_round_model_predicted_triple_mutants_better_than_V140T: 74/90 (82%),<br>human_intuited_S99T/V140T_triple_mutants_better_than_V140T: 4/36 (11%),<br>general_correlation_with_experimental_results: relatively weak / little overall consistency (qualitative) |
| **Application Domains** | Protein engineering,<br>Synthetic biology,<br>Biocatalysis,<br>Metabolic engineering,<br>Natural product discovery,<br>Biotechnology,<br>Medicine (enzyme applications),<br>Renewable energy and sustainable chemistry (industrial enzyme applications) |

---


### [289. Machine-learning design of ductile FeNiCoAlTa alloys with high strength](https://doi.org/10.1038/s41586-025-09160-2), Nature *(July 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Training dataset of FCC HEAs (AlCoCrFeNiTa system),<br>Experimental synthesis / validation dataset (iterative active-learning candidates),<br>Atom probe tomography / SAXS measurement datasets (characterization data) |
| **Models** | not specified (surrogate machine learning model) |
| **Tasks** | Regression,<br>Optimization,<br>Experimental Design,<br>Ranking |
| **Learning Methods** | Active Learning,<br>Supervised Learning |
| **Performance Highlights** | ML_prediction_accuracy: not reported (numerical ML metrics such as RMSE/R² not provided in main text),<br>experimental_outcome_yield_strength: 1.75 GPa ± 0.05 GPa (representative HEA05 after 750 °C for 1 h aging),<br>experimental_outcome_ultimate_tensile_strength: 2.403 GPa ± 0.046 GPa,<br>experimental_outcome_uniform_elongation: 25% ± 1.5%,<br>work_hardening_rate: > 2 GPa (stable across wide strain range),<br>true_stress_peak: ≈ 3 GPa (true stress),<br>performance_range_sigma_y_vs_eu: σy range 1.5–1.95 GPa with εu range 31%–15% reported across processing variants,<br>σUTS_minus_σy: 650 MPa (work hardening capability example),<br>yield_ratio_σy/σUTS: 0.73 (example),<br>σUTS_times_εf: ≈ 60 GPa% (representative) |
| **Application Domains** | Materials science,<br>Metallurgy,<br>High-entropy alloy design,<br>Mechanical engineering (structural materials) |

---


### [286. Large language models to accelerate organic chemistry synthesis](https://doi.org/10.1038/s42256-025-01066-y), Nature Machine Intelligence *(July 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | USPTO-50k,<br>Open Reaction Database (ORD),<br>Suzuki–Miyaura (HTE) dataset,<br>Imidazole C–H arylation (HTE) dataset,<br>Buchwald–Hartwig (ELN) dataset,<br>Regioselectivity dataset (Li et al.),<br>Enantioselectivity dataset (Zahrt et al.),<br>Pd-catalysed carbonylation literature dataset,<br>Curated Q&A training dataset (Chemma) |
| **Models** | Transformer,<br>GPT,<br>Seq2Seq,<br>Multi-Layer Perceptron,<br>Feedforward Neural Network,<br>Random Forest,<br>Graph Neural Network,<br>Gaussian Process |
| **Tasks** | Sequence-to-Sequence,<br>Retrosynthesis,<br>Forward prediction,<br>Condition generation,<br>Regression,<br>Recommendation,<br>Optimization,<br>Active Learning,<br>Selectivity (as regression),<br>Image/other tasks not applicable |
| **Learning Methods** | Supervised Learning,<br>Fine-Tuning,<br>Pre-training,<br>Reinforcement Learning,<br>In-Context Learning,<br>Zero-Shot Learning,<br>Active Learning,<br>Transfer Learning |
| **Performance Highlights** | top-1_accuracy: 72.2%,<br>Suzuki–Miyaura (example): R2 = 0.86, RMSE = 5.20%,<br>Suzuki–Miyaura (other splits): R2 = 0.85, RMSE = 5.40%,<br>Buchwald–Hartwig (ELN): R2 = 0.79, RMSE = 6.56%,<br>Imidazole C–H arylation: R2 = 0.74, RMSE = 6.59%,<br>Another reported value for imidazole C–H arylation (figure): R2 = 0.83, RMSE = 6.02%,<br>Buchwald–Hartwig (other figure): R2 = 0.81, RMSE = 5.51%,<br>Regioselectivity: R2 = 0.93, RMSE = 0.74 kcal mol−1, site_accuracy = 78.74%,<br>Enantioselectivity (chiral phosphoric acid catalysed thiol addition): R2 = 0.89, RMSE = 0.25 kcal mol−1 (Chemma) ; comparison: Li et al. reported R2 = 0.915, RMSE = 0.197 kcal mol−1,<br>Ligand_recommendation_median_performance: For 15 of the 16 base–solvent combinations, the recommended ligand performs best in terms of median reaction yields (paper summary statistic),<br>Chemma-enhanced_RF (5% real + generated data) Suzuki–Miyaura: R2 = 0.53,<br>Chemma-enhanced_RF (5% real + generated data) Buchwald–Hartwig: R2 = 0.72,<br>RF with 90% real data (reference): approx R2 = 0.6 (Suzuki) and 0.8 (Buchwald) as baseline reported in paper,<br>Optimization_speed_Suzuki–Miyaura: Chemma-BO achieves 98.5% yield within first 15 experiments (3 batches) vs BO and GPT-4 requiring ~50 experiments,<br>Optimization_speed_Buchwald–Hartwig: Chemma-BO achieves 98.7% within first 10 experiments; within first 25 experiments reaches 99.8% yield while BO requires at least 50 experiments |
| **Application Domains** | Organic chemistry synthesis,<br>Drug discovery (medicinal chemistry / synthesis planning),<br>Materials and energy (catalyst design and synthesis),<br>Automated / autonomous experimentation (robotic chemistry and HTE integration) |

---


### [285. UMA: A Family of Universal Models for Atoms](https://doi.org/10.48550/arXiv.2506.23971), Preprint *(June 30, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | OMat24,<br>OMol25 (OMol-preview used in main training),<br>OC20++ (OC20 All + MD + Rattled + clean surface + OC20-Multi-Adsorbate mAds),<br>OMC25,<br>ODAC25 (subset overlapping ODAC23),<br>Combined UMA training corpus,<br>MPTrj (fine-tuning),<br>sAlex (fine-tuning) |
| **Models** | Graph Neural Network,<br>Message Passing Neural Network,<br>Multi-Layer Perceptron,<br>Transformer |
| **Tasks** | Regression,<br>Classification,<br>Ranking,<br>Optimization,<br>Image/graph matching (mapped to Graph Matching) |
| **Learning Methods** | Supervised Learning,<br>Multi-Task Learning,<br>Pre-training,<br>Fine-Tuning,<br>Transfer Learning,<br>Representation Learning |
| **Performance Highlights** | Materials Energy/Atom (meV): 20.0,<br>Materials Forces (meV/Å): 60.8,<br>Materials Stress (meV/Å^3): 4.4,<br>Matbench F1: 0.916,<br>AdsorbML Success Rate: 68.35%,<br>OMol25 Ligand-strain MAE (meV): 4.39,<br>CSP Lattice Energy MAE (kJ/mol): 2.695,<br>Inference (1000 atoms) steps/sec: 16,<br>Materials Energy/Atom (meV): 18.1,<br>Materials Forces (meV/Å): 51.4,<br>Materials Stress (meV/Å^3): 4.3,<br>Matbench F1: 0.93,<br>AdsorbML Success Rate: 71.12%,<br>OMol25 Ligand-strain MAE (meV): 2.45,<br>CSP Lattice Energy MAE (kJ/mol): 2.664,<br>ODAC Test Ads. Energy (meV): 290.2,<br>Inference (1000 atoms) steps/sec: 3,<br>Materials Energy/Atom (meV): 17.6,<br>Materials Forces (meV/Å): 45.5,<br>Materials Stress (meV/Å^3): 3.8,<br>Matbench F1: 0.928,<br>AdsorbML Success Rate: 74.41% (25% improvement in successful adsorption energy calculations reported for catalysis vs previous SOTA),<br>OMol25 Ligand-strain MAE (meV): 3.37,<br>CSP Lattice Energy MAE (kJ/mol): 2.488,<br>ODAC Test Ads. Energy (meV): 291.1,<br>Inference (1000 atoms) steps/sec: 1.6,<br>MoLE vs Dense compute-optimal gain (∆): ≈2.5× fewer active parameters for MoLE to achieve equivalent loss (reported for UMA-M),<br>Validation loss behavior: MoLE models achieve lower validation loss at fixed FLOPs in experiments (Figures 3 & 4 discussed),<br>AdsorbML previous SOTA success rate (EquiformerV2): ≈60.80% (literature baseline),<br>UMA-L AdsorbML success rate: 74.41%,<br>Example baseline OC20 Ads. Energy Force MAEs (GemNet / eqv2 reported values): GemNet-OC20: Ads. Energy 163.5 meV, Forces 16.3 meV/Å (literature column),<br>UMA improvements: UMA models reduce OC20 adsorption energy errors by ~80% in some evaluations (paper statement) |
| **Application Domains** | computational chemistry,<br>materials science,<br>catalysis,<br>drug discovery / structure-based drug design,<br>energy storage (battery materials),<br>semiconductor materials,<br>molecular crystals / crystal structure prediction,<br>metal-organic frameworks (MOFs) and direct-air capture applications,<br>molecular dynamics simulations |

---


### [284. Rethinking chemical research in the age of large language models](https://doi.org/10.1038/s43588-025-00811-y), Nature Computational Science *(June 24, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | MoleculeNet,<br>Tox21,<br>ChemBench,<br>LAION-5B,<br>QM9,<br>Reaxys,<br>SciFinder,<br>Chatbot Arena (human preference data) |
| **Models** | Transformer,<br>GPT,<br>BERT,<br>Graph Neural Network,<br>Decision Tree,<br>Random Forest,<br>Autoencoder,<br>Variational Autoencoder,<br>Feedforward Neural Network,<br>Decision Transformer,<br>CLIP (implied),<br>ChemLLM (domain-specific LLM) |
| **Tasks** | Planning,<br>Optimization,<br>Question Answering,<br>Sequence-to-Sequence,<br>Classification,<br>Regression,<br>Language Modeling,<br>Text Generation |
| **Learning Methods** | Pre-training,<br>Fine-Tuning,<br>Reinforcement Learning,<br>Transfer Learning,<br>Prompt Learning,<br>Knowledge Distillation,<br>Self-Supervised Learning,<br>Model-Based Reinforcement Learning (implied) |
| **Performance Highlights** | accuracy_Tox21: >80%,<br>accuracy_translation: <10%,<br>accuracy_yield_general: 70-80%,<br>accuracy_yield_custom_methods: >95%,<br>accuracy_reaction_product_general: ~20%,<br>accuracy_reaction_product_bespoke: >90% |
| **Application Domains** | Chemistry,<br>Chemical engineering,<br>Materials science,<br>Medicinal chemistry / drug discovery,<br>Analytical chemistry (spectroscopy, NMR, mass spectrometry, IR),<br>Automated experimentation / cloud labs / robotic laboratories,<br>Knowledge management and ontology / knowledge graphs in scientific domains |

---


### [283. Agent-based multimodal information extraction for nanomaterials](https://doi.org/10.1038/s41524-025-01674-7), npj Computational Materials *(June 23, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | DiZyme nanomaterials subset (test),<br>DiZyme nanomaterials larger set (Jaccard evaluation),<br>DiZyme nanozyme dataset,<br>NER annotated corpus (training),<br>NER annotated corpus (test),<br>YOLO figure detection dataset |
| **Models** | GPT,<br>Transformer,<br>YOLO,<br>BERT |
| **Tasks** | Named Entity Recognition,<br>Object Detection,<br>Structured Prediction,<br>Sequence Labeling,<br>Information Retrieval |
| **Learning Methods** | Fine-Tuning,<br>Pre-training,<br>Zero-Shot Learning,<br>Gradient Descent,<br>End-to-End Learning |
| **Performance Highlights** | Mw(coating)_precision_text_only: 0.62,<br>Mw(coating)_precision_text+NER: 0.66,<br>Mw(coating)_recall_text_only: 0.73,<br>Mw(coating)_recall_text+NER: 0.86,<br>nanoMINER_avg_precision: 0.89,<br>nanoMINER_avg_recall: 0.72,<br>nanoMINER_F1: 0.79,<br>training_box_loss_end: 0.2,<br>training_classification_loss_end: 0.17,<br>trained_on_images: 537,<br>GPT-4.1_avg_precision: 0.71,<br>GPT-4.1_avg_recall: 0.65,<br>GPT-4.1_F1: 0.68,<br>o3-mini_avg_precision: 0.68,<br>o3-mini_avg_recall: 0.57,<br>o3-mini_F1: 0.62,<br>o4-mini_avg_precision: 0.78,<br>o4-mini_avg_recall: 0.69,<br>o4-mini_F1: 0.74,<br>crystal_system_inference_accuracy: 0.86,<br>Cmin_precision_text_only: 0.9,<br>Cmin_precision_text+vision: 0.97,<br>Cmax_precision_text_only: 0.91,<br>Cmax_precision_text+vision: 0.98,<br>Km_precision_overall: 0.97,<br>Vmax_precision_overall: 0.96,<br>pH_precision_overall: 0.89,<br>Temperature_precision_overall: 0.68,<br>Km_recall_range: 0.87-0.91,<br>Vmax_recall_range: 0.79-0.83,<br>concentration_recall_range: 0.38-0.54 |
| **Application Domains** | Materials science (nanomaterials),<br>Nanozymes / bionanotechnology,<br>Chemistry,<br>Biomedical data extraction (mentioned as extensible application),<br>Scientific literature mining / knowledge base construction |

---


### [282. All-atom Diffusion Transformers: Unified generative modelling of molecules and materials](https://doi.org/), International Conference on Machine Learning *(June 18, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | MP20,<br>QM9,<br>GEOM-DRUGS,<br>QMOF |
| **Models** | Variational Autoencoder,<br>Autoencoder,<br>Transformer,<br>Denoising Diffusion Probabilistic Model,<br>Autoencoder |
| **Tasks** | Data Generation,<br>Synthetic Data Generation,<br>Representation Learning,<br>Distribution Estimation |
| **Learning Methods** | Unsupervised Learning,<br>Self-Supervised Learning,<br>Representation Learning,<br>Transfer Learning,<br>Batch Learning |
| **Performance Highlights** | MP20_joint_transformer_match_rate (%): 88.6,<br>MP20_joint_transformer_RMSD (Å): 0.0239,<br>QM9_joint_transformer_match_rate (%): 97.00,<br>QM9_joint_transformer_RMSD (Å): 0.0399,<br>MP20_structure_validity (%): 99.74,<br>MP20_compositional_validity (%): 92.14,<br>MP20_overall_validity (%): 91.92,<br>MP20_metastable_rate (%): 81.0,<br>MP20_stable_rate (%): 15.4,<br>MP20_M.S.U.N. (%): 28.2,<br>MP20_S.U.N. (%): 5.3,<br>QM9_validity (%): 97.43,<br>QM9_uniqueness (%): 96.92,<br>PoseBusters_atoms_connected (%): 99.70,<br>PoseBusters_bond_angles (%): 99.85,<br>PoseBusters_bond_lengths (%): 99.41,<br>PoseBusters_ring_flat (%): 100.00,<br>PoseBusters_double_bond_flat (%): 99.98,<br>PoseBusters_internal_energy (%): 95.86,<br>PoseBusters_no_steric_clash (%): 99.79,<br>GEOM-DRUGS_validity (%): 95.3,<br>GEOM-DRUGS_uniqueness (%): 100.0,<br>PoseBusters_valid (%): 85.3,<br>PoseBusters_atoms_connected (%): 93.0,<br>PoseBusters_ring_flat (%): 95.4,<br>DiT-S_params: 32M,<br>DiT-B_params: 130M (150M reported in some configs),<br>DiT-L_params: 450M,<br>Correlation_training_loss_vs_params_Pearson_at_epoch2000: -1.00,<br>Correlation_crystal_validity_vs_params_Pearson: 0.91,<br>Correlation_molecule_validity_vs_params_Pearson: 0.94,<br>QMOF_only_validity_rate (%): 15.7,<br>Joint_QMOF_validity_rate (%): 10.2,<br>time_to_sample_10k_on_V100: under 20 minutes (ADiT reported),<br>baseline_time_examples: equivariant baselines up to 2.5 hours on same hardware,<br>speedup: Order-of-magnitude faster than equivariant diffusion baselines for 10k samples on single V100 |
| **Application Domains** | Generative chemistry,<br>Materials design / inorganic crystals,<br>Small-molecule design (drug-like molecules),<br>Metal-organic frameworks (MOFs),<br>Foundation models for atomic-scale structure generation |

---


### [281. Data-Driven Design of Random Heteropolypeptides as Synthetic Polyclonal Antibodies](https://doi.org/10.1021/jacs.5c06240), Journal of the American Chemical Society *(June 18, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | RHP library for IFN campaign (384 randomly sampled candidates, 6 optimization iterations),<br>RHP library for TNF-α campaign (524 randomly sampled candidates, 6 optimization iterations),<br>ELISA measurement dataset (Target and Control signals used to compute Target, Control, and composite Score),<br>BLI validation dataset (biolayer interferometry KD measurements),<br>Functional neutralization assay dataset (L929 cell cytotoxicity, IC50 measurements),<br>LP-EM single-molecule imaging dataset and MD simulation outputs |
| **Models** | Linear Model,<br>Gaussian Process,<br>Multi-Model (surrogate + evolutionary) |
| **Tasks** | Regression,<br>Optimization,<br>Experimental Design,<br>Ranking |
| **Learning Methods** | Supervised Learning,<br>Evolutionary Learning,<br>Model-Based Learning |
| **Performance Highlights** | R2_Target_test: 0.7,<br>R2_Control_test: 0.75,<br>R2_Score_test: 0.25,<br>R2_Target_train: 0.96,<br>R2_Control_train: 0.97,<br>R2_Score_train: 0.69,<br>R2_Target_test: 0.89,<br>R2_Control_test: 0.9,<br>R2_Score_test: 0.4,<br>SpAb_T1_KD_TNF-alpha: 7.9 nM,<br>SpAb_T1_KD_HSA: 3.3 μM,<br>Selectivity_TNF-alpha_over_HSA: ≈418-fold,<br>SpAb_T1_KD_after_affinity_purification: <1.6 nM,<br>SpAb_T2_KD_TNF-alpha: 413 nM,<br>SpAb_T2_KD_HSA: 2.1 μM,<br>SpAb_I2_KD_IFN: 103 nM,<br>Neutralization_IC50_TNF-alpha_alone: 0.36 pg/mL,<br>Neutralization_IC50_TNF-alpha_plus_SpAb_T1: 70 pg/mL,<br>Neutralization_IC50_TNF-alpha_plus_RHP_T2: 0.37 pg/mL,<br>Neutralization_IC50_TNF-alpha_plus_purified_SpAb_T1: 272 pg/mL,<br>Neutralization_IC50_TNF-alpha_plus_anti-TNF-alpha_mAb: 1591 pg/mL |
| **Application Domains** | biomedicine,<br>therapeutics,<br>diagnostics,<br>polymer materials discovery,<br>biomolecular recognition,<br>drug development,<br>experimental automation / self-driving labs |

---


### [279. Agents for self-driving laboratories applied to quantum computing](https://doi.org/10.48550/arXiv.2412.07978), Preprint *(June 05, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Translation benchmark: 80 instructions from 8 experiments,<br>Visual inspection synthetic dataset (four experiment types),<br>LeeQ built-in experiments indexed by k-agents (benchmark & real runs),<br>Real hardware experiment logs — single-qubit calibration, two-qubit siZZle parameter search, GHZ tomography |
| **Models** | GPT,<br>Transformer,<br>Gaussian Mixture Model |
| **Tasks** | Text Generation,<br>Sequence-to-Sequence,<br>Binary Classification,<br>Image Classification,<br>Clustering,<br>Optimization,<br>Experimental Design |
| **Learning Methods** | Few-Shot Learning,<br>Zero-Shot Learning,<br>Fine-Tuning,<br>Prompt Learning,<br>Pre-training,<br>Representation Learning,<br>Ensemble Learning |
| **Performance Highlights** | translation_accuracy: 97%,<br>translation_accuracy_agent_based_GPT-4o(Agents): 99.17%,<br>translation_accuracy_GPT-4o(LongContext): 97.92%,<br>parameter_search_experiments_run: 100 experiments (3 hours), tested up to 20 frequencies,<br>discovered_parameters: frequency 4726 MHz, amplitude 0.3049 (successful set reported),<br>GHZ_state_fidelity: 83.83% |
| **Application Domains** | Quantum computing (superconducting quantum processors),<br>Laboratory automation / self-driving laboratories,<br>Scientific experiment execution and analysis (multimodal: text + images + code) |

---


### [278. SciAgents: Automating Scientific Discovery Through Bioinspired Multi-Agent Intelligent Graph Reasoning](https://doi.org/10.1002/adma.202413523), Advanced Materials *(June 05, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Ontological knowledge graph (global graph from prior work),<br>Semantic Scholar search results (publication abstracts returned per query) |
| **Models** | GPT,<br>Transformer |
| **Tasks** | Language Modeling,<br>Text Generation,<br>Question Answering,<br>Text Summarization,<br>Information Retrieval,<br>Novelty Detection,<br>Graph Generation,<br>Data Generation |
| **Learning Methods** | In-Context Learning,<br>Multi-Agent Learning,<br>Prompt Learning,<br>Pre-training,<br>Fine-Tuning,<br>Adversarial Training |
| **Performance Highlights** | document_length: 8100 words (example generated document),<br>tensile_strength_prediction_for_proposed_material: up to 1.5 GPa (compared to traditional 0.5–1.0 GPa),<br>energy_consumption_reduction: ~30% (projected for low-temperature processing),<br>novelty_score_examples: Idea 1: 8, Idea 2: 8, Idea 3: 6, Idea 4: 7, Idea 5: 8 (Novelty / Feasibility pairs reported in Table 4),<br>feasibility_score_examples: Idea 1: 7, Idea 2: 7, Idea 3: 8, Idea 4: 8, Idea 5: 7,<br>MD_simulation_protocol_duration: 100–500 ns (protocol suggested by Critic agent for MD runs),<br>MD_analysis_outputs: interaction energies, binding sites, cluster analysis of self-assembled structures (qualitative outputs suggested),<br>general_claim: foundation LLMs (Transformer-based) provide strong generative capabilities but face accuracy and explainability challenges (qualitative) |
| **Application Domains** | bio-inspired materials,<br>materials science,<br>biomaterials,<br>molecular modeling (molecular dynamics, DFT),<br>synthetic biology,<br>microfluidics,<br>bioelectronics,<br>generative materials informatics / scientific discovery automation,<br>scientific literature mining / information retrieval |

---


### [277. A data-driven platform for automated characterization of polymer electrolytes](https://doi.org/10.1016/j.matt.2025.102129), Matter *(June 04, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | PEO-salt polymer electrolyte dataset (this work) |
| **Models** | _None_ |
| **Tasks** | Regression,<br>Optimization,<br>Feature Extraction,<br>Feature Selection |
| **Learning Methods** | Supervised Learning,<br>Active Learning |
| **Performance Highlights** | dataset_size: 70 unique formulations; 330 samples; ~2,000 ionic conductivity measurements,<br>throughput: 67.5 samples per researcher hour (stated: "67.5 samples per researcher hour" / "over 60 samples per researcher hour"),<br>HT_experiment_capacity: 90 electrolyte samples processed from start to finish in just under 5 days (includes 24 h drying downtime),<br>measurement_precision_examples: in situ actuator thickness measurement inherent error < ±5 μm (quoted as ±5mm in text formatting), actuator position resolution 0.01 mm, force resolution 5 mN, temperature sensor 0.1 °C resolution |
| **Application Domains** | Battery materials / Electrolytes,<br>Polymer electrolyte characterization,<br>Materials science (experimental high-throughput data generation),<br>Sodium-ion and Lithium-ion battery research,<br>Data-driven materials discovery (enabling ML model training and optimization) |

---


### [276. IvoryOS: an interoperable web interface for orchestrating Python-based self-driving laboratories](https://doi.org/10.1038/s41467-025-60514-w), Nature Communications *(June 04, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | _None_ |
| **Models** | GPT,<br>Convolutional Neural Network,<br>Gaussian Process |
| **Tasks** | Optimization,<br>Image Matching,<br>Text Generation,<br>Hyperparameter Optimization |
| **Learning Methods** | Prompt Learning,<br>In-Context Learning,<br>Model-Based Learning,<br>Supervised Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | Chemistry (self-driving laboratories, automated experimentation),<br>Materials chemistry,<br>Drug discovery,<br>Formulation science,<br>Laboratory automation and robotics,<br>Analytical chemistry (HPLC, reaction monitoring),<br>Automated synthesis and purification workflows |

---


### [275. An unsupervised machine learning based approach to identify efficient spin-orbit torque materials](https://doi.org/10.1038/s41524-025-01626-1), npj Computational Materials *(June 03, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | APS and IEEE abstracts (1970-2020),<br>Literature reports of spin Hall conductivities and measured SOT efficiencies |
| **Models** | Feedforward Neural Network,<br>Transformer |
| **Tasks** | Representation Learning,<br>Embedding Learning,<br>Ranking,<br>Regression,<br>Clustering,<br>Dimensionality Reduction,<br>Information Retrieval,<br>Feature Extraction |
| **Learning Methods** | Unsupervised Learning,<br>Representation Learning,<br>Embedding Learning |
| **Performance Highlights** | training_corpus_size_abstracts: Approximately 1,000,000 abstracts,<br>embedding_dimension: 200,<br>embedding_model_training_time: ~20 hours on Intel Xeon X5550, 24 GB RAM,<br>new_candidates_identified: 97,<br>high_SOT_candidates_predicted_xiNN>=1: 16,<br>FeSi_experimental_xiSOT: 2 (experimental, Table 1),<br>FeSi_xiNN_prediction: 1.82,<br>example_Pt_experiment_vs_prediction: Pt ξ_SOT_exp=0.07, ξ_NN=0.07 (Table 1),<br>example_Ta_experiment_vs_prediction: Ta ξ_SOT_exp=0.15, ξ_NN=0.42 (Table 1) |
| **Application Domains** | Materials science,<br>Spintronics,<br>Condensed matter physics,<br>Device engineering (e.g., MRAM and nanomagnet switching),<br>Scientific text mining / literature-based materials discovery |

---


### [274. Biomni: A General-Purpose Biomedical AI Agent](https://doi.org/10.1101/2025.05.30.656746), Preprint *(June 02, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | bioRxiv corpus (25 subject categories, 100 recent publications per category),<br>Biomni-E1 environment (curated resources),<br>LAB-Bench (subset used),<br>Humanity's Last Exam (HLE) (subset used),<br>Open Targets genetics ground truth set (processed),<br>GWAS causal gene detection dataset (Shringarpure et al.),<br>CRISPR perturbation screen dataset (Schmidt et al.),<br>scRNA-seq annotation datasets (various author-provided datasets),<br>Microbiome benchmark datasets (5 datasets),<br>Drug repurposing dataset (Huang et al.),<br>Rare disease diagnosis dataset (MyGene2 curated by Alsentzer et al.),<br>Patient gene prioritization dataset (Alsentzer et al.),<br>Wearable sensor case-study data (CGM + body temperature),<br>Wearable sleep data (case study),<br>Multi-omics datasets merged with wearable data (case study),<br>Human embryonic skeletal multi-omic atlas (To et al.) |
| **Models** | Transformer,<br>GPT,<br>Gradient Boosting Tree,<br>Attention Mechanism |
| **Tasks** | Question Answering,<br>Ranking,<br>Optimization,<br>Multi-class Classification,<br>Recommendation,<br>Clustering,<br>Dimensionality Reduction,<br>Experimental Design,<br>Feature Extraction |
| **Learning Methods** | Zero-Shot Learning,<br>In-Context Learning,<br>Prompt Learning,<br>Pre-training,<br>Reinforcement Learning |
| **Performance Highlights** | DbQA_accuracy: 74.4%,<br>SeqQA_accuracy: 81.9%,<br>HLE_accuracy: 17.3%,<br>relative_performance_gain_vs_base_LLM_avg_across_8_tasks: 402.3% (average relative gain),<br>relative_gain_vs_coding_agent: 43.0%,<br>relative_gain_vs_Biomni-ReAct: 20.4%,<br>evaluation_metric: average post-perturbed effect (used to compare designed gene panels); specific numeric value not provided in paper for Biomni absolute score,<br>cloning_benchmark_accuracy_vs_expert: Biomni matched human expert in accuracy and completeness across 10 realistic cloning tasks (scored by blinded expert using rubric); trainee-level human performed worse,<br>wetlab_validation: Successful colonies on plates; Sanger sequencing of two picked colonies showed perfect alignment (successful insertion),<br>GRN_findings: Recovered 566-589 regulons; identified known regulators (e.g., RUNX2) and novel regulators (AUTS2, ZFHX3, PBX1) with notable activity patterns,<br>runtime: Full run completed in just over five hours (end-to-end for GRN analysis pipeline) |
| **Application Domains** | Biomedical research (general),<br>Genetics,<br>Genomics,<br>Molecular biology,<br>Single-cell biology / multi-omics,<br>Microbiology / Microbiome analysis,<br>Pharmacology / Drug repurposing,<br>Clinical medicine / Rare disease diagnosis,<br>Bioinformatics,<br>Bioengineering,<br>Biophysics,<br>Pathology,<br>Consumer health / wearable data analysis,<br>Experimental wet-lab protocol design (molecular cloning) |

---


### [273. A multimodal conversational agent for DNA, RNA and protein tasks](https://doi.org/10.1038/s42256-025-01047-1), Nature Machine Intelligence *(June 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Nucleotide Transformer benchmark (instructional version),<br>Curated genomics instructions dataset (27 tasks) [created by this paper],<br>APARENT2 dataset (polyadenylation),<br>Saluki dataset (RNA degradation),<br>ESM2 protein benchmark datasets (protein properties),<br>AgroNT benchmark (plant genomes / enhancers),<br>DeepSTARR dataset (enhancer activity in Drosophila),<br>ChromTransfer dataset (regulatory element accessibility),<br>BEND benchmark (subset used) |
| **Models** | Transformer,<br>GPT,<br>BERT,<br>Convolutional Neural Network,<br>Encoder-Decoder,<br>Attention Mechanism,<br>Cross-Attention,<br>Self-Attention Network,<br>Multi-Head Attention |
| **Tasks** | Binary Classification,<br>Multi-label Classification,<br>Regression,<br>Sequence-to-Sequence,<br>Language Modeling,<br>Image Classification |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Multi-Task Learning,<br>Supervised Learning,<br>Transfer Learning,<br>Backpropagation,<br>End-to-End Learning,<br>Representation Learning |
| **Performance Highlights** | MCC: 0.77 (ChatNT with English-aware projection, mean across 18 tasks),<br>MCC_non_aware_projection: 0.71 (ChatNT with Perceiver resampler not conditioned on question),<br>MCC_baseline_NTv2_500M: 0.69 (previous state-of-the-art Nucleotide Transformer v2 (500M)),<br>Splice sites_MCC: 0.98,<br>Promoters_MCC: 0.95,<br>DNA_methylation_AUROC: 0.97 (HUES64),<br>Promoter_strength_tobacco_PCC: 0.82,<br>RNA_polyadenylation_PCC: 0.91 (ChatNT) vs 0.90 (APARENT2),<br>Protein_melting_PCC: 0.89 (ChatNT) vs 0.85 (ESM2),<br>RNA_degradation_PCC_human: 0.62 (ChatNT) vs 0.74 (Saluki),<br>RNA_degradation_PCC_mouse: 0.63 (ChatNT) vs 0.71 (Saluki),<br>Calibration_example: Examples predicted with probability 0.9 are correct ~90%; medium-confidence area less calibrated before Platt scaling,<br>Overall_performance_preserved: Same performance (MCC) across tasks after deriving perplexity-based probabilities; calibration improved after Platt's model |
| **Application Domains** | Genomics,<br>Transcriptomics,<br>Proteomics,<br>Molecular biology / regulatory genomics,<br>Computational biology / bioinformatics,<br>Biomedical research (potential healthcare applications mentioned as extension) |

---


### [272. Predicting expression-altering promoter mutations with deep learning](https://doi.org/10.1126/science.ads7373), Science *(May 29, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | GTEx v8 (Genotype-Tissue Expression),<br>ENCODE bigWigs,<br>FANTOM5 CAGE-seq bigWigs,<br>Promoter variant training set (GTEx-derived),<br>gnomAD r3.0,<br>UK Biobank (UKBB) proteomics,<br>Genomics England (GEL) 100,000 Genomes Project (aggV2/v17),<br>Massively Parallel Reporter Assay (MPRA) eQTL dataset (published / generated),<br>Promoter MPRA library (GEL-targeted),<br>GENCODE v39 (human) and vM25 (mouse) TSS annotations,<br>ClinVar (clinvar_20240819 VCF) |
| **Models** | Convolutional Neural Network,<br>Feedforward Neural Network,<br>Multi-Layer Perceptron,<br>Autoencoder |
| **Tasks** | Regression,<br>Binary Classification,<br>Multi-class Classification,<br>Anomaly Detection,<br>Feature Extraction,<br>Clustering,<br>Dimensionality Reduction |
| **Learning Methods** | Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Ensemble Learning,<br>Representation Learning,<br>Batch Learning,<br>Mini-Batch Learning,<br>Gradient Descent,<br>Backpropagation,<br>End-to-End Learning |
| **Performance Highlights** | validation_profile_prediction_loss: not reported numerically in text (model selected by lowest validation loss),<br>GTEx_outlier_classification_auROC_under_vs_over: 0.89,<br>GTEx_outlier_classification_auROC_under_vs_null: 0.80,<br>GTEx_outlier_classification_auROC_over_vs_null: 0.74,<br>GTEx_fine-mapped_eQTLs_auROC_under_vs_over: 0.87,<br>GTEx_fine-mapped_eQTLs_auROC_under_vs_null: 0.79,<br>GTEx_fine-mapped_eQTLs_auROC_over_vs_null: 0.75,<br>correlation_with_eQTL_effect_sizes_Pearson_r: 0.56 (p = 8.2e-36),<br>MPRA_eQTL_correlation_Pearson_r: 0.63 (p = 6.1e-77),<br>MPRA_eQTL_auROC_under_vs_over: 0.90,<br>MPRA_eQTL_auROC_under_vs_null: 0.81,<br>MPRA_eQTL_auROC_over_vs_null: 0.83,<br>UKBB_promoter_pQTLs_correlation_Pearson_r: 0.56 (p = 7.9e-10) for 104 cis-pQTLs not in LD; correlation increased to 0.60 for genes with high mRNA stability,<br>UKBB_rare_promoter_variants_proteomics_correlation_Pearson_r: 0.48 (p = 2.8e-101),<br>UKBB_proteomics_auROC_under_vs_over: 0.91,<br>UKBB_proteomics_auROC_under_vs_null: 0.77,<br>UKBB_proteomics_auROC_over_vs_null: 0.78,<br>GEL_RNAseq_correlation_Pearson_r: 0.61 (p = 6.6e-171),<br>GEL_RNAseq_auROC_under_vs_over: 0.90,<br>GEL_RNAseq_auROC_under_vs_null: 0.78,<br>GEL_RNAseq_auROC_over_vs_null: 0.78,<br>ClinVar_auROC_pathogenic_vs_benign: 0.76,<br>trans_expression_correction: model used to reduce trans-regulatory confounding to increase detection of multitissue outliers (quantitative improvement reported as increases from 2540 to 4030 outliers after progressive corrections),<br>contextual_result_counts: 2540 outliers after PC correction -> 3116 after cis-eQTL correction -> 4030 after trans-expression correction,<br>OUTRIDER_comparison: PromoterAI outperformed outlier detection based on OUTRIDER autoencoder in number/enrichment of multitissue outliers when matching false discovery rate in shuffled background (Fig. 1E) |
| **Application Domains** | Disease Genomics,<br>Rare Disease Diagnosis,<br>Functional Genomics,<br>Population Genetics,<br>Clinical Genetics / Diagnostic Interpretation,<br>Proteomics / Biomarker discovery,<br>Regulatory genomics / promoter biology,<br>Computational genomics / variant effect prediction |

---


### [271. Probabilistic phase labeling and lattice refinement for autonomous materials research](https://doi.org/10.1038/s41524-025-01627-0), npj Computational Materials *(May 24, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Synthetic Ta-Sn-O calibration datasets (3 noise levels),<br>Al-Li-Fe-O synthetic benchmark,<br>CrxFe0.5−xVO4 experimental XRD set,<br>High-quality Ca5(PO4)3F XRD spectrum,<br>Ta-Sn-O experimental lg-LSA high-throughput dataset,<br>XCA calibration dataset (single-phase synthetic patterns) |
| **Models** | Convolutional Neural Network,<br>Non-negative Matrix Factorization,<br>Gaussian Process |
| **Tasks** | Multi-label Classification,<br>Classification,<br>Regression,<br>Dimensionality Reduction,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Expectation-Maximization,<br>Ensemble Learning,<br>Maximum A Posteriori,<br>Backpropagation |
| **Performance Highlights** | ECE_low_noise: 9.81%,<br>ECE_high_noise: 9.12%,<br>training_set_sizes: models trained on 24,000 and 40,000 simulated spectra (synthetic benchmark); other training sizes used for experimental tests: 20k, 32.5k, 52k,<br>reduction_example: 201 XRD patterns reduced to 4 basis patterns in the lg-LSA Ta-Sn-O example,<br>speed_impact: enabled phase labeling on 4 NMF bases in 9 s (probabilistic labeling) and extension to all 201 spectra in 4 s on a 4-core M1 MacBook Air,<br>background_modeling: Background modeled by kernel regressor with Matern kernel; jointly optimized with phase model to prevent overfitting,<br>contextual_performance: Joint optimization prevents the background from overfitting, leading to physically meaningful decompositions (Fig. 2a). |
| **Application Domains** | Materials science (X-ray diffraction, crystallography),<br>High-throughput experimentation (autonomous materials discovery),<br>Spectral de-mixing and analysis (deterministic spectroscopy) |

---


### [270. Data-Driven Design of Mechanically Hard Soft Magnetic High-Entropy Alloys](https://doi.org/10.1002/advs.202500867), Advanced Science *(May 22, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | HTP-DFT HEA database (this work) |
| **Models** | Random Forest,<br>Gradient Boosting Tree,<br>Multi-Layer Perceptron,<br>Decision Tree |
| **Tasks** | Binary Classification,<br>Regression,<br>Dimensionality Reduction,<br>Feature Selection |
| **Learning Methods** | Ensemble Learning,<br>Supervised Learning,<br>Feature Learning,<br>Dimensionality Reduction,<br>Gradient Boosting |
| **Performance Highlights** | accuracy: >90% (overall, training and test datasets); test accuracy ≈6% lower than training,<br>R2_train_B0: 97.4%,<br>R2_test_B0: 92.1%,<br>R2_Mtot_train: >98%,<br>R2_Mtot_test: >98%,<br>R2_TC_train: 98.7%,<br>R2_TC_test: ≈94.3% (test set performance 4.4% lower than training) |
| **Application Domains** | materials science,<br>high-entropy alloys (HEAs),<br>magnetic materials / soft magnets,<br>mechanical property prediction (bulk modulus, alloy strength),<br>computational materials design / high-throughput materials discovery |

---


### [269. A novel training-free approach to efficiently extracting material microstructures via visual large model](https://doi.org/10.1016/j.actamat.2025.120962), Acta Materialia *(May 15, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | PI-1 (pure iron),<br>PI-2 (pure iron),<br>SS (stainless steel),<br>HEA (high-entropy alloy),<br>LCS (low carbon steel) [11],<br>DP590-1 (dual-phase steel),<br>DP590-2 (dual-phase steel),<br>NBS-1 (Ni-based superalloy) [16],<br>NBS-2 (Ni-based superalloy) [16],<br>AZA (AlZn alloy) [25],<br>UHCS (ultrahigh carbon steel) [11] |
| **Models** | Vision Transformer,<br>Multi-Layer Perceptron,<br>Self-Attention Network,<br>Cross-Attention,<br>Attention Mechanism |
| **Tasks** | Semantic Segmentation,<br>Instance Segmentation,<br>Clustering |
| **Learning Methods** | Pre-training,<br>Zero-Shot Learning,<br>Prompt Learning,<br>Transfer Learning,<br>Fine-Tuning |
| **Performance Highlights** | PI-1 ARI (MatSAM): 0.62 (±2.4%),<br>PI-1 F1 (MatSAM): 0.71 (±1.6%),<br>SS ARI (MatSAM): 0.56 (±6.6%),<br>PI-2 ARI (MatSAM): 0.75 (±2.0%),<br>LCS ARI (MatSAM): 0.96 (±3.7%),<br>DP590-1 IoU (MatSAM): 0.77 (±8.2%),<br>DP590-2 IoU (MatSAM): 0.82 (±3.1%),<br>NBS-1 IoU (MatSAM): 0.91 (±1.2%),<br>NBS-2 IoU (MatSAM): 0.82 (±6.3%),<br>AZA IoU (MatSAM): 0.96 (±0.6%),<br>UHCS IoU (MatSAM): 0.76 (±0.7%),<br>Average relative improvement vs best rule-based (ARI+IoU): 35.4% (reported average relative improvement combining ARI and IoU over best-performing conventional rule-based methods),<br>Average improvement vs original SAM: 13.9% (reported average improvement over original SAM),<br>Average IoU improvement vs specialist DL models (on 4 public datasets): 7.5% (reported average improvement),<br>SAM baseline PI-1 ARI: 0.48 (±0.8%),<br>SAM baseline PI-2 ARI: 0.69 (±3.3%),<br>SAM baseline DP590-2 IoU: 0.68 (±6.7%),<br>SAM baseline NBS-1 IoU: 0.85 (±7.6%),<br>SAM baseline AZA IoU: 0.73 (±6.8%),<br>LCS ARI (MatSAM vs OTSU vs Canny vs SAM): MatSAM 0.96 (±3.7%), SAM 0.86 (±9.8%), Canny 0.73 (±5.3%), OTSU 0.64 (±4.3%),<br>NBS-1 inference time (MatSAM): 1812.67 ms per image (IoU 0.91),<br>LCS inference time (MatSAM): 1926.35 ms per image (ARI 0.96) |
| **Application Domains** | Materials science,<br>Materials microstructure analysis / characterization,<br>Microscopy image analysis (OM, SEM, TEM, XCT),<br>Automated quantitative microstructural characterization |

---


### [268. Interpretable Machine Learning Applications: A Promising Prospect of AI for Materials](https://doi.org/10.1002/adfm.202507734), Advanced Functional Materials *(May 13, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | 3.3 million materials science article abstracts (Tshitoyan et al.),<br>SteelBERT pre-training corpus,<br>Haeckelite candidate pool,<br>Zhong et al. catalytic surfaces dataset,<br>GNoME discovered stable-structure set,<br>Polysulfone candidate set (dielectric polymer screening),<br>DFT / first-principles datasets for ML interatomic potentials (examples) |
| **Models** | Linear Model,<br>Polynomial Model,<br>Decision Tree,<br>Support Vector Machine,<br>Gaussian Process,<br>Gradient Boosting Tree,<br>Ensemble Learning,<br>Feedforward Neural Network,<br>Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>Variational Autoencoder,<br>Generative Adversarial Network,<br>Diffusion Model,<br>Transformer,<br>BERT,<br>GPT,<br>Graph Neural Network,<br>ResNet |
| **Tasks** | Regression,<br>Classification,<br>Image Generation,<br>Image Classification,<br>Feature Selection,<br>Feature Extraction,<br>Clustering,<br>Dimensionality Reduction,<br>Language Modeling,<br>Text Classification,<br>Optimization,<br>Image-to-Image Translation,<br>Representation Learning |
| **Learning Methods** | Active Learning,<br>Transfer Learning,<br>Pre-training,<br>Fine-Tuning,<br>Adversarial Training,<br>Self-Supervised Learning,<br>Multi-Task Learning,<br>In-Context Learning,<br>Prompt Learning,<br>Representation Learning,<br>Ensemble Learning,<br>Active Learning |
| **Performance Highlights** | discovered_structures: over 2.2 million potentially stable structures,<br>search_space: explore over 1e60 possible compounds (conceptual capability),<br>image_fidelity: high-fidelity synthetic images closely resembling experimental images,<br>predicted_bandgap: identified hybrid perovskite composition with bandgap = 1.39 eV,<br>classification_accuracy: >90%,<br>example_designs: compositions with UTS of 600–950 MPa and electrical conductivity of 50.0% IACS |
| **Application Domains** | Materials science (general),<br>Metallic structural materials (alloy design, high-entropy alloys),<br>High-temperature alloys / superalloys,<br>Battery materials and solid electrolytes,<br>Perovskite photovoltaic materials,<br>Catalytic materials and electrocatalysis (CO2 reduction, OER),<br>Polymers (dielectric, high-thermal-conductivity polymers),<br>Microstructure-informed manufacturing (additive manufacturing / LPBF),<br>Protein structure prediction / biomolecular materials (AlphaFold examples),<br>Drug-like molecule generation (TamGen),<br>Optoelectronic and ferroelectric materials,<br>Glass and ceramic materials,<br>Porous materials and MOFs,<br>Composite materials |

---


### [267. Exploration of crystal chemical space using text-guided generative artificial intelligence](https://doi.org/10.1038/s41467-025-59636-y), Nature Communications *(May 12, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project (MP-40 dataset, filtered <=40 atoms),<br>MP-20 dataset,<br>MatTPUSciBERT pretraining corpus,<br>Generated TiO2 polymorphs (from Chemeleon),<br>Generated Ti-Zn-O candidates,<br>Generated Li-P-S-Cl candidates (quaternary),<br>Test set (chronological split) |
| **Models** | Denoising Diffusion Probabilistic Model,<br>Graph Neural Network,<br>BERT,<br>Transformer,<br>Variational Autoencoder |
| **Tasks** | Structured Prediction,<br>Graph Generation,<br>Synthetic Data Generation,<br>Distribution Estimation |
| **Learning Methods** | Contrastive Learning,<br>Pre-training,<br>Self-Supervised Learning,<br>Generative Learning,<br>Fine-Tuning |
| **Performance Highlights** | Validity_composition_prompt_BaselineBERT: 0.99,<br>Uniqueness_composition_prompt_BaselineBERT: 0.94,<br>StructureMatching_composition_prompt_BaselineBERT: 0.13,<br>Metastability_composition_prompt_BaselineBERT: 0.22,<br>Validity_formatted_text_BaselineBERT: 0.99,<br>Uniqueness_formatted_text_BaselineBERT: 0.97,<br>StructureMatching_formatted_text_BaselineBERT: 0.09,<br>Metastability_formatted_text_BaselineBERT: 0.21,<br>Validity_general_text_BaselineBERT: 0.99,<br>Uniqueness_general_text_BaselineBERT: 0.97,<br>StructureMatching_general_text_BaselineBERT: 0.06,<br>Metastability_general_text_BaselineBERT: 0.23,<br>Validity_composition_prompt_CrystalCLIP: 0.99,<br>Uniqueness_composition_prompt_CrystalCLIP: 0.90,<br>StructureMatching_composition_prompt_CrystalCLIP: 0.20,<br>Metastability_composition_prompt_CrystalCLIP: 0.25,<br>Validity_formatted_text_CrystalCLIP: 0.98,<br>Uniqueness_formatted_text_CrystalCLIP: 0.92,<br>StructureMatching_formatted_text_CrystalCLIP: 0.17,<br>Metastability_formatted_text_CrystalCLIP: 0.19,<br>Validity_general_text_CrystalCLIP: 0.99,<br>Uniqueness_general_text_CrystalCLIP: 0.90,<br>StructureMatching_general_text_CrystalCLIP: 0.20,<br>Metastability_general_text_CrystalCLIP: 0.25,<br>StructureMatching_overall_test_CrystalCLIP: 0.20,<br>CompositionMatchingRate_on_MP-20_Chemeleon: 67.52%,<br>StructureMatchRate_relative: lower than DiffCSP and FlowMM (strict criterion: structure match only counted when composition also matches),<br>RMSE_relative: worse than DiffCSP; similar to FlowMM (exact RMSE values not provided),<br>TiO2_generated_count: 549 sampled polymorphs (539 converged with MACE-MP),<br>TiO2_DFT_identified_metastable_count: 122 unique metastable TiO2 structures (DFT-refined),<br>TiO2_new_spacegroups: 50 structures with space groups not previously observed in known TiO2 polymorphs,<br>Ti-Zn-O_predicted_stable: 1 structure below convex hull,<br>Ti-Zn-O_predicted_metastable: 58 metastable structures,<br>Li-P-S-Cl_predicted_stable: 17 new stable structures proposed,<br>Li-P-S-Cl_predicted_metastable: 435 metastable structures generated,<br>Li-P-S-Cl_energy_distribution_within_0.15eV: ≈80% of sampled configurations within 0.15 eV/atom above convex hull,<br>StructureGeneration_runtime_Li-P-S-Cl: ≈72 hours on a single A100 GPU (search of Li-P-S-Cl space),<br>CompositionMatching_ratio_trend: composition-matching ratio declines with increasing number of atoms; Crystal CLIP outperforms Baseline BERT by up to ~3x in composition matching across atom counts (Figure 3a) |
| **Application Domains** | Materials science / inorganic crystals,<br>Crystal structure prediction and generation,<br>Computational materials discovery and high-throughput screening,<br>Solid-state battery materials (Li-P-S-Cl electrolyte space),<br>Polymorph exploration (e.g., TiO2 polymorphs),<br>Phase diagram construction |

---


### [266. Using GNN property predictors as molecule generators](https://doi.org/10.1038/s41467-025-59439-1), Nature Communications *(May 08, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | QM9,<br>ZINC subset (250,000 molecules),<br>Generated DFT dataset (this work),<br>QM9 (used as random draw / baseline) |
| **Models** | Graph Neural Network,<br>Graph Convolutional Network,<br>Graph Isomorphism Network,<br>Graph Attention Network,<br>GraphSAGE,<br>Message Passing Neural Network,<br>Variational Autoencoder,<br>Normalizing Flow,<br>Diffusion Model,<br>Graph Convolutional Policy Network,<br>CrippenNet,<br>Genetic Algorithm (evolutionary baseline) |
| **Tasks** | Regression,<br>Synthetic Data Generation,<br>Optimization,<br>Representation Learning |
| **Learning Methods** | Supervised Learning,<br>Gradient Descent,<br>Backpropagation,<br>Mini-Batch Learning,<br>Evolutionary Learning,<br>Reinforcement Learning,<br>Active Learning |
| **Performance Highlights** | test_MAE_QM9: 0.12 eV,<br>generated_MAE_approx: 0.8 eV (observed on generated molecules vs DFT),<br>ncalcs_per_target: 100 (per target in experiments),<br>n_within_±0.5eV_for_4.1eV: 46 / 100,<br>MAE_to_target_for_4.1eV: 0.81 eV,<br>Diversity_for_4.1eV: 0.91 (average pairwise Tanimoto distance),<br>n_within_±0.5eV_for_6.8eV: 50 / 100,<br>MAE_to_target_for_6.8eV: 0.83 eV,<br>Diversity_for_6.8eV: 0.90,<br>n_within_±0.5eV_for_9.3eV: 34 / 100,<br>MAE_to_target_for_9.3eV: 0.83 eV,<br>Diversity_for_9.3eV: 0.83,<br>Pearson_correlation_ρ_between_ML_and_DFT_all300: 0.86,<br>average_time_per_in-target_molecule: 12.0 s (4.1 eV), 2.1 s (6.8 eV), 10.4 s (9.3 eV) on 4-CPU 3.40 GHz machine,<br>training_data: ZINC subset (250k) + QM9,<br>used_as_proxy_for_generation: yes,<br>success_rate_−2.5<=logP<=−2: 43.5% (Proxy evaluation),<br>diversity_−2.5..−2: 0.932 (average pairwise Tanimoto distance),<br>success_rate_5<=logP<=5.5: 14.4% (Proxy evaluation),<br>diversity_5..5.5: 0.917,<br>average_time_per_in-target_molecule: 5.6 s for −2.5 to −2; 3.4 s for 5 to 5.5 on 4-CPU 3.40GHz,<br>MAE_on_QM9_test: 0.048 eV,<br>MAE_on_generated_molecules: 1.16 eV (worse on generated molecules),<br>generation_performance: slightly worse than the authors' simple GNN used in main DIDgen experiments (no exact numbers in main text; details in SI),<br>JANUS_DFT_ncalcs_for_4.1eV: 197,<br>JANUS_DFT_n_within_±0.5eV_for_4.1eV: 24 (12.2%),<br>JANUS_DFT_MAE_for_4.1eV: 0.96 eV,<br>JANUS_DFT_diversity_for_4.1eV: 0.79,<br>JANUS_Proxy_for_4.1eV_MAE: 1.05 eV (proxy-run),<br>comparison_note: DIDgen nearly matches or outperforms JANUS on the nine metrics reported in Table 1,<br>success_rate_−2.5..−2_(Proxy): 11.3%,<br>diversity_−2.5..−2: 0.846,<br>success_rate_5..5.5_(Proxy): 7.6%,<br>diversity_5..5.5: 0.907,<br>success_rate_−2.5..−2_(Oracle): 85.5%,<br>diversity_−2.5..−2: 0.392 (oracle-evaluated diversity low for that case),<br>success_rate_5..5.5_(Oracle): 54.7%,<br>diversity_5..5.5: 0.855 |
| **Application Domains** | Computational materials science,<br>Molecular discovery / cheminformatics,<br>Drug discovery (logP as proxy for cell permeability),<br>Organic electronics / OLED materials (HOMO-LUMO gap targeting for emission wavelength),<br>Automated experimentation / autonomous labs (context and motivation) |

---


### [265. Discovery of Sustainable Energy Materials Via the Machine-Learned Material Space](https://doi.org/10.1002/smll.202412519), Small *(May 05, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | OptiMate dataset (Tr[Im(ϵ_ij)]/3 calculated with 300 meV broadening),<br>Alexandria database (referenced) |
| **Models** | Graph Attention Network,<br>Multi-Layer Perceptron,<br>Message Passing Neural Network |
| **Tasks** | Regression,<br>Dimensionality Reduction,<br>Clustering,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Representation Learning,<br>Feature Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | Materials science,<br>Optical materials,<br>Energy materials,<br>Photovoltaics (PV) and multijunction solar cells,<br>Solar hydrogen generation,<br>Optical sensors,<br>Epsilon-near-zero materials,<br>Energy-efficient light-emitting devices |

---


### [264. End-to-end data-driven weather prediction](https://doi.org/10.1038/s41586-025-08897-0), Nature *(May 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | ERA5 reanalysis,<br>HadISD (Hadley Centre integrated surface dataset),<br>ICOADS (International Comprehensive Ocean-Atmosphere Data Set),<br>IGRA (Integrated Global Radiosonde Archive),<br>ASCAT (Metop Advanced Scatterometer) Level 1B,<br>AMSU-A / AMSU-B / Microwave Humidity Sounder / HIRS,<br>IASI (Infrared Atmospheric Sounding Interferometer),<br>GridSat (Gridded Geostationary Brightness Temperature Data),<br>HRES (ECMWF Integrated Forecasting System high-resolution) forecasts,<br>GFS (NCEP Global Forecast System) forecasts,<br>NDFD (National Digital Forecast Database) |
| **Models** | Vision Transformer,<br>U-Net,<br>Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>Encoder-Decoder,<br>Multi-Head Attention,<br>Self-Attention Network |
| **Tasks** | Time Series Forecasting,<br>Regression,<br>Image-to-Image Translation,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>End-to-End Learning,<br>Transfer Learning,<br>Stochastic Gradient Descent,<br>Representation Learning |
| **Performance Highlights** | LW-RMSE: Aardvark achieved lower latitude-weighted RMSE than GFS across most lead times for many variables; approached HRES performance for most variables and lead times (held-out test year 2018, ERA5 ground truth),<br>LW-RMSE at t=0: initial-state estimation error reported and compared to HRES analysis; Aardvark has non-zero error at t=0 against ERA5 whereas HRES also non-zero,<br>MAE: Aardvark produced skilful station forecasts up to 10 days lead time; competitive with station-corrected HRES and matched NDFD over CONUS for 2-m temperature; for 10-m wind, mixed results (worse than station-corrected HRES over CONUS but outperformed NDFD).,<br>Fine-tuning improvement (MAE %): 2-m temperature: −6% MAE (Europe, West Africa, Pacific, Global), −3% MAE (CONUS). 10-m wind speed: 1–2% MAE improvements for most regions (except Pacific).,<br>Inference speed: Full forecast generation ~1 second on four NVIDIA A100 GPUs,<br>Computational cost comparison: HRES data assimilation and forecasting ~1,000 node hours (operational NWP) |
| **Application Domains** | Numerical weather forecasting / atmospheric sciences,<br>Local weather forecasting (station-level forecasts),<br>Transportation (weather impacts),<br>Agriculture (heatwaves, cold waves forecasting),<br>Energy and renewable energy (wind forecasts),<br>Public safety and emergency services (extreme weather warnings, tropical cyclones),<br>Marine forecasting (ocean/ship observations),<br>Insurance and finance (weather risk modelling),<br>Environmental monitoring (potential extension to atmospheric chemistry and air quality),<br>Operational meteorology (replacement/augmentation of NWP pipelines) |

---


### [263. Engineering principles for self-driving laboratories](https://doi.org/10.1038/s44286-025-00217-7), Nature Chemical Engineering *(May 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | _None_ |
| **Models** | _None_ |
| **Tasks** | Optimization,<br>Experimental Design,<br>Data Generation,<br>Resource Allocation |
| **Learning Methods** | Reinforcement Learning,<br>Active Learning,<br>Online Learning |
| **Performance Highlights** | material_consumption_ratio_vs_conventional: less than 1/500,<br>data_generation_rate_equivalent: over 100 researchers,<br>quality_improvement: achieved superior optical properties compared with literature protocols,<br>experiments_reduction: substantially reduces the number of experiments required (qualitative / orders-of-magnitude claims),<br>timeline_reduction: reduce discovery and development timelines by orders of magnitude (qualitative) |
| **Application Domains** | Chemical engineering,<br>Materials discovery,<br>Pharmaceutical process development,<br>Synthesis of (bio)molecules,<br>Colloidal atomic layer deposition / nanostructure synthesis,<br>Automated laboratory operations / self-driving laboratories (SDLs) |

---


### [262. Large language model-driven database for thermoelectric materials](https://doi.org/10.1016/j.commatsci.2025.113855), Computational Materials Science *(May 01, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Northeast Thermoelectric Materials Database (this work),<br>Corpus of collected DOIs (Elsevier),<br>Validation sample of extracted entries,<br>Sierepeklis & Cole automated thermoelectric database,<br>Gaultois et al. thermoelectric dataset (reference),<br>Na and Chang thermoelectric database (reference),<br>Various first-principles / ab initio thermoelectric databases (references: JARVIS, ab initio transport DB, etc.) |
| **Models** | GPT,<br>Transformer,<br>Graph Neural Network,<br>Attention Mechanism |
| **Tasks** | Information Retrieval,<br>Feature Extraction,<br>Data Generation,<br>Regression,<br>Information Retrieval |
| **Learning Methods** | Prompt Learning,<br>Supervised Learning,<br>Unsupervised Learning,<br>Pre-training |
| **Performance Highlights** | Composition_accuracy_percent: 90,<br>Type_accuracy_percent: 91,<br>Seebeck_Coefficient_accuracy_percent: 100,<br>Seebeck_Obs_Temp_accuracy_percent: 100,<br>Electrical_Conductivity_accuracy_percent: 100,<br>Electrical_Conductivity_Obs_Temp_accuracy_percent: 100,<br>Thermal_Conductivity_accuracy_percent: 100,<br>Thermal_Conductivity_Obs_Temp_accuracy_percent: 100,<br>Power_Factor_accuracy_percent: 100,<br>Power_Factor_Obs_Temp_accuracy_percent: 100,<br>ZT_accuracy_percent: 100,<br>ZT_Obs_Temp_accuracy_percent: 100,<br>Crystal_Structure_accuracy_percent: 97,<br>Lattice_Structure_accuracy_percent: 96,<br>Lattice_Parameters_accuracy_percent: 100,<br>Space_Group_accuracy_percent: 100,<br>Experimental_flag_accuracy_percent: 100 |
| **Application Domains** | Thermoelectric materials,<br>Materials science,<br>Energy harvesting,<br>Sustainable energy / thermoelectric device design,<br>Scientific text mining / literature curation |

---


### [261. Leveraging generative models with periodicity-aware, invertible and invariant representations for crystalline materials design](https://doi.org/10.1038/s43588-025-00797-7), Nature Computational Science *(May 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project (MP),<br>NOMAD,<br>AFLOW,<br>Vanadium–Oxygen dataset (example),<br>CrystaLLM training set (standardized tokenized CIFs),<br>MatterSim DFT training data (referenced),<br>Open Catalyst Project (OC22) |
| **Models** | Generative Adversarial Network,<br>Variational Autoencoder,<br>Diffusion Model,<br>Denoising Diffusion Probabilistic Model,<br>Graph Neural Network,<br>Graph Convolutional Network,<br>Graph Attention Network,<br>Transformer,<br>Convolutional Neural Network,<br>Message Passing Neural Network,<br>Normalizing Flow |
| **Tasks** | Synthetic Data Generation,<br>Graph Generation,<br>Regression,<br>Classification,<br>Property Prediction (mapped to Regression/Classification),<br>Optimization |
| **Learning Methods** | Generative Learning,<br>Adversarial Training,<br>Unsupervised Learning,<br>Pre-training,<br>Transfer Learning,<br>Fine-Tuning,<br>Active Learning,<br>Domain Adaptation,<br>Incremental Learning,<br>Ensemble Learning,<br>Representation Learning,<br>Reinforcement Learning |
| **Performance Highlights** | local_energy_minimum_proximity: >15x closer (relative measure),<br>samples_generated: 10,000,<br>space_groups_sampled: 113,<br>predicted_formation_energy: ~3.1 eV per atom (reported as 'low'),<br>pretraining_dataset_size: 2.3 million structures |
| **Application Domains** | crystalline materials design / inorganic crystals,<br>battery materials,<br>catalytic materials,<br>superconductors,<br>photoanodes,<br>thermoelectric materials (power factor prediction),<br>magnetic materials (low-supply-chain-risk magnets),<br>electrolyte and cathode materials (high-temperature operation),<br>optical/optoelectronic materials (e.g., n-type doped low-dimensional materials) |

---


### [260. Automated processing and transfer of two-dimensional materials with robotics](https://doi.org/10.1038/s44286-025-00227-5), Nature Chemical Engineering *(May 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Transferred 4-inch graphene wafer electrical mapping (97 devices),<br>Array of transferred graphene wafers (production runs),<br>Transferred 2-inch MoS2 domains on 4-inch graphene wafer (heterostructure mapping),<br>Surface and morphology characterization dataset (OM/AFM/Raman/PL/XPS/SEM/TEM data),<br>Life cycle inventory (LCA) datasets and results,<br>Automation control code (spin-coating and lamination/delamination machines) |
| **Models** | _None_ |
| **Tasks** | Control,<br>Planning,<br>Optimization,<br>Decision Making,<br>Experimental Design,<br>Resource Allocation |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | 2D materials manufacturing,<br>Electronics (device fabrication),<br>Photonics,<br>Quantum technology,<br>Materials characterization and heterostructure fabrication,<br>Industrial automation and robotics,<br>Environmental assessment / life cycle analysis |

---


### [259. Self-driving nanoparticle synthesis](https://doi.org/10.1038/s44286-025-00225-7), Nature Chemical Engineering *(May 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | spectroscopic properties extracted from existing literature,<br>AFION online extinction spectra (experimental data),<br>Offline TEM and energy-dispersive X-ray spectroscopy (EDX) validation data |
| **Models** | Gaussian Process |
| **Tasks** | Optimization,<br>Experimental Design,<br>Hyperparameter Optimization,<br>Regression |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Online Learning |
| **Performance Highlights** | experiments_to_identify_optimum: fewer than 30,<br>time_to_identify_optimum: 30 hours,<br>distinct_np_types_synthesized: 8,<br>training_data_size: fewer than 30 experimental runs (as above) |
| **Application Domains** | Nanoparticle synthesis,<br>Chemical engineering,<br>Materials research,<br>Autonomous experimentation / self-driving laboratories,<br>Microfluidics-based synthesis and inline spectroscopy |

---


### [258. MatterChat: A Multi-Modal LLM for Material Science](https://doi.org/10.48550/arXiv.2502.13107), Preprint *(April 26, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project Trajectory (MPtrj) (relaxed samples),<br>GNoME (newly discovered materials set used for comparative evaluation) |
| **Models** | Graph Neural Network,<br>Transformer,<br>Attention Mechanism,<br>Multi-Head Attention |
| **Tasks** | Classification,<br>Regression,<br>Question Answering,<br>Text Generation,<br>Dimensionality Reduction,<br>Clustering,<br>Representation Learning |
| **Learning Methods** | Contrastive Learning,<br>Supervised Learning,<br>Fine-Tuning,<br>Pre-training,<br>Transfer Learning,<br>Representation Learning,<br>Distributed Learning |
| **Performance Highlights** | Accuracy: 0.6373,<br>Accuracy: 0.6864,<br>Accuracy: 0.8683,<br>Accuracy: 0.8873,<br>Accuracy: 0.8629,<br>Accuracy: 0.7839,<br>Accuracy: 0.8753,<br>Accuracy: 0.8797,<br>Accuracy: 0.7418,<br>Accuracy: 0.7944,<br>Accuracy: 0.8515,<br>Accuracy: 0.8573,<br>Accuracy: 0.7171,<br>Accuracy: 0.6549,<br>Accuracy: 0.8504,<br>Accuracy: 0.857,<br>Accuracy: 0.8339,<br>Accuracy: 0.6833,<br>Accuracy: 0.9368,<br>Accuracy: 0.9333,<br>Accuracy: 0.7759,<br>Accuracy: 0.4238,<br>Accuracy: 0.857,<br>Accuracy: 0.8535,<br>RMSE (eV/atom): 0.4105,<br>RMSE (eV/atom): 1.8059,<br>RMSE (eV/atom): 0.15,<br>RMSE (eV/atom): 0.1212,<br>RMSE (eV/atom): 0.4415,<br>RMSE (eV/atom): 0.4051,<br>RMSE (eV/atom): 0.1053,<br>RMSE (eV/atom): 0.0964,<br>RMSE (eV): 1.2516,<br>RMSE (eV): 1.4725,<br>RMSE (eV): 0.559,<br>RMSE (eV): 0.5058 |
| **Application Domains** | Materials science (inorganic materials),<br>Energy (materials for energy applications),<br>Electronics (semiconductor materials),<br>Catalysis (materials discovery for catalysis),<br>Scientific human-AI interaction (material synthesis guidance and reasoning) |

---


### [257. Towards AI-driven autonomous growth of 2D materials based on a graphene case study](https://doi.org/10.1038/s42005-025-02086-1), Communications Physics *(April 25, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Raman spectroscopy measurements (experimentally collected spectra used for scoring),<br>Atomic Force Microscopy (AFM) adhesion force maps,<br>X-ray Photoelectron Spectroscopy (XPS) core-level spectra (C1s),<br>Angle-Resolved Photoemission Spectroscopy (ARPES) intensity maps and MDCs |
| **Models** | Multi-Layer Perceptron,<br>Gaussian Process |
| **Tasks** | Optimization,<br>Control,<br>Experimental Design |
| **Learning Methods** | Evolutionary Learning,<br>Active Learning,<br>Backpropagation,<br>Gradient Descent |
| **Performance Highlights** | AFM_graphene_area_protocol1: 22.4%,<br>AFM_graphene_area_protocol5: 88.2%,<br>XPS_graphene_area_PTC1: 18.94%,<br>XPS_graphene_area_PTC2: 19.60%,<br>XPS_graphene_area_PTC4: 31.30%,<br>XPS_graphene_area_PTC5: 23.10%,<br>ARPES_MDC_FWHM_PTC1: 0.044 Å^-1,<br>ARPES_MDC_FWHM_PTC5: 0.020 Å^-1,<br>experiments_to_learn: "a few tens of experiments" (stated in text),<br>one_experiment_per_learning_step: yes |
| **Application Domains** | Graphene growth / 2D materials synthesis,<br>Materials science (epitaxial growth on SiC),<br>Autonomous laboratories and experimental optimization,<br>Surface characterization and electronic structure validation (Raman, AFM, XPS, ARPES) |

---


### [256. Science acceleration and accessibility with self-driving labs](https://doi.org/10.1038/s41467-025-59231-1), Nature Communications *(April 24, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Galaxy Zoo,<br>Foldit,<br>The Harvard Clean Energy Project |
| **Models** | Gaussian Process,<br>Transformer,<br>GPT,<br>Multi-Layer Perceptron,<br>Random Forest |
| **Tasks** | Optimization,<br>Experimental Design,<br>Image Classification,<br>Ranking,<br>Data Generation,<br>Hyperparameter Optimization |
| **Learning Methods** | Active Learning,<br>Reinforcement Learning,<br>Ensemble Learning,<br>Pre-training,<br>Transfer Learning,<br>Ensemble Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | Chemical sciences / Chemistry,<br>Materials science,<br>Energy (e.g., photovoltaics, battery materials),<br>Medicine / Pharmaceutical discovery,<br>Nanoparticles and nanomaterials synthesis,<br>Microscopy / Scanning probe and electron microscopy,<br>Analytical method development (e.g., HPLC),<br>Industrial R&D and specialty chemicals |

---


### [255. Learning Smooth and Expressive Interatomic Potentials for Physical Property Prediction](https://doi.org/10.48550/arXiv.2502.12147), Preprint *(April 23, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | MPTrj,<br>SPICE-MACE-OFF,<br>OMat24 (O M A T 24),<br>Matbench-Discovery,<br>MDR Phonon benchmark,<br>TM23,<br>MD22,<br>sAlex (subsampled Alexandria),<br>SPICE (SPICE-1.0),<br>PubChem / DES370K / Dipeptides / Sol. AA / Water / QMugs |
| **Models** | Message Passing Neural Network,<br>Graph Neural Network,<br>Transformer |
| **Tasks** | Binary Classification,<br>Regression,<br>Regression |
| **Learning Methods** | Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Self-Supervised Learning,<br>Transfer Learning,<br>Backpropagation,<br>Batch Learning |
| **Performance Highlights** | F1: 0.831,<br>κSRME: 0.34,<br>MAE (energy, eV/atom): 0.033,<br>RMSD: 0.0752,<br>Accuracy: 0.946,<br>Precision: 0.804,<br>R2: 0.822,<br>F1: 0.925,<br>κSRME: 0.17,<br>MAE (energy, eV/atom): 0.018,<br>RMSD: 0.0608,<br>Accuracy: 0.977,<br>Precision: 0.928,<br>R2: 0.866,<br>MAE(ωmax) (K): 21,<br>MAE(S) (J/K/mol): 13,<br>MAE(F) (kJ/mol): 5,<br>MAE(CV) (J/K/mol): 4,<br>Energy MAE (meV/atom) - MPTrj test: 17.02,<br>Force MAE (meV/Å) - MPTrj test: 43.96,<br>Stress MAE (meV/Å/atom) - MPTrj test: 0.14,<br>Energy MAE (meV/atom) - SPICE test: 0.23,<br>Force MAE (meV/Å) - SPICE test: 6.36,<br>SPICE-MACE-OFF test splits (eSEN-6.5M) Energy MAE (meV/atom): {'PubChem': 0.15, 'DES370K M.': 0.13, 'DES370K D.': 0.15, 'Dipeptides': 0.07, 'Sol. AA': 0.25, 'Water': 0.15, 'QMugs': 0.12},<br>SPICE-MACE-OFF test splits (eSEN-6.5M) Force MAE (meV/Å): {'PubChem': 4.21, 'DES370K M.': 1.24, 'DES370K D.': 2.12, 'Dipeptides': 2.0, 'Sol. AA': 3.68, 'Water': 2.5, 'QMugs': 3.78},<br>Training efficiency reduction (wallclock): 40% reduction (conservative fine-tuned model vs from-scratch conservative trained for equivalent validation loss),<br>Validation loss convergence: Conservative fine-tuned model achieves lower validation loss after 40 epochs compared to from-scratch conservative model trained for 100 epochs (Figure 3). |
| **Application Domains** | Materials science (inorganic materials, crystal stability prediction),<br>Computational chemistry (molecular forces, energies for organic molecules and peptides),<br>Molecular dynamics simulations (MD energy conservation and simulation stability),<br>Phonon and vibrational property prediction (thermal conductivity, vibrational entropy, free energy, heat capacity),<br>Drug discovery / biomolecular modeling (molecular datasets, peptides),<br>Benchmarking and methodology for ML interatomic potentials (MLIPs) |

---


### [254. Harnessing database-supported high-throughput screening for the design of stable interlayers in halide-based all-solid-state batteries](https://doi.org/10.1038/s41467-025-58522-x), Nature Communications *(April 17, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project database (retrieved 21,576 Li-containing materials),<br>Li3OCl experimental/characterization data (this work; Supplementary Data),<br>DFT simulation data (interface models, DOS, reaction energies) — Supplementary Data 8 & 9,<br>OMat24 inorganic materials dataset,<br>MatterSim (deep learning atomistic model) (ref. 45) |
| **Models** | Transformer,<br>Graph Neural Network,<br>None (Density Functional Theory / DFT — not in provided model list) |
| **Tasks** | Ranking,<br>Binary Classification,<br>Regression,<br>Recommendation,<br>Text Generation |
| **Learning Methods** | Prompt Learning,<br>Supervised Learning,<br>High-throughput Screening (not in provided list) |
| **Performance Highlights** | _None_ |
| **Application Domains** | All-solid-state Li metal batteries (ASSLMBs),<br>Materials discovery for interlayer materials in halide solid-state electrolytes,<br>Computational materials science (first-principles DFT + database screening),<br>Electrochemistry / battery interface engineering,<br>Scientific writing assisted by large language models (manuscript language editing) |

---


### [253. Data-driven discovery of biaxially strained single atoms array for hydrogen production](https://doi.org/10.1038/s41467-025-59053-1), Nature Communications *(April 17, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | HT-DFT SAA Au-bMX2 hydrogen adsorption dataset (ΔGH*) |
| **Models** | Random Forest |
| **Tasks** | Regression,<br>Feature Selection,<br>Feature Extraction,<br>Ranking,<br>Classification |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Bagging,<br>Feature Selection,<br>Cross-Validation |
| **Performance Highlights** | R2_test: 0.992,<br>RMSE_test: 0.077,<br>R2: 0.953,<br>R2: 0.972,<br>top_four_features_contribution_model_N: ≈70%,<br>top_four_features_contribution_model_F: >76%,<br>εads_importance_model_N: 53.98%,<br>key_feature_identified: Electron affinity (EA) identified as most drastic influence on ΔGH* via SHAP/PFI |
| **Application Domains** | Electrocatalysis (Hydrogen Evolution Reaction),<br>Computational materials discovery,<br>Data-driven catalyst screening,<br>Single-atom catalysis on transition metal dichalcogenides (TMDs) |

---


### [252. A Multiagent-Driven Robotic AI Chemist Enabling Autonomous Chemical Research On Demand](https://doi.org/10.1021/jacs.4c17738), Journal of the American Chemical Society *(April 16, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Literature Database (local),<br>Protocol Library,<br>Model Library (pretrained models),<br>MO-HEC experimental dataset (random sampling),<br>MO-HEC combined measured set (including optimized sample),<br>Task 1 FTIR dataset,<br>Task 2 PXRD dataset,<br>Task 3 PQD fluorescence dataset,<br>Task 4 g-C3N4 factorial experiment data,<br>Task 5 BiOX photocatalytic degradation data,<br>Task 7 photoreduction GC-MS timecourse data |
| **Models** | Transformer,<br>Multi-Layer Perceptron,<br>Dimensionality Reduction,<br>Unsupervised Learning |
| **Tasks** | Information Retrieval,<br>Feature Extraction,<br>Regression,<br>Optimization,<br>Dimensionality Reduction,<br>Feature Extraction,<br>Information Retrieval |
| **Learning Methods** | Pre-training,<br>Fine-Tuning,<br>Transfer Learning,<br>Supervised Learning,<br>Unsupervised Learning,<br>Prompt Learning,<br>In-Context Learning |
| **Performance Highlights** | best_experimental_overpotential_at_10_mA_cm^-2_mV: 266.1,<br>random_sampling_overpotentials_all_above_mV: 300+,<br>stability_reduction_over_500_h_percent: <2%,<br>HER_performance_range_mmol_g^-1: 9.28e-5 to 2.10e-3 |
| **Application Domains** | chemistry,<br>materials science,<br>electrocatalysis (oxygen evolution reaction),<br>photocatalysis,<br>organic synthesis / photocatalytic organic reactions,<br>laboratory automation / robotics,<br>autonomous self-driving laboratories (SDLs) |

---


### [251. Generative deep learning for predicting ultrahigh lattice thermal conductivity materials](https://doi.org/10.1038/s41524-025-01592-8), npj Computational Materials *(April 11, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | AIRSS carbon periodic structures (Pickard),<br>Generated CDVAE structures (this work),<br>GAP-2020 subset (training set for pre-trained Allegro),<br>Active-learning MLIP dataset (this work),<br>Benchmarks (selected for detailed κL evaluation) |
| **Models** | Variational Autoencoder,<br>Diffusion Model,<br>Message Passing Neural Network,<br>Multi-Layer Perceptron,<br>Autoencoder |
| **Tasks** | Synthetic Data Generation,<br>Regression,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Extraction,<br>Representation Learning,<br>Data Generation |
| **Learning Methods** | Generative Learning,<br>Active Learning,<br>Supervised Learning,<br>Representation Learning,<br>Ensemble Learning,<br>Pre-training,<br>Fine-Tuning,<br>Gradient Descent,<br>Active Learning |
| **Performance Highlights** | generated_structures: 100,000,<br>generation_speed_per_structure: 0.48 seconds per structure (single RTX 2080 Ti GPU),<br>unique_after_deduplication: 7,213 (≈7.2% of generated),<br>candidates_after_symmetry_filter: 1,361,<br>see: same as CDVAE generation performance above (100k generated, 7213 unique, 1361 candidates),<br>energy_MAE_on_test: 24.3 meV atom^-1,<br>force_RMSE_on_test: 273 meV Å^-1,<br>uncertainty_threshold: 15 meV atom^-1,<br>benchmarks_with_κL_over_800: 9 of 53 benchmarks confirmed,<br>total_identified_ultrahigh_κL_structures: 34 structures with κL > 800 W m^-1K^-1,<br>max_κL_found: up to 2,400 W m^-1K^-1 (aside from diamond),<br>component_role: used inside Allegro (2-body latent MLP: [128,128,128]; edge energy MLP: [256,128,64]),<br>global_model_performance: see Allegro metrics above (energy MAE 24.3 meV atom^-1, force RMSE 273 meV Å^-1),<br>latent_space_used_for: sampling new structures; facilitates global optimization in latent space (conceptual),<br>benchmarks_selected: 50 most diverse via FPS + 5 reported = 53 benchmarks,<br>KNN_selected_unique_structures: 64 unique structures (from KNN clusters),<br>KNN_high_κL_rate: Over 50% of KNN-selected materials exhibit κL > 800 W m^-1K^-1 |
| **Application Domains** | Materials discovery / computational materials science,<br>Thermal management and heat transport (lattice thermal conductivity prediction),<br>Crystal structure prediction,<br>Carbon materials / carbon allotropes,<br>Atomistic simulations and interatomic potential development,<br>High-throughput virtual screening of materials |

---


### [250. Electronic Structure Guided Inverse Design Using Generative Models](https://doi.org/10.48550/arXiv.2504.06249), Preprint *(April 08, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | MP DOS (curated by this paper) |
| **Models** | Denoising Diffusion Probabilistic Model,<br>Graph Neural Network,<br>Variational Autoencoder,<br>Generative Adversarial Network,<br>Normalizing Flow |
| **Tasks** | Synthetic Data Generation,<br>Graph Generation,<br>Regression,<br>Data Generation |
| **Learning Methods** | Generative Learning,<br>Self-Supervised Learning,<br>Supervised Learning |
| **Performance Highlights** | Classifier-free MAE(ŷgen, y) T=200: 0.102,<br>Classifier-free MAE(ŷgen,ŷ) T=200: 0.049,<br>Classifier-free MAE(ŷgen, y) T=500: 0.109,<br>Classifier-free MAE(ŷgen,ŷ) T=500: 0.065,<br>Classifier-free MAE(ŷgen, y) T=1000: 0.120,<br>Classifier-free MAE(ŷgen,ŷ) T=1000: 0.088,<br>Structure match % (Classifier-free) T=200: 81.6,<br>Composition match % (Classifier-free) T=200: 94.4,<br>Structure match % (Classifier-free) T=500: 42.0,<br>Composition match % (Classifier-free) T=500: 73.0,<br>Structure match % (Classifier-free) T=1000: 14.7,<br>Composition match % (Classifier-free) T=1000: 58.7,<br>Generated set size for large-scale screening: 10,000,<br>Post-filtering candidates (formation energy ≤ -1.5 eV/atom): 108,<br>Selected for DFT validation: 8,<br>Classifier MAE(ŷgen, y) T=200: 0.114,<br>Classifier MAE(ŷgen,ŷ) T=200: 0.068,<br>Classifier MAE(ŷgen, y) T=500: 0.200,<br>Classifier MAE(ŷgen,ŷ) T=500: 0.182,<br>Classifier MAE(ŷgen, y) T=1000: 0.268,<br>Classifier MAE(ŷgen,ŷ) T=1000: 0.256,<br>Structure match % (Classifier) T=200: 63.5,<br>Composition match % (Classifier) T=200: 73.8,<br>Structure match % (Classifier) T=500: 4.23 (some exclusions),<br>Composition match % (Classifier) T=500: 6.01 (some exclusions),<br>Structure match % (Classifier) T=1000: 0.07,<br>Composition match % (Classifier) T=1000: 2.95,<br>Failures mapping to surrogate at T=1000 (Classifier): 314 out of 10,158,<br>Surrogate forward model MAE(ŷ, y): 0.096 |
| **Application Domains** | materials discovery / inverse materials design,<br>electronic structure prediction and design (density of states conditioned design),<br>catalysis (design of catalytic materials),<br>photovoltaics,<br>superconductors,<br>computational materials science (high-throughput screening with MLFF and DFT validation) |

---


### [249. Leveraging data mining, active learning, and domain adaptation for efficient discovery of advanced oxygen evolution electrocatalysts](https://doi.org/10.1126/sciadv.adr9038), Science Advances *(April 04, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Domain-knowledge literature dataset (full),<br>Domain-knowledge literature dataset (high-quality subset),<br>Active-learning experimental dataset (DASH experimental runs),<br>High-throughput DFT dataset (source domain S),<br>DFT dataset (target domain T) |
| **Models** | Support Vector Machine,<br>Gradient Boosting Tree,<br>XGBoost,<br>LightGBM,<br>CatBoost,<br>Random Forest,<br>Decision Tree,<br>XGBoost,<br>Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>Recurrent Neural Network,<br>Long Short-Term Memory,<br>Gated Recurrent Unit,<br>Bidirectional LSTM,<br>Temporal Convolutional Network |
| **Tasks** | Regression,<br>Dimensionality Reduction,<br>Feature Selection,<br>Feature Extraction,<br>Experimental Design,<br>Optimization,<br>Hyperparameter Optimization |
| **Learning Methods** | Unsupervised Learning,<br>Supervised Learning,<br>Active Learning,<br>Domain Adaptation,<br>Transfer Learning,<br>Fine-Tuning,<br>Ensemble Learning,<br>Boosting |
| **Performance Highlights** | R2: 0.84,<br>MAE: 29.76 mV (for η10 on full dataset),<br>MAE: 27.21 mV (for η10 on high-quality dataset),<br>R2 (stability, full dataset): 0.86,<br>R2: 0.89 (stability on high-quality dataset),<br>R2: close to or over 0.99 on Dataset S (source domain),<br>R2_on_T: substantially improved over Committee T (exact numeric improvement not specified),<br>R2_on_S_post-adaptation: primarily in range 0.8 to 0.9 (retained predictive power on S),<br>best_η10_over_iterations: decreased from 209 mV to 154 mV,<br>failure_rate_first_batch: 47%,<br>failure_rate_fourth_and_fifth_batches: 0%,<br>total_experimental_samples: 258 samples over five iterations,<br>DFT_theoretical_overpotential_for_sample_C: 376 mV in commonly studied descriptor scenario,<br>stability_descriptors_for_sample_C: Udiss = 3.34 V; ΔGVO = 3.84 eV,<br>experimental_decay_rate_for_sample_C: 0.1728 and 0.1964 mV hour−1 at 10 and 20 mA cm−2 over 125 hours |
| **Application Domains** | Materials Science,<br>Electrocatalysis,<br>Acidic Oxygen Evolution Reaction (OER),<br>Proton Exchange Membrane (PEM) Water Electrolysis,<br>Computational Materials Science / DFT surrogate modeling |

---


### [248. A high-throughput experimentation platform for data-driven discovery in electrochemistry](https://doi.org/10.1126/sciadv.adu4391), Science Advances *(April 04, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | LCE dataset of electrolyte formulations (final),<br>Library of 180 small-molecule additives,<br>DFT descriptors dataset (quantum chemistry descriptors for selected additives),<br>Coin cell cycling validation dataset (reservoir half-cell protocol),<br>Reproducibility CE measurements (platform benchmarking) |
| **Models** | Linear Model,<br>Support Vector Machine,<br>Gaussian Process,<br>Random Forest,<br>Gradient Boosting Tree,<br>XGBoost,<br>Multi-Layer Perceptron |
| **Tasks** | Regression,<br>Feature Selection,<br>Feature Extraction,<br>Data Generation |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Stacking,<br>Feature Selection |
| **Performance Highlights** | R2: 0.86,<br>RMSE: 0.142,<br>MAE: 0.104,<br>R2: 0.81,<br>RMSE: 0.165,<br>MAE: 0.112,<br>R2: 0.32,<br>RMSE: 0.310,<br>MAE: 0.251,<br>RMSE_percent_CE: 2.074%,<br>MAE_percent_CE: 1.768%,<br>Predicted_CE_percent: 98.02%,<br>Predicted_LCE: 1.703,<br>Experimental_average_CE_percent_over_200_cycles: 99.52% |
| **Application Domains** | Electrochemistry,<br>Aqueous zinc metal batteries (AZMBs) / Battery research,<br>Energy storage and conversion,<br>Automated high-throughput experimentation (HTE) for materials discovery,<br>Materials science (electrode/electrolyte optimization, electroplating),<br>Electrocatalysis / electro-organic synthesis / corrosion studies (noted as broader applicability) |

---


### [247. Physics-informed, dual-objective optimization of high-entropy-alloy nanozymes by a robotic AI chemist](https://doi.org/10.1016/j.matt.2025.102009), Matter *(April 02, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Web of Science peroxidase abstracts (literature + patents),<br>MD / composition dataset (simulated HEA compositions),<br>DFT surface-structure dataset,<br>Experimental HEA dataset (synthesized and measured),<br>ML training split for thermodynamic NN models |
| **Models** | Gaussian Process,<br>Feedforward Neural Network,<br>Multi-Layer Perceptron,<br>XGBoost,<br>GPT |
| **Tasks** | Optimization,<br>Regression,<br>Binary Classification,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Boosting,<br>Ensemble Learning,<br>In-Context Learning |
| **Performance Highlights** | best_Vmax/KM_PI-DO-BO_step12_s^-1: 2.973e-3,<br>best_Vmax/KM_PI-DO-BO_step12_s^-1_alt: 2.19e-3,<br>kcat/KM (derived) M^-1 s^-1: 4.41e7 and 3.25e7,<br>baseline_random_highest_Vmax/KM_s^-1: 6.18e-5,<br>standard_DO-BO_highest_Vmax/KM_s^-1: 2.86e-5,<br>step6_highest_Vmax/KM_s^-1: <1.03e-4,<br>steps7-10_highest_Vmax/KM_s^-1: 1.58e-3,<br>natural_HRP_kcat/KM_M^-1 s^-1 (reported literature): 9.42e5,<br>training_dataset_size: 12,205 compositions (80% train / 20% test),<br>classification_target: DG--OH_des class 0/1 (threshold: -1 eV),<br>impact_on_search_efficiency: Not provided as single numeric ML metric; reflected by improvement in discovered catalytic efficiencies (Vmax/KM increased from <1e-4 to 2.973e-3 after GPT-in-the-loop introduction),<br>calibration_model_accuracy: reported as high predictive accuracies for formulation→composition NN (see Figures S7 F–S7O), no numeric test error in main text |
| **Application Domains** | materials science,<br>catalysis,<br>nanozymes / enzymatic mimics,<br>automated materials discovery / robotic experimentation,<br>computational materials chemistry (DFT/MD + ML integration),<br>chemical synthesis optimization |

---


### [246. Towards multimodal foundation models in molecular cell biology](https://doi.org/10.1038/s41586-025-08710-y), Nature *(April 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Human Cell Atlas (HCA),<br>Human Biomolecular Atlas Program (HuBMAP),<br>Human Tumor Atlas Network (HTAN),<br>ENCODE,<br>International Human Epigenome Consortium (IHEC),<br>CellxGENE aggregated collection (including HCA and HuBMAP),<br>Perturb-seq / large-scale CRISPR perturbation datasets,<br>Protocol-specific paired-modal datasets (10x Multiome, CITE-seq, ASAP-seq, TEA-seq etc.),<br>Reference pretraining collections cited (examples used to illustrate scale) |
| **Models** | Transformer,<br>GPT,<br>BERT,<br>Swin Transformer,<br>Autoencoder,<br>Graph Neural Network |
| **Tasks** | Clustering,<br>Dimensionality Reduction,<br>Classification,<br>Regression,<br>Sequence-to-Sequence,<br>Time Series Forecasting,<br>Data Generation,<br>Synthetic Data Generation,<br>Graph Generation |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Transfer Learning,<br>Fine-Tuning,<br>In-Context Learning,<br>Contrastive Learning,<br>Supervised Learning,<br>Prompt Learning,<br>Active Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | Molecular cell biology,<br>Genomics,<br>Transcriptomics (single-cell RNA-seq),<br>Epigenomics (chromatin accessibility, methylation),<br>Proteomics,<br>Metabolomics,<br>Spatial profiling / spatial transcriptomics,<br>Drug discovery and perturbation response prediction,<br>Biomarker discovery,<br>Personalized medicine / clinical cohort analysis |

---


### [245. Applications of natural language processing and large language models in materials discovery](https://doi.org/10.1038/s41524-025-01554-0), npj Computational Materials *(March 24, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials-related abstracts (Tshitoyan corpus),<br>Materials-related abstracts (Pei corpus),<br>ChemDataExtractor auto-generated datasets (perovskite, dye-sensitized, band gaps, etc.),<br>800 hand-annotated NER corpus (Weston et al.),<br>Polymer annotated abstracts (Shetty et al.),<br>Superalloy dataset (Wang et al.),<br>Superalloy article corpus (processing/synthesis actions),<br>Solid-state synthesis recipes knowledge base,<br>Large article corpus for oxide synthesis extraction (Kim et al.),<br>Perovskite solar cell dataset (Xie et al.),<br>TransPolymer pretraining data (PI1M augmented),<br>polyBERT training data (hypothetical polymers),<br>Steel corpus for SteelBERT,<br>Domain-specific corpora: MatSciBERT / SCIBERT / MaterialBERT / BatteryBERT / OpticalBERT |
| **Models** | Transformer,<br>BERT,<br>GPT,<br>Attention Mechanism,<br>Self-Attention Network,<br>Seq2Seq,<br>Bidirectional LSTM,<br>Conditional Random Field,<br>Latent Dirichlet Allocation,<br>Variational Autoencoder |
| **Tasks** | Information Retrieval,<br>Named Entity Recognition,<br>Relation Extraction,<br>Text Classification,<br>Sequence-to-Sequence,<br>Clustering,<br>Question Answering,<br>Information Retrieval,<br>Regression,<br>Data Generation,<br>Structured Prediction,<br>Sequence Labeling |
| **Learning Methods** | Self-Supervised Learning,<br>Supervised Learning,<br>Semi-Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Prompt Learning,<br>Few-Shot Learning,<br>Zero-Shot Learning,<br>Transfer Learning,<br>Reinforcement Learning,<br>Multi-Task Learning,<br>Knowledge Distillation,<br>Contrastive Learning,<br>In-Context Learning |
| **Performance Highlights** | F1-score: 87%,<br>extracted_records: ~300,000 polymer property records,<br>precision: 90–99%,<br>recall: 90–99%,<br>F1-score: 90–99%,<br>F1-score: up to 0.98,<br>accuracy: 0.96,<br>bulk_modulus_precision: 90.8%,<br>bulk_modulus_recall: 87.7%,<br>critical_cooling_precision: 91.6%,<br>critical_cooling_recall: 83.6%,<br>schema_generation_F1: 87.14%,<br>R2_yield_strength: 78.17% (±3.40%),<br>R2_ultimate_tensile_strength: 82.56% (±1.96%),<br>R2_total_elongation: 81.44% (±2.98%),<br>text_search_accuracy: 96.9%,<br>property_prediction_accuracy: 95.7%,<br>structure_generation_accuracy: 87.5%,<br>MaScQA_improvement: up to 20.61%,<br>SciQA_improvement: up to 45.73%,<br>extraction_accuracy: 73% (parsing success) |
| **Application Domains** | Materials science (general),<br>Alloys and superalloys,<br>Polymers,<br>Perovskite solar cells / photovoltaics,<br>Metal-organic frameworks (MOFs),<br>Catalysis (inorganic catalysts / binary alloy catalysts),<br>Inorganic materials synthesis (solid-state, solution-based),<br>Steels and metallurgy,<br>Batteries (battery materials),<br>Optical materials,<br>Thermoelectrics,<br>Metallic glasses,<br>Cement and concrete,<br>Two-dimensional materials,<br>Autonomous chemical experiments / laboratory automation |

---


### [244. Elemental numerical descriptions to enhance classification and regression model performance for high-entropy alloys](https://doi.org/10.1038/s41524-025-01560-2), npj Computational Materials *(March 18, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | HEA phase dataset (Al-Ti-V-Cr-Fe-Co-Ni-Cu),<br>Hardness dataset for HEAs,<br>Virtual composition space (sampling pool),<br>Experimental validation set (synthesized HEAs A1-A15),<br>Six additional materials datasets (tested for generality) |
| **Models** | Generalized Linear Model,<br>Gradient Boosting Tree,<br>Decision Tree,<br>Random Forest,<br>Naive Bayes,<br>Multi-Layer Perceptron,<br>Support Vector Machine |
| **Tasks** | Binary Classification,<br>Multi-class Classification,<br>Regression,<br>Feature Extraction,<br>Feature Selection |
| **Learning Methods** | Supervised Learning,<br>Evolutionary Learning,<br>Active Learning,<br>Reinforcement Learning,<br>Boosting,<br>Ensemble Learning |
| **Performance Highlights** | accuracy: 0.87,<br>accuracy: 0.97,<br>MAE: 45.8 HV,<br>R2: 0.88,<br>R2: improved (values not explicitly listed in main text),<br>MAPE: improved (values not explicitly listed in main text),<br>accuracy_gain_range: 3% to 22%,<br>experimental_validation: 13/15 correct for SS/NSS; 8/9 correct for FCC/BCC/DP |
| **Application Domains** | Materials science,<br>High-entropy alloys (HEAs) phase prediction and property prediction,<br>Mechanical property prediction (hardness, high-temperature strength, fracture strain),<br>Functional ceramics (BaTiO3: piezoelectric d33, electrostrain, dielectric energy storage density),<br>Shape memory alloys (NiTi-based transformation temperatures) |

---


### [242. Transformers and genome language models](https://doi.org/10.1038/s42256-025-01007-9), Nature Machine Intelligence *(March 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | GenBank,<br>RefSeq,<br>Sequence Read Archive (SRA),<br>ENCODE,<br>Roadmap Epigenomics,<br>GTEx,<br>1000 Genomes Project,<br>GenomicBenchmarks (GenomicBenchmarks / GenomicBenchmarks datasets),<br>Genome Understanding Evaluation (GUE),<br>BEND (Benchmarking DNA Language models),<br>NCBI Genome (eight Brassicales reference genomes),<br>Single reference human genome (one reference genome),<br>Prokaryotic genomes (whole genomes),<br>CRISPR perturbation screens (reference experiments) |
| **Models** | Transformer,<br>BERT,<br>GPT,<br>Convolutional Neural Network,<br>Recurrent Neural Network,<br>Encoder-Decoder,<br>Seq2Seq,<br>Attention Mechanism,<br>Self-Attention Network,<br>Multi-Head Attention,<br>State Space Model,<br>U-Net |
| **Tasks** | Language Modeling,<br>Self-Supervised Learning,<br>Classification,<br>Binary Classification,<br>Regression,<br>Structured Prediction,<br>Sequence-to-Sequence,<br>Language Modeling,<br>Feature Extraction,<br>Anomaly Detection |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Transfer Learning,<br>Unsupervised Learning,<br>Contrastive Learning,<br>Few-Shot Learning,<br>Zero-Shot Learning,<br>Supervised Learning,<br>Language Modeling,<br>Representation Learning |
| **Performance Highlights** | accuracy_description: high accuracy reported (no numeric value provided in paper),<br>comparative_performance: DNABERT-2 performs comparably to Nucleotide Transformer on several tasks despite 21x fewer parameters (no numeric metrics provided),<br>zero_few_shot_rank: Nucleotide Transformer Multi-Species and original DNABERT had the best zero- and few-shot embeddings (qualitative, no numeric values given),<br>benchmark_performance: HyenaDNA achieved 'state-of-the-art performance on all eight datasets from GenomicBenchmarks' (qualitative statement, no numeric values provided),<br>comparative_performance: Hybrid models combining CNN + transformer (e.g., Enformer, Borzoi) show improved assay prediction and increased receptive field; exact numeric metrics not provided in review,<br>efficiency_claim: Selective SSMs / Mamba and Hyena-like layers claim improved scaling (subquadratic) and competitive accuracy with lower compute cost (qualitative in paper),<br>specific_task_performance: HyenaDNA reported strong performance at much larger context windows (1 million nucleotides) and strong benchmark results (see HyenaDNA entry).,<br>generalization_claim: Models pretrained on inter-species (multi-species) data generalize better on human prediction tasks than intra-species (population-scale) pretraining (qualitative claim, no numeric metrics provided),<br>qualitative: GPN (a convolutional/transformer-modified model) learned non-coding variant effects from unsupervised pretraining and outperformed supervised DeepSEA in reported comparisons (qualitative claim in review),<br>embedding_distance_use: Cosine distances between reference and variant sequence embeddings used to indicate functional differences (method described; no numeric metrics given) |
| **Application Domains** | genomics / regulatory genomics,<br>functional genomics (TF-binding, chromatin accessibility, histone marks),<br>3D genome architecture and chromatin contact mapping,<br>variant effect prediction and interpretation,<br>gene expression prediction (bulk and single-cell contexts),<br>computational biology and network biology,<br>synthetic biology (in silico perturbation and design),<br>personalized medicine and clinical genomics,<br>drug discovery (future direction),<br>single-cell transcriptomics (scRNA-seq foundation models) |

---


### [241. The deep finite element method: A deep learning framework integrating the physics-informed neural networks with the finite element method](https://doi.org/10.1016/j.cma.2024.117681), Computer Methods in Applied Mechanics and Engineering *(March 01, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Plate with a circular hole (case study),<br>Non-symmetric stretching plate (case study),<br>Rock drill boom (3D case study),<br>FEM (Abaqus) reference solutions |
| **Models** | Multi-Layer Perceptron,<br>Feedforward Neural Network |
| **Tasks** | Regression,<br>Optimization |
| **Learning Methods** | Pre-training,<br>Fine-Tuning,<br>Stochastic Gradient Descent,<br>Gradient Descent,<br>Backpropagation,<br>Supervised Learning |
| **Performance Highlights** | plate_with_hole_relative_error_max_DFEM_selected_points: <= 0.16 %,<br>plate_with_hole_relative_error_range_DEM_selected_points: 10.25 % - 16.71 %,<br>plate_with_hole_max_abs_error_DFEM_u: 0.02,<br>plate_with_hole_max_abs_error_DFEM_v: 0.015,<br>plate_with_hole_max_abs_error_DEM_u: 0.2,<br>plate_with_hole_max_abs_error_DEM_v: 0.08,<br>training_time_reduction_near_pretraining_load: DFEM ~1/8 of FEM time (example),<br>pretraining_convergence_epochs: 8-10 epochs (fine-tuning convergence reported),<br>rock_drill_boom_elements: 46,337,<br>rock_drill_boom_nodal_points: 10,205,<br>rock_drill_boom_max_error_u: 2.5e-3,<br>rock_drill_boom_max_error_v: 1.8e-3,<br>rock_drill_boom_max_error_w: 1.3e-2,<br>rock_drill_boom_avg_error_u: 3.2e-4,<br>rock_drill_boom_avg_error_v: 4.6e-4,<br>rock_drill_boom_avg_error_w: 2.8e-3,<br>rock_drill_boom_relative_errors_selected_points_DFEM: <= 0.54 %,<br>computation_time_reduction_with_pretraining_vs_FEM: ≈ 66% (example from Table 6),<br>DFEM_relative_error_table3_example_Point1: 0.62 %,<br>DEM_relative_error_table3_example_Point1_(2500_iter): 14.68 %,<br>DEM_relative_error_table3_example_Point1_(5000_iter): 99.99 %,<br>DEM_special_training_relative_error_Point1: 1.15 %,<br>DFEM_error_reduction_vs_DEM: up to 99% reduction in relative error (reported across cases) |
| **Application Domains** | Solid elasticity mechanics,<br>Computational mechanics,<br>Three-dimensional structural analysis,<br>Engineering structural analysis,<br>Digital twin applications |

---


### [240. A generative model for inorganic materials design](https://doi.org/10.1038/s41586-025-08628-5), Nature *(March 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Alex-MP-20,<br>Alex-MP-ICSD,<br>Materials Project (MP),<br>Alexandria,<br>Labelled magnetic density dataset,<br>Labelled bandgap dataset,<br>Labelled bulk modulus dataset,<br>ICSD (test structures) |
| **Models** | Denoising Diffusion Probabilistic Model,<br>Variational Autoencoder,<br>Graph Neural Network,<br>Feedforward Neural Network |
| **Tasks** | Synthetic Data Generation,<br>Graph Generation,<br>Regression,<br>Optimization,<br>Experimental Design |
| **Learning Methods** | Pre-training,<br>Fine-Tuning,<br>Supervised Learning,<br>Unsupervised Learning,<br>Generative Learning,<br>Transfer Learning |
| **Performance Highlights** | energy_above_hull_below_0.1_eV_MP: 78%,<br>energy_above_hull_below_0_eV_MP: 13%,<br>energy_above_hull_below_0.1_eV_Alex-MP-ICSD: 75%,<br>energy_above_hull_below_0_eV_Alex-MP-ICSD: 3%,<br>RMSD_below_0.076_A_for_generated_structures: 95% (of generated 1,024 structures),<br>unique_at_1000_samples: 100%,<br>unique_at_10_million_samples: 52%,<br>new_structures_fraction: 61% (new vs Alex-MP-ICSD),<br>SUN_increase_vs_CDVAE_and_DiffCSP: 60% more SUN structures,<br>average_RMSD_reduction_vs_baselines: 50% lower RMSD,<br>SUN_increase_over_MatterGen-MP: 70% increase,<br>RMSD_decrease_over_MatterGen-MP: 5x decrease,<br>overall_improvement_vs_prior_SOTA: more than 2x likelihood to be SUN; up to order-of-magnitude closer to local energy minimum,<br>labelled_dataset_size: 605,000 DFT magnetic density labels,<br>SUN_structures_with_mag_density_>0.2_A^-3_using_180_DFT_calcs: up to 18,<br>labelled_dataset_size: 42,000 DFT bandgap labels,<br>target_bandgap: 3.0 eV,<br>distribution_shift_towards_target: substantial shift in property distribution among SUN samples towards desired target (Fig. 4b),<br>labelled_dataset_size: 5,000 DFT bulk modulus labels,<br>SUN_found_with_budget_180_DFT_calcs: 106 SUN structures (95 distinct compositions),<br>screening_baseline_with_budget_180_DFT_calcs: 40 SUN structures (28 distinct compositions),<br>DFT_predicted_bulk_modulus_of_ordered_target_structure: 222 GPa,<br>experimental_estimated_bulk_modulus: up to 169 GPa (best of four measurements); 158 ± 11 GPa reported,<br>DFT_MAE_on_95_matches: 23 GPa,<br>DFT_RMSE_on_95_matches: 32 GPa,<br>samples_per_target_for_generation: 8,192 candidates per target bulk modulus value,<br>generated_samples_needed_for_quinary_system_performance: 10,240 (MatterGen) vs ~70,000 (substitution) vs ~600,000 (RSS),<br>higher_percentage_of_SUN_structures_across_system_types: MatterGen generates highest percentage of SUN structures for each system type and complexity (Fig. 3a,b),<br>unique_structures_on_combined_convex_hull: MatterGen finds highest number in partially and well-explored systems (Fig. 3c) |
| **Application Domains** | Materials design (inorganic crystalline materials),<br>Energy storage (materials discovery),<br>Catalysis (materials discovery),<br>Carbon capture (materials discovery),<br>Permanent magnet discovery (magnetic materials),<br>Superhard materials discovery (mechanical properties),<br>Experimental materials synthesis and validation |

---


### [239. CrystalFlow: A Flow-Based Generative Model for Crystalline Materials](https://doi.org/10.48550/arXiv.2412.11693), Preprint *(February 24, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | MP-20,<br>MPTS-52,<br>MP-CALYPSO-60 |
| **Models** | Normalizing Flow,<br>Graph Neural Network,<br>Multi-Layer Perceptron,<br>Variational Autoencoder,<br>Diffusion Model,<br>Normalizing Flow,<br>Transformer |
| **Tasks** | Data Generation,<br>Synthetic Data Generation,<br>Graph Generation,<br>Optimization |
| **Learning Methods** | Supervised Learning,<br>Backpropagation,<br>Representation Learning,<br>End-to-End Learning,<br>Gradient Descent |
| **Performance Highlights** | MP-20 k=1 MR (%): 62.02,<br>MP-20 k=1 RMSE: 0.0710,<br>MP-20 k=20 MR (%): 78.34,<br>MP-20 k=20 RMSE: 0.0577,<br>MP-20 k=100 MR (%): 82.49,<br>MP-20 k=100 RMSE: 0.0513,<br>MPTS-52 k=1 MR (%): 22.71,<br>MPTS-52 k=1 RMSE: 0.1548,<br>MPTS-52 k=20 MR (%): 40.37,<br>MPTS-52 k=20 RMSE: 0.1576,<br>MPTS-52 k=100 MR (%): 52.14,<br>MPTS-52 k=100 RMSE: 0.1603,<br>MP-CALYPSO-60 (500 generated) convergence rate CR (%) Cond-CDVAE S=5000: 82.20,<br>MP-CALYPSO-60 (500) ion-steps Cond-CDVAE S=5000: 45.91,<br>MP-CALYPSO-60 (500) CR (%) CrystalFlow S=100: 89.20,<br>MP-CALYPSO-60 (500) ion-steps CrystalFlow S=100: 49.84,<br>MP-CALYPSO-60 (500) CR (%) CrystalFlow S=1000: 90.20,<br>MP-CALYPSO-60 (500) ion-steps CrystalFlow S=1000: 39.40,<br>MP-CALYPSO-60 (500) CR (%) CrystalFlow S=5000: 90.60,<br>MP-CALYPSO-60 (500) ion-steps CrystalFlow S=5000: 39.82,<br>SiO2 case (200 samples) CR (%) Cond-CDVAE S=5000: 96.00,<br>SiO2 case (200) ion-steps Cond-CDVAE S=5000: 44.36,<br>SiO2 case (200) CR (%) CrystalFlow S=100: 100.00,<br>SiO2 case (200) ion-steps CrystalFlow S=100: 35.65,<br>SiO2 case (200) CR (%) CrystalFlow S=1000: 100.00,<br>SiO2 case (200) ion-steps CrystalFlow S=1000: 35.84,<br>SiO2 case (200) CR (%) CrystalFlow S=5000: 100.00,<br>SiO2 case (200) ion-steps CrystalFlow S=5000: 31.99,<br>DNG: structural validity (%): 99.55,<br>DNG: compositional validity (%): 81.96,<br>DNG: coverage recall (%): 98.21,<br>DNG: coverage precision (%): 99.84,<br>DNG: wdist(density): 0.169 (smallest among compared),<br>DNG: wdist(Nel): 0.259,<br>Inference time (min/10k generated) CrystalFlow RTX 4090 S=100: 4.1,<br>Inference time (min/10k) CrystalFlow RTX 4090 S=1000: 37.0,<br>Baseline DiffCSP RTX 4090 S=1000: 44.7,<br>FlowMM A800 S=750: 65.1,<br>FlowLLM A800 S=250: 89.6,<br>MP-20 k=1 MR (%) CDVAE: 33.90,<br>MP-20 k=1 RMSE CDVAE: 0.1045,<br>MPTS-52 k=1 MR (%) CDVAE: 5.34,<br>MPTS-52 k=1 RMSE CDVAE: 0.2106,<br>MP-20 k=1 MR (%) DiffCSP: 51.49,<br>MP-20 k=1 RMSE DiffCSP: 0.0631,<br>MPTS-52 k=1 MR (%) DiffCSP: 12.19,<br>MPTS-52 k=1 RMSE DiffCSP: 0.1786,<br>MP-20 k=20 MR (%) DiffCSP: 77.93,<br>MP-20 k=20 RMSE DiffCSP: 0.0492,<br>MPTS-52 k=20 MR (%) DiffCSP: 34.02,<br>MPTS-52 k=20 RMSE DiffCSP: 0.1749,<br>MP-20 k=1 MR (%) FlowMM: 61.39,<br>MP-20 k=1 RMSE FlowMM: 0.0566,<br>MPTS-52 k=1 MR (%) FlowMM: 17.54,<br>MPTS-52 k=1 RMSE FlowMM: 0.1726 |
| **Application Domains** | Crystalline materials generation,<br>Crystal structure prediction (CSP),<br>Inverse materials design / de novo materials generation,<br>High-pressure materials discovery (pressure-conditioned generation),<br>Computational condensed matter physics and materials science |

---


### [238. Genome modeling and design across all domains of life with Evo 2](https://doi.org/10.1101/2025.02.18.638918), Preprint *(February 21, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | OpenGenome2,<br>GTDB representative prokaryotic genomes,<br>Eukaryotic reference genomes (NCBI),<br>Metagenomic sequences (curated),<br>Organelle genomes (NCBI Organelle),<br>mRNA and ncRNA transcripts (GTF-derived),<br>EPDnew promoter sequences,<br>ClinVar (2024.02.28 release),<br>SpliceVarDB,<br>BRCA1 saturation mutagenesis dataset (Findlay et al. 2018),<br>BRCA2 variant dataset (Huang et al., 2025),<br>Deep Mutational Scanning (DMS) datasets (ProteinGym and other compendia),<br>DEG (Database of Essential Genes) and phage essentiality screens,<br>lncRNA essentiality screens (Liang et al., 2024),<br>Woolly mammoth genome (Sandoval-Velasco et al., 2024),<br>Mycoplasma genitalium reference genome (Gibson et al., 2008),<br>Saccharomyces cerevisiae chromosome III |
| **Models** | Convolutional Neural Network,<br>Attention Mechanism,<br>Self-Attention Network,<br>Multi-Head Attention,<br>Autoencoder,<br>Feedforward Neural Network,<br>Multi-Layer Perceptron,<br>Transformer |
| **Tasks** | Regression,<br>Binary Classification,<br>Sequence Labeling,<br>Synthetic Data Generation,<br>Feature Extraction,<br>Optimization,<br>Experimental Design,<br>Classification |
| **Learning Methods** | Self-Supervised Learning,<br>Zero-Shot Learning,<br>Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Representation Learning,<br>Contrastive Learning,<br>Ensemble Learning |
| **Performance Highlights** | Spearman_correlation_DMS: competitive with state-of-the-art autoregressive protein language models; state-of-the-art on noncoding RNA fitness prediction (no single aggregated numeric value reported in main text),<br>ClinVar_noncoding_SNV_AUROC_Evo2_40B: 0.987,<br>ClinVar_noncoding_SNV_AUPRC_Evo2_40B: 0.974,<br>ClinVar_coding_SNV_AUROC_Evo2_40B: 0.841,<br>ClinVar_coding_SNV_AUPRC_Evo2_40B: 0.889,<br>SpliceVarDB_intronic_SNV_AUROC_Evo2_40B: 0.926,<br>SpliceVarDB_intronic_SNV_AUPRC_Evo2_40B: 0.971,<br>SpliceVarDB_exonic_SNV_AUROC_Evo2_40B: 0.684,<br>SpliceVarDB_exonic_SNV_AUPRC_Evo2_40B: 0.523,<br>BRCA1_supervised_test_AUROC: 0.95,<br>BRCA1_supervised_test_AUPRC: 0.86,<br>BRCA1_coding_SNV_test_AUROC: 0.94,<br>BRCA1_coding_SNV_test_AUPRC: 0.84,<br>Exon_classifier_AUROC_range: 0.82-0.99 across eight held-out species,<br>SAE_features_identified: features corresponded to exon/intron boundaries, transcription factor motifs, protein secondary structure, prophage regions (qualitative mapping; numeric F1/precision/recall reported for certain features e.g., exon/intron features evaluated on 1,000 genes but aggregate values varied per feature),<br>Designed_chromatin_AUROC: ≈0.9 for many patterns when sampling >=30 128-bp chunks and selecting top-2 chunks per step; inference-time scaling shows log-linear improvement with more sampled tokens,<br>Token_sampling_examples: sampling 30 or more 128-bp chunks and selecting top 2 chunks per step sufficient to achieve AUROCs around 0.9,<br>Mitochondrial_generation_gene_counts: Evo 2 generated mitochondria with the correct number of CDS, tRNA, and rRNA genes (annotation via MitoZ); BLASTp analyses show varied percent identity to natural proteins (see Table S6: many genes with high percent identity; examples: mt-Atp6 91.59% to Alcelaphus buselaphus; mt-Nd6 94.43% to Neolissochilus hendersoni),<br>M_genitalium_Pfam_hit_rate: ∼70% of Prodigal-annotated Evo 2 40B genes had significant Pfam/HHpred hits (E-value < 0.001), versus Evo 1 131k at ∼18%,<br>Yeast_generation_feature_presence: Generated yeast chromosomes contained predicted tRNAs, promoters, and intronic structure albeit at lower density than native genome (quantified distributions in Figures 5L and S8) |
| **Application Domains** | genomics (prokaryotic and eukaryotic),<br>clinical genetics / variant interpretation,<br>molecular biology (protein/RNA function prediction),<br>synthetic biology / genome design,<br>epigenomics (chromatin accessibility design),<br>comparative genomics and paleogenomics,<br>bioinformatics / genome annotation,<br>protein structure prediction (downstream evaluation via AlphaFold/ESMFold) |

---


### [237. Accelerating crystal structure search through active learning with neural networks for rapid relaxations](https://doi.org/10.1038/s41524-025-01523-7), npj Computational Materials *(February 20, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Initial candidate pools (benchmark systems: Si16, Na8Cl8, Ga8As8, Al4O6),<br>Transferability candidate pools (Si transfer / Al2O3 transfer experiments),<br>Training datasets (iteratively labeled by DFT during active learning),<br>MaterialsProject reference structures (target minima) |
| **Models** | Message Passing Neural Network,<br>Multi-Layer Perceptron,<br>Graph Neural Network,<br>Gaussian Process |
| **Tasks** | Regression,<br>Optimization,<br>Clustering,<br>Data Generation,<br>Feature Extraction,<br>Representation Learning |
| **Learning Methods** | Active Learning,<br>Supervised Learning,<br>Ensemble Learning,<br>Unsupervised Learning,<br>Transfer Learning,<br>Representation Learning,<br>Pre-training |
| **Performance Highlights** | single-point DFT evaluations until convergence (benchmarks): 400–700,<br>reduction_in_DFT_evaluations: up to two orders of magnitude (compared to AIRSS, Bayesian optimization, LAQA baselines),<br>avoided_DFT: up to 95% of demanding DFT calculations (statement in Discussion),<br>median_validation_steps_per_structure: 10.5 steps,<br>median_energy_difference_after_validation: 2.6 meV/atom,<br>Si transferability_total_DFT_CPU_hours: transfer: ~4322 CPU hours vs baseline: ~8863 CPU hours (≈58% reduction),<br>Al2O3 transferability_reduction: ≈74% reduction in computational cost for DFT calculations (CPU hours) for Al2O3 transferability experiments,<br>DFT_evaluations_for_transfer_Si: ~500 single-point DFT evaluations until convergence (per transfer simulation, ~4–5 cycles),<br>reported_issue: learning interatomic forces with graph neural networks on relaxation trajectories of crystal structures has shown poor performance (cited Ref.66) |
| **Application Domains** | crystal structure search,<br>materials discovery / computational materials science,<br>global optimization of crystal compositions,<br>high-throughput virtual screening (HTVS) for materials |

---


### [236. Autonomous platform for solution processing of electronic polymers](https://doi.org/10.1038/s41467-024-55655-3), Nature Communications *(February 17, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | PEDOT_experiment.csv (Polybot experimental dataset) |
| **Models** | Gaussian Process,<br>Random Forest |
| **Tasks** | Regression,<br>Regression,<br>Optimization,<br>Image Classification,<br>Dimensionality Reduction,<br>Feature Selection,<br>Hyperparameter Optimization |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Ensemble Learning |
| **Performance Highlights** | training_data_average_conductivity_S/cm: 276,<br>all_data_average_conductivity_S/cm: 664,<br>scale-up_average_conductivity_S/cm: >4500,<br>training_data_average_coverage_%: 48,<br>all_data_average_coverage_%: 72,<br>search_space_size_conditions: 933,120 possible experimental conditions,<br>experiment_iterations_run: initial 30 LHS + iterations shown 1–45 (in manuscript figures) ; termination criteria: two-week budget or performance plateau |
| **Application Domains** | Materials science,<br>Electronic polymers (PEDOT:PSS),<br>Thin-film processing and manufacturing,<br>Printable electronics / transparent conductive films,<br>Autonomous laboratories / self-driving lab platforms |

---


### [235. Developing novel low-density high-entropy superalloys with high strength and superior creep resistance guided by automated machine learning](https://doi.org/10.1016/j.actamat.2024.120656), Acta Materialia *(February 15, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Curated Ni-Fe-Co-Al-Ti-Nb-Ta-Cr-Mo-W superalloy database (domain-knowledge preprocessed) |
| **Models** | Gradient Boosting Tree |
| **Tasks** | Regression,<br>Optimization,<br>Feature Extraction,<br>Hyperparameter Optimization |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Stacking,<br>Evolutionary Learning,<br>Feature Selection |
| **Performance Highlights** | R2_training_σYS-RT: 98%,<br>RMSE_training_σYS-RT: 33~35 MPa,<br>R2_training_σYS-HT: 96%,<br>RMSE_training_σYS-HT: 40~45 MPa,<br>Reported_overall: R2 ≥ 95% and RMSE ≤ 50 MPa (summary in Conclusions),<br>AutoML_summary: AutoGluon autotuning and ensembling produced high predictive accuracy (see R2/RMSE above),<br>population_generated: 46,380,000 candidate compositions generated when sampling wide composition ranges (reported as possible population),<br>GA_iterations: GA terminated after 100 iterations; convergence observed after ~30 iterations in example,<br>candidate_reduction_example: Without cluster-formula constraint, 156 candidate compositions correspond to σYSs = 1150 MPa; with cluster-formula constraint only R2 composition selected |
| **Application Domains** | Materials science,<br>Alloy design / metallurgy,<br>High-temperature structural materials (superalloys),<br>Automated materials discovery (AI-guided composition design) |

---


### [234. Machine Learning in Solid-State Hydrogen Storage Materials: Challenges and Perspectives](https://doi.org/10.1002/adma.202413430), Advanced Materials *(February 12, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | HydPARK,<br>ML-HydPARK v0.0.0,<br>ML-HydPARK v0.0.1,<br>ML-HydPARK v0.0.2,<br>ML-HydPARK v0.0.3,<br>ML-HydPARK v0.0.4,<br>ML-HydPARK v0.0.5,<br>Complex and high-density HSMs proprietary dataset,<br>AB2-type HSMs dataset (proprietary / curated),<br>Mg-based HSMs (DFT data),<br>Mg-based experimental dataset,<br>Materials Project (MP),<br>Crystallography Open Database (COD),<br>ICSD / OQMD / NOMAD / AFLOWLIB / Pauling file / NIST,<br>Proprietary LiBH4 catalyst dataset,<br>Hypothetical ternary borohydrides dataset (CGCNN training),<br>MaterialClouds 2D structure database (used for 2D MgH2 sheets GAN) |
| **Models** | Linear Model,<br>Support Vector Machine,<br>K-Nearest Neighbors,<br>Decision Tree,<br>Random Forest,<br>Gradient Boosting Tree,<br>XGBoost,<br>Gaussian Process,<br>Multi-Layer Perceptron,<br>Feedforward Neural Network,<br>Convolutional Neural Network,<br>Graph Neural Network,<br>Gaussian Process,<br>Generative Adversarial Network,<br>Deep Convolutional GAN,<br>Denoising Diffusion Probabilistic Model,<br>Crystal Graph Convolutional Neural Network,<br>MEGNet (Materials Graph Network) — mapped to Graph Neural Network,<br>DeePMD (Deep Potential Molecular Dynamics),<br>SchNet (as part of SchNet-SSCHA),<br>Ensemble models (stacking / bagging / boosting),<br>Radial Basis Function Network |
| **Tasks** | Regression,<br>Multi-class Classification,<br>Clustering,<br>Image Generation,<br>Data Generation / Synthetic Data Generation,<br>Clustering (k-means),<br>Feature Selection / Dimensionality Reduction,<br>Image Classification |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Transfer Learning,<br>Semi-Supervised Learning,<br>Active Learning,<br>Ensemble Learning,<br>Bagging,<br>Boosting,<br>Stacking,<br>Evolutionary Learning,<br>Pre-training / Fine-Tuning,<br>Active/Automated Data Augmentation (GAN/VAE),<br>Multi-Objective Optimization (Bayesian Optimization / MOBO) |
| **Performance Highlights** | MAE: 0.003 wt% H2,<br>RMSE: 0.012 wt% H2,<br>R2: 0.83,<br>Accuracy: 0.8,<br>Number_of_clusters: 3,<br>Intra-cluster_cohesion: 0.6,<br>Inter-cluster_separation: 1.4,<br>MSE: 0.102 eV^2,<br>MAE_train: 0.47,<br>MAE_test: 1.52,<br>MAE: 8.56 kJ mol^-1,<br>MRE: 28%,<br>HYST_R2: 0.81,<br>HYST_MAE: 0.45 wt% H2,<br>THOR_R2: 0.89,<br>THOR_MAE: 4.53 kJ mol^-1 H2,<br>Hydride_formation_enthalpy_R2: 0.647,<br>Hydride_enthalpy_MAE: 4.36 kJ mol^-1 H2,<br>Phase_abundance_R2: 0.832,<br>Hydrogen_storage_capacity_R2: 0.688,<br>Hydrogen_storage_capacity_MAE: 0.101 wt% H2,<br>R2: 0.969,<br>MRE: 2.291%,<br>MSE: 3.909 kJ^2 mol^-2 H2,<br>RMSE: 2.501 kJ mol^-1 H2,<br>STD: 1.878 kJ mol^-1 H2,<br>Best_model_R2: 0.980,<br>STD: 0.043 wt% H2,<br>MSE: 0.002 wt%2 H2,<br>RMSE: 0.045 wt% H2,<br>MAPE_max_discharge: 2.35%,<br>RMSE_max_discharge: 9.74 mAh/g,<br>R_max_discharge: 0.808,<br>MAPE_fast_discharge: 0.89%,<br>RMSE_fast_discharge: 1.38,<br>R_fast_discharge: 0.991,<br>MAE: 8.58 kJ mol^-1,<br>RMSE: 11.73 kJ mol^-1,<br>R2: 0.783,<br>Pearson_Correlation: 0.885,<br>H/M_MAE: 0.12,<br>H/M_R2: 0.79,<br>Enthalpy_MAE: 4.2 kJ mol^-1 H2 (R2=0.90),<br>Entropy_MAE: 11 J mol^-1 K^-1 H2 (R2=0.69),<br>LnPeq_MAE: 1.1 (R2=0.93),<br>MAE: 3.1 meV/atom,<br>ΔEmono_MAE: 0.04 eV (R2=0.71),<br>ΔEdi_MAE: 0.04 eV (R2=0.93),<br>Correlation_coefficient_R: >0.95,<br>R2: 0.95,<br>RMSE: 29.98 K,<br>RMSE: 0.43 meV/atom,<br>Energy_MAE: 0.02 eV,<br>Force_MAE: 0.02 eV Å^-1,<br>Structure_validation_ratio: 96.8%,<br>Structure_generation_ratio: 87.3%,<br>MSE: 1.144 wt%2 H2,<br>RMSE: 1.066 wt% H2,<br>EV: 0.889,<br>R2: 0.888,<br>Pearson_Correlation: 0.944,<br>Spearman_Correlation: 0.949,<br>MAE: 0.063 eV/atom,<br>R2: >=0.96,<br>Max_relative_error_absorption: < 8.0%,<br>Max_relative_error_desorption: < 6.6%,<br>Temperature-based_model_R2: 0.9798,<br>Temperature-based_model_MAE: 0.046,<br>Pressure-based_model_R2: 0.9946,<br>Pressure-based_model_MAE: 0.00267,<br>MAE: 5.5 kJ mol^-1 H2 |
| **Application Domains** | solid-state hydrogen storage materials (HSMs) research and design,<br>high-throughput materials screening and discovery,<br>thermodynamic and kinetic property prediction (formation energy, hydride enthalpy, equilibrium pressure, de-/hydrogenation temperature),<br>design and optimization of hydrogen storage devices (hydride beds, reactors),<br>interatomic potential development and molecular dynamics simulation (MLIPs for Mg-H systems),<br>inverse materials design and multi-objective optimization for alloy selection,<br>data augmentation and structure generation (2D hydride structure generation via GANs),<br>electrochemical hydrogen storage (Ni-MH battery cathode performance prediction) |

---


### [232. ORGANA: A robotic assistant for automated chemistry experimentation and characterization](https://doi.org/10.1016/j.matt.2024.10.015), Matter *(February 05, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | ORGANA perception evaluation dataset,<br>Electrochemistry experiment measurements (ORGANA runs),<br>Electrochemistry experiment measurements (human chemists),<br>Solubility / recrystallization / pH experiment measurements,<br>User study data (chemists interacting with ORGANA) |
| **Models** | GPT,<br>Transformer,<br>Vision Transformer,<br>CLIP,<br>Seq2Seq |
| **Tasks** | Sequence-to-Sequence,<br>Language Modeling,<br>Zero-Shot Learning,<br>Object Detection,<br>Instance Segmentation,<br>Pose Estimation,<br>Regression,<br>Image Classification,<br>Sequence Labeling |
| **Learning Methods** | Prompt Learning,<br>In-Context Learning,<br>Zero-Shot Learning,<br>Pre-training,<br>Maximum Likelihood Estimation,<br>Fine-Tuning |
| **Performance Highlights** | solubility_accuracy_vs_literature: salt 7±2%, sugar 11±2%, alum 12±3%,<br>user_time_startup_written: 7.35 min,<br>user_time_startup_spoken: 4.27 min,<br>troubleshooting_time: 1.30 min,<br>CLAIRify_style_time: 17.65 min,<br>ORGANA_pKa1: 8.03 ± 0.17,<br>chemists_pKa1: 8.02,<br>ORGANA_slope: -61.4 ± 0.5 mV/pH unit,<br>chemists_slope: -62.7 mV/pH unit,<br>sequential_execution_time_avg: 21.67 min,<br>parallel_execution_time_avg: 17.10 min,<br>time_reduction: 21.1%,<br>sequential_planning_time: 61.52 ± 0.1 s,<br>temporal_TAMP_planning_time: 186.3 ± 46.0 s |
| **Application Domains** | chemistry lab automation,<br>electrochemistry / flow battery characterization,<br>materials discovery,<br>robotics (manipulation and motion planning),<br>robotic perception (transparent object perception),<br>human-robot interaction / usability in lab settings |

---


### [231. Harnessing Large Language Models to Collect and Analyze Metal–Organic Framework Property Data Set](https://doi.org/10.1021/jacs.4c11085), Journal of the American Chemical Society *(February 05, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | L2M3_Database (text-mined MOF dataset),<br>Extracted synthesis-condition records,<br>Density dataset (for regression experiments),<br>Cambridge Structural Database (CSD) subset / CoREMOF references |
| **Models** | GPT,<br>Transformer,<br>Graph Convolutional Network,<br>Random Forest,<br>XGBoost,<br>Support Vector Machine,<br>K-nearest neighbor |
| **Tasks** | Information Retrieval,<br>Classification,<br>Structured Prediction,<br>Regression,<br>Recommendation,<br>Question Answering,<br>Dialog Generation,<br>Entity Matching,<br>Data Generation |
| **Learning Methods** | Fine-Tuning,<br>Prompt Learning,<br>Transfer Learning,<br>Pre-training,<br>Few-Shot Learning,<br>Zero-Shot Learning,<br>Supervised Learning |
| **Performance Highlights** | Categorization_Synthesis: {'Precision': 1.0, 'Recall': 0.98, 'F1Score': 0.99},<br>Inclusion_Synthesis: {'Precision': 0.96, 'Recall': 0.91, 'F1Score': 0.94},<br>Extraction_Synthesis: {'Precision': 0.96, 'Recall': 0.9, 'F1Score': 0.93},<br>Categorization_Property: {'Precision': 0.98, 'Recall': 0.94, 'F1Score': 0.96},<br>Inclusion_Property: {'Precision': 0.98, 'Recall': 0.98, 'F1Score': 0.98},<br>Extraction_Property: {'Precision': 0.97, 'Recall': 0.9, 'F1Score': 0.93},<br>Categorization_Table: {'Precision': 0.99, 'Recall': 1.0, 'F1Score': 1.0},<br>Inclusion_Table: {'Precision': 1.0, 'Recall': 1.0, 'F1Score': 1.0},<br>Extraction_Table: {'Precision': 1.0, 'Recall': 1.0, 'F1Score': 1.0},<br>R2_TrainExp_TestExp: 0.803,<br>R2_TrainSim_TestExp: 0.495,<br>R2_TrainExp_TestExp: 0.793,<br>R2_TrainSim_TestExp: 0.469,<br>R2_TrainExp_TestExp: 0.734,<br>R2_TrainSim_TestExp: 0.415,<br>R2_TrainExp_TestExp: 0.573,<br>R2_TrainSim_TestExp: 0.151,<br>R2_TrainExp_TestExp_CGCNN: 0.815,<br>R2_TrainSim_TestExp_CGCNN: 0.4,<br>R2_TrainExp_TestExp_MOFTransformer: 0.892,<br>R2_TrainSim_TestExp_MOFTransformer: 0.38,<br>RecommendationScore_FineTuned_GPT-4o_median: 0.83,<br>RecommendationScore_FineTuned_GPT-3.5-turbo_median: 0.83,<br>RecommendationScore_ZeroShot: approx. random (low),<br>RecommendationScore_FewShot_n=100: exceeds statistical method but below fine-tuned models |
| **Application Domains** | Materials Science,<br>Metal−Organic Frameworks (MOFs),<br>Chemistry (synthetic chemistry / synthesis planning),<br>Crystallography (structure-property linking),<br>Scientific text mining / Natural Language Processing for scientific literature |

---


### [230. Automating the practice of science: Opportunities, challenges, and implications](https://doi.org/10.1073/pnas.2401238121), Proceedings of the National Academy of Sciences *(February 04, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Open Quantum Materials Database (OQMD),<br>Materials Project (materials genome),<br>Materials databases (general; stable materials databases referenced),<br>DANDI,<br>OpenNeuro,<br>DABI,<br>BossDB,<br>BIDS (Brain Imaging Data Structure) - community standard,<br>Open Science Framework,<br>Large gene databases (general reference),<br>Amazon Mechanical Turk (subject pool/platform),<br>Prolific (prolific.ac) (subject pool/platform),<br>JDRF-CGM trial data (Juvenile Diabetes Research Foundation continuous glucose monitoring trial),<br>Elicit training data (LLM trained on paper abstracts) - referenced tool |
| **Models** | Transformer,<br>GPT,<br>Attention Mechanism,<br>Autoencoder,<br>Variational Autoencoder,<br>Gaussian Process,<br>Multi-Layer Perceptron,<br>Autoencoder (again - reduced-order modeling) ,<br>Neural Architecture Search |
| **Tasks** | Regression,<br>Dimensionality Reduction,<br>Language Modeling,<br>Text Summarization,<br>Text Generation,<br>Information Retrieval,<br>Optimization |
| **Learning Methods** | Active Learning,<br>Reinforcement Learning,<br>Transfer Learning,<br>Fine-Tuning,<br>Pre-training,<br>Representation Learning,<br>Self-Supervised Learning,<br>Neural Architecture Search |
| **Performance Highlights** | qualitative: BrainGPT "demonstrated the capability to outperform human experts in predicting the results of neuroscience experiments",<br>cost_per_article_usd: 15 |
| **Application Domains** | Materials science,<br>Chemistry,<br>Functional genomics / biology,<br>Drug discovery,<br>Behavioral sciences / psychology / cognitive science,<br>Neuroscience,<br>Physics (including plasma physics, fluid dynamics),<br>Engineering (automation, robotics),<br>Clinical health (e.g., diabetes monitoring) |

---


### [229. Exploration of Chemical Space Through Automated Reasoning](https://doi.org/10.1002/ange.202417657), Angewandte Chemie *(February 03, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Liverpool ionic conductivity dataset,<br>Compositions generated by Comgen (this work) |
| **Models** | Diffusion Model |
| **Tasks** | Clustering,<br>Classification,<br>Data Generation,<br>Synthetic Data Generation |
| **Learning Methods** | Unsupervised Learning,<br>Supervised Learning,<br>Generative Learning |
| **Performance Highlights** | predicted_high_conductivity_count_>1e-4_S_cm^-1: 9 candidates,<br>candidates_with_energy_from_convex_hull_<=45_meV_atom^-1: 8 candidates,<br>selected_high_conductivity_reference_subset_count_>=1e-3_S_cm^-1: 55 compounds (from Liverpool dataset) used as reference set for Mg-analogue search |
| **Application Domains** | Materials discovery,<br>Materials chemistry,<br>Solid-state battery electrolytes (Li-ion, Mg-ion conductors),<br>Crystal structure prediction / computational materials design,<br>Automated scientific reasoning / computational design workflows |

---


### [228. From text to insight: large language models for chemical data extraction](https://doi.org/10.1039/D4CS00913D), Chemical Society Reviews *(February 03, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | EuroPMC,<br>arXiv,<br>ChemRxiv,<br>S2ORC (Semantic Scholar Open Research Corpus),<br>Elsevier OA CC-BY Corpus,<br>Open Reaction Database (ORD),<br>USPTO,<br>SciBERT pretraining corpus,<br>MatSciBERT pretraining corpus,<br>Llama 3 public data (mentioned) |
| **Models** | Transformer,<br>GPT,<br>BERT,<br>Vision Transformer,<br>Attention Mechanism,<br>Self-Attention Network,<br>Multi-Head Attention,<br>Recurrent Neural Network |
| **Tasks** | Named Entity Recognition,<br>Relation Extraction,<br>Sequence Labeling,<br>Information Retrieval,<br>Text Classification,<br>Image Classification,<br>Question Answering,<br>Clustering,<br>Image-to-Image Translation |
| **Learning Methods** | Zero-Shot Learning,<br>Few-Shot Learning,<br>One-Shot Learning,<br>In-Context Learning,<br>Prompt Learning,<br>Fine-Tuning,<br>Pre-training,<br>Supervised Learning,<br>Self-Supervised Learning,<br>Reinforcement Learning,<br>Transfer Learning,<br>Multi-Task Learning,<br>Few-Shot Learning |
| **Performance Highlights** | annotation_time_reduction: >50% (time per sample reduced by more than half for last annotated abstracts, as reported) |
| **Application Domains** | Chemistry,<br>Materials science,<br>Organic synthesis,<br>Inorganic materials and metal–organic frameworks (MOFs),<br>Nanoparticles / nanomaterials,<br>Polymers,<br>Catalysis,<br>Battery and energy materials,<br>Spectroscopy / NMR data extraction,<br>Scientific publishing / literature mining |

---


### [227. Balancing autonomy and expertise in autonomous synthesis laboratories](https://doi.org/10.1038/s43588-025-00769-x), Nature Computational Science *(February 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | generic computational datasets,<br>low-cost and fast proxy measurement datasets,<br>generic experimental datasets,<br>specialized, standardized, and carefully evaluated datasets,<br>simulated characterization data |
| **Models** | _None_ |
| **Tasks** | Information Retrieval,<br>Feature Extraction,<br>Experimental Design,<br>Data Augmentation,<br>Anomaly Detection,<br>Regression,<br>Binary Classification |
| **Learning Methods** | Pre-training,<br>Fine-Tuning,<br>Transfer Learning,<br>Active Learning,<br>Supervised Learning,<br>Domain Adaptation,<br>Representation Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | autonomous synthesis laboratories,<br>materials synthesis,<br>chemical synthesis,<br>materials characterization,<br>laboratory automation and robotics,<br>experimental chemistry |

---


### [226. Knowledge-guided large language model for material science](https://doi.org/10.1016/j.revmat.2025.100007), Review of Materials Research *(February 01, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Common Crawl,<br>The Pile,<br>StarCoder,<br>Hugging Face Datasets,<br>LIMA dataset,<br>arxiv-physics-instruct (arxiv_instructed_Physics),<br>peS2o,<br>Gutenberg Project,<br>PRM800K,<br>Materials Project,<br>OQMD (Open Quantum Materials Database),<br>OMat24 (Open Materials 2024),<br>OCx24 (Open Catalyst Experiments 2024),<br>MoLFormer training data |
| **Models** | Transformer,<br>Recurrent Neural Network,<br>BERT,<br>GPT,<br>Attention Mechanism,<br>Graph Neural Network,<br>Diffusion Model,<br>Sequence-to-Sequence |
| **Tasks** | Information Retrieval,<br>Named Entity Recognition,<br>Structured Prediction,<br>Regression,<br>Graph Generation,<br>Language Modeling,<br>Sequence-to-Sequence,<br>Experimental Design,<br>Planning,<br>Data Generation,<br>Anomaly Detection |
| **Learning Methods** | Fine-Tuning,<br>Supervised Learning,<br>Reinforcement Learning,<br>Pre-training,<br>In-Context Learning,<br>Prompt Learning,<br>Active Learning,<br>Self-Supervised Learning,<br>Transfer Learning |
| **Performance Highlights** | search_accuracy: 96.9%,<br>property_prediction_accuracy: 95.7%,<br>novelty_stability:  >2x (more than twice as novel and stable),<br>local_energy_proximity: 15x (15 times closer to the local energy minimum),<br>benchmark_outperformance: outperforms graph-based and supervised models across ten molecular property prediction benchmarks,<br>training_efficiency: requires 60x fewer GPUs for training,<br>relative_metrics: outperform BERT-based methods in precision, recall, and F1 (no numeric values given),<br>precision: ≈90%,<br>recall: ≈90%,<br>experiments_run: 355,<br>success_rate_targets: 71% (41 of 58 targets),<br>throughput: over two new materials per day,<br>synthesis_success_examples: synthesized DEET, three thiourea organo-catalysts, and a novel chromophore (qualitative successes),<br>validity_uniqueness_novelty: reported as high validity, uniqueness, and novelty (no numeric values provided) |
| **Application Domains** | Materials science (materials informatics, inorganic materials, metal-organic frameworks, polymers, catalysts),<br>Chemistry / computational chemistry (molecular generation, reaction planning),<br>Autonomous laboratories and robotics (experimental synthesis automation),<br>Scientific text mining and structured information extraction,<br>Drug discovery / molecular property prediction (mentioned as analogous domain) |

---


### [225. Battery lifetime prediction across diverse ageing conditions with inter-cell deep learning](https://doi.org/10.1038/s42256-024-00972-x), Nature Machine Intelligence *(February 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | MATR (MATR-1, MATR-2),<br>HUST,<br>CLO,<br>CALCE,<br>HNEI,<br>UL-PUR,<br>RWTH,<br>SNL,<br>MIX (MIX-100 and MIX-20),<br>LFP subset (rich-resource chemistry),<br>LCO subset (target chemistry in transfer experiments),<br>NCA subset (target chemistry in transfer experiments),<br>NMC subset (target chemistry in transfer experiments) |
| **Models** | Convolutional Neural Network,<br>Multi-Layer Perceptron,<br>Long Short-Term Memory,<br>Random Forest,<br>Support Vector Machine,<br>Linear Model |
| **Tasks** | Regression,<br>Time Series Forecasting,<br>Transfer Learning |
| **Learning Methods** | Supervised Learning,<br>Multi-Task Learning,<br>Pre-training,<br>Fine-Tuning,<br>Representation Learning,<br>Transfer Learning |
| **Performance Highlights** | r.m.s.e._reduction_vs_best_baseline_%_MATR-1: 36.5,<br>r.m.s.e._reduction_vs_best_baseline_%_MATR-2: 6.8,<br>r.m.s.e._reduction_vs_best_baseline_%_HUST: 20.1,<br>r.m.s.e._reduction_vs_best_baseline_%_MIX-100: 27.4,<br>r.m.s.e._reduction_vs_best_baseline_%_MIX-20: 40.1,<br>MAPE_reduction_vs_single-cell_CNN_%_average: up to 40 |
| **Application Domains** | Lithium-ion battery lifetime prediction / battery degradation modeling,<br>Battery state-of-charge and state-of-health estimation (potential application mentioned),<br>Cross-chemistry transfer for battery materials (LFP → LCO/NCA/NMC) and low-resource chemistries,<br>Potential extension to fast-charging protocols and emerging chemistries (solid-state, sodium-ion) |

---


### [224. A guidance to intelligent metamaterials and metamaterials intelligence](https://doi.org/10.1038/s41467-025-56122-3), Nature Communications *(January 29, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | _None_ |
| **Models** | Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>Recurrent Neural Network,<br>Variational Autoencoder,<br>Generative Adversarial Network,<br>Diffusion Model,<br>Graph Neural Network,<br>Autoencoder,<br>Encoder-Decoder,<br>Bayesian Network |
| **Tasks** | Regression,<br>Image-to-Image Translation,<br>Image Classification,<br>Image Generation,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Semi-Supervised Learning,<br>Transfer Learning,<br>Reinforcement Learning,<br>Backpropagation,<br>Pre-training,<br>Fine-Tuning,<br>Ensemble Learning |
| **Performance Highlights** | accuracy: 87%,<br>relative_error_reduction: 23%,<br>steps_to_solution: 9000,<br>training_time: 8 hours for a five-layer diffractive ONN with 0.2M neurons,<br>compute_reduction: two to three orders of magnitude reduction in required computation (example),<br>spectral_similarity: 99.8% (in KK-driven causal neural network example),<br>average_element_error: 1e-4,<br>solved_equations: 8 complex equations demonstrated |
| **Application Domains** | Metamaterials / Metasurfaces design,<br>Photonics / Nanophotonics,<br>Optics (imaging, holography, lenses),<br>Wireless communication (RIS, intelligent reflection surfaces),<br>Invisibility cloaks and stealth,<br>Sensing and detection (spectral recovery),<br>Computational imaging and image classification,<br>Analogue / wave-based computing and optical neural networks,<br>Acoustics, water waves, and heat flow (cross-physical applications),<br>Quantum mechanics / many-body physics (AI applied to physical discovery),<br>Autonomous systems / augmented reality (low-latency, high-throughput processing) |

---


### [223. InterPLM: Discovering Interpretable Features in Protein Language Models via Sparse Autoencoders](https://doi.org/10.1101/2024.11.14.623630), Preprint *(January 28, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | UniRef50 (random sample),<br>Swiss-Prot (reviewed subset, sampled),<br>AlphaFold Database (structures) |
| **Models** | Transformer,<br>Autoencoder,<br>Transformer |
| **Tasks** | Representation Learning,<br>Feature Extraction,<br>Feature Selection,<br>Clustering,<br>Dimensionality Reduction,<br>Binary Classification,<br>Regression,<br>Language Modeling,<br>Text Generation,<br>Clustering,<br>Hyperparameter Optimization |
| **Learning Methods** | Self-Supervised Learning,<br>Unsupervised Learning,<br>Transfer Learning,<br>Representation Learning,<br>Feature Learning,<br>Pre-training,<br>In-Context Learning,<br>Prompt Learning,<br>Hyperparameter Optimization |
| **Performance Highlights** | features_with_strong_concept_alignment_per_layer: up to 2309,<br>features_identified_by_SAE_vs_neurons_increase: SAEs extract 3x the concepts found in ESM-2-8M neurons and 7x the concepts found in ESM-2-650M neurons,<br>number_of_distinct_SwissProt_concepts_detected_by_SAEs: 143 (expanded from 15 by neurons),<br>SwissProt_concepts_evaluated: 433 concepts evaluated,<br>example_feature_F1_high: f/1503 F1 = 0.998,<br>example_feature_F1_medium: f/??? F1 = 0.793, 0.611 (other beta-barrel features),<br>glycine_specific_features_F1: 0.995, 0.990, 0.86,<br>validation_selection_threshold: feature-concept pairs with F1 > 0.5 counted per layer,<br>steering_effect_examples: steering increases P(Glycine) at steered and nearby periodic positions; periodic glycine features propagated effect for multiple repeats with diminishing intensity,<br>example_correlation_r_values_on_steering_fig7: r = .19, r = .16, r = .24, r = .004 (shown in figure panels for steering experiments),<br>median_Pearson_r_on_validation_of_LLM_predictions: 0.72 (median Pearson r correlation across diverse proteins / 1200 features),<br>example_feature_Pearson_r_values: examples: 9390 r=0.98, 10091 r=0.83, 9047 r=0.80, 4616 r=0.76, 4360 r=0.75,<br>example_L0_values: ESM-2-8M L1 L0=128, L2=163, L3=100, L4=106, L5=134, L6=178; ESM-2-650M L1 L0=50, L9=211, L18=190, L24=121, L30=182, L33=148,<br>example_percent_loss_recovered: values per-layer: e.g., ESM-2-8M L1 %LossRecovered=99.73, L2=99.72, L3=99.40, L4=98.94, L5=99.55, L6=100.00; 650M layer values include 99.83, 99.49, 94.28, 92.42, 96.24, 100.00 |
| **Application Domains** | Protein biology,<br>Structural biology,<br>Protein engineering / design,<br>Computational biology / bioinformatics,<br>Model interpretability / mechanistic interpretability |

---


### [222. Probing out-of-distribution generalization in machine learning for materials](https://doi.org/10.1038/s43246-024-00731-w), Communications Materials *(January 11, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project (MP),<br>JARVIS,<br>OQMD |
| **Models** | Random Forest,<br>XGBoost,<br>Graph Neural Network,<br>Multi-Layer Perceptron,<br>Transformer |
| **Tasks** | Regression,<br>Dimensionality Reduction,<br>Feature Extraction,<br>Anomaly Detection |
| **Learning Methods** | Supervised Learning,<br>Representation Learning,<br>Ensemble Learning,<br>Pre-training,<br>Fine-Tuning,<br>Out-of-Distribution Learning |
| **Performance Highlights** | MAE_MP_(eV/atom): 0.033,<br>R2_MP: 0.996,<br>MAE_JARVIS_(eV/atom): 0.036,<br>R2_JARVIS: 0.995,<br>MAE_OQMD_(eV/atom): 0.020,<br>R2_OQMD: 0.998,<br>leave-one-element-out_R2_>0.95_fraction_on_MP: 85%,<br>structure-based_leave-one-space-group_out_R2_>0.95_fraction: 100% (ALIGNN achieves R2 > 0.95 in all tasks; 88% of tasks have R2 > 0.98),<br>MAE_MP_(eV/atom): 0.078,<br>R2_MP: 0.979,<br>MAE_JARVIS_(eV/atom): 0.074,<br>R2_JARVIS: 0.981,<br>MAE_OQMD_(eV/atom): 0.070,<br>R2_OQMD: 0.987,<br>leave-one-element-out_R2_>0.95_fraction_on_MP: 68%,<br>MAE_MP_(eV/atom): 0.090,<br>R2_MP: 0.970,<br>MAE_JARVIS_(eV/atom): 0.099,<br>R2_JARVIS: 0.968,<br>MAE_OQMD_(eV/atom): 0.065,<br>R2_OQMD: 0.985,<br>MAE_MP_(eV/atom): 0.052,<br>R2_MP: 0.992,<br>MAE_JARVIS_(eV/atom): 0.081,<br>R2_JARVIS: 0.985,<br>MAE_OQMD_(eV/atom): 0.038,<br>R2_OQMD: 0.995,<br>MAE_MP_(eV/atom): 0.063,<br>R2_MP: 0.981,<br>MAE_JARVIS_(eV/atom): 0.068,<br>R2_JARVIS: 0.982,<br>MAE_OQMD_(eV/atom): 0.045,<br>R2_OQMD: 0.995,<br>representationally_ID_tasks_behavior: neural scaling laws hold (ID and representationally ID OOD errors decrease with more data/compute),<br>representationally_OOD_tasks_behavior: scaling marginally beneficial or adverse; e.g., OOD MAE increases with more training beyond ~20 epochs for some tasks,<br>example_5x_increase: 5-fold increase in OOD MAE for leave-H-out when training set size increased from 10^4 to 10^6 (OQMD) |
| **Application Domains** | Materials science,<br>Computational materials discovery,<br>Computational chemistry / materials property prediction |

---


### [221. Integrating artificial intelligence with mechanistic epidemiological modeling: a scoping review of opportunities and challenges](https://doi.org/10.1038/s41467-024-55461-x), Nature Communications *(January 10, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | COVID-19 datasets,<br>Influenza datasets,<br>Dengue datasets,<br>HIV datasets,<br>Synthetic datasets generated by epidemiological models,<br>Social media content (self-reported symptom tweets, mobility info),<br>Google Search Trends,<br>Satellite imagery / remote sensing data,<br>Electronic health records / emergency department reports,<br>Mobility data / origin-destination matrices |
| **Models** | Long Short-Term Memory,<br>Recurrent Neural Network,<br>Graph Neural Network,<br>Convolutional Neural Network,<br>Multi-Layer Perceptron,<br>Decision Tree,<br>Support Vector Machine,<br>Generative Adversarial Network,<br>Graph Convolutional Network,<br>Attention Mechanism,<br>Random Forest,<br>Gradient Boosting Tree,<br>Feedforward Neural Network,<br>Bayesian Network |
| **Tasks** | Infectious disease forecasting,<br>Model parameterization and calibration,<br>Disease intervention assessment and optimization,<br>Retrospective epidemic course analysis,<br>Transmission inference,<br>Outbreak detection |
| **Learning Methods** | Supervised Learning,<br>End-to-End Learning,<br>Ensemble Learning,<br>Reinforcement Learning,<br>Variational Inference,<br>Pre-training,<br>Simulation-based inference |
| **Performance Highlights** | _None_ |
| **Application Domains** | Infectious disease epidemiology,<br>Public health planning and response,<br>Epidemic forecasting and surveillance,<br>Intervention design and optimization (vaccination, NPIs),<br>Transmission network analysis / contact tracing,<br>Vector-borne disease risk mapping (climate/environmental drivers),<br>Agent-based simulation and policy evaluation |

---


### [220. Transforming the synthesis of carbon nanotubes with machine learning models and automation](https://doi.org/10.1016/j.matt.2024.11.007), Matter *(January 08, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Standardized CVD experimental database (CARCO),<br>Virtual experiments (digital twin outputs),<br>Carbon-materials literature corpus (for model fine-tuning),<br>Ion-implantation experimental sample pool,<br>Characterization dataset (SEM, Raman, XPS, HRTEM, AFM) |
| **Models** | Transformer,<br>GPT,<br>BERT,<br>Random Forest,<br>Gradient Boosting Tree,<br>XGBoost,<br>Decision Tree,<br>Generalized Linear Model |
| **Tasks** | Recommendation,<br>Optimization,<br>Regression,<br>Binary Classification,<br>Synthetic Data Generation,<br>Hyperparameter Optimization |
| **Learning Methods** | Fine-Tuning,<br>Pre-training,<br>Self-Supervised Learning,<br>Supervised Learning,<br>Transfer Learning,<br>Ensemble Learning,<br>Representation Learning,<br>Hyperparameter Optimization |
| **Performance Highlights** | R2: 0.67,<br>R2: 0.65,<br>R2: 0.64,<br>Spearman_correlation_on_test_questions: increase from 0.1 to 0.3,<br>Density_control_precision_workflow: 56.25% (27/49),<br>Regression_only_precision: 39.74% (31/79),<br>Human_filtering_precision: 49.15% (29/60),<br>Human_after_classification_precision: 61.36% (27/45) |
| **Application Domains** | Carbon-based nanomaterials (CBNs),<br>Horizontally aligned carbon nanotube (HACNT) array synthesis,<br>Materials science and nanomaterials synthesis,<br>Catalyst discovery and screening,<br>Automated/robotic chemical vapor deposition (CVD) systems,<br>Electronics and optoelectronics (applications of HACNT arrays),<br>Biomedical sensors (application context) |

---


### [219. Synthesis Strategies for High Entropy Nanoparticles](https://doi.org/10.1002/adma.202412337), Advanced Materials *(January 08, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | _None_ |
| **Models** | Non-negative Matrix Factorization |
| **Tasks** | _None_ |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | Materials Science,<br>Nanoparticle synthesis,<br>Catalysis,<br>Electrocatalysis,<br>Photocatalysis,<br>Energy storage (Batteries, Supercapacitors),<br>Thermoelectrics,<br>Photovoltaics,<br>Biomedical coatings/implants,<br>Aerospace and coatings,<br>Nuclear reactors |

---


### [218. Development and validation of a real-time prediction model for acute kidney injury in hospitalized patients](https://doi.org/10.1038/s41467-024-55629-5), Nature Communications *(January 02, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Combined multicenter EHR cohort (derivation + internal + external),<br>Derivation cohort (Site 1),<br>Internal validation cohort (Site 1, 2020),<br>External validation cohorts (Sites 2-5) |
| **Models** | Random Forest,<br>LightGBM |
| **Tasks** | Binary Classification,<br>Classification,<br>Survival Analysis,<br>Time Series Forecasting,<br>Feature Selection,<br>Hyperparameter Optimization |
| **Learning Methods** | Supervised Learning,<br>Imbalanced Learning,<br>Ensemble Learning,<br>Hyperparameter Optimization |
| **Performance Highlights** | Derivation AUC (AKI in 24 h, test set): 0.92 (95% CI, 0.90–0.93),<br>Derivation AUC (AKI in 48 h, test set): 0.91 (95% CI, 0.90–0.92),<br>Derivation AUC (AKI in 72 h, test set): 0.91 (95% CI, 0.90–0.91),<br>Derivation AUC (Severe AKI in 24 h, test set): 0.95 (95% CI, 0.94–0.97),<br>Derivation AUC (Severe AKI in 48 h, test set): 0.95 (95% CI, 0.94–0.96),<br>Derivation AUC (Severe AKI in 72 h, test set): 0.94 (95% CI, 0.94–0.95),<br>Transported model AUC range (AKI within 48 h, external sites): 0.74–0.85 (per-site: Table 2 shows Site1 0.85; Site2 0.78; Site3 0.74; Site4 0.81; Site5 0.78),<br>Transported model AUC range (Severe AKI within 48 h, external sites): 0.83–0.90 (per-site: Table 2 shows Site1 0.90; Site2 0.85; Site3 0.83; Site4 0.89; Site5 0.86),<br>Re-fitted model AUC range (AKI within 48 h, validation cohorts): 0.81–0.90 (per-site: Table 2: Site1 0.90; Site2 0.86; Site3 0.81; Site4 0.89; Site5 0.90),<br>Re-fitted model AUC range (Severe AKI within 48 h, validation cohorts): 0.88–0.95 (per-site: Table 2: Site1 0.95; Site2 0.92; Site3 0.88; Site4 0.93; Site5 0.91),<br>Derivation AUC (Severe AKI in 48 h, test set): 0.95 (95% CI, 0.94–0.96),<br>Re-fitted model AUC (Severe AKI in 48 h) Site1 (internal): 0.95 (95% CI, 0.94–0.95),<br>Re-fitted model AUC (Severe AKI in 48 h) Site2: 0.92; Site3: 0.88; Site4: 0.93; Site5: 0.91,<br>Probability cutoff: 0.45,<br>Median lead-time to AKI (Site1): 72 hours (IQR 24–198),<br>Sensitivity (Site1, cutoff 0.45): 89.4%,<br>Specificity (Site1, cutoff 0.45): 89.3%,<br>Negative Predictive Value (NPV, Site1, cutoff 0.45): 99.6%,<br>Positive Predictive Value (PPV, Site1, cutoff 0.45): 24.1%,<br>Probability cutoff: 0.4,<br>Median lead-time to severe AKI (Site1): 114 hours,<br>Sensitivity (Site1, cutoff 0.4): 90.9%,<br>Specificity (Site1, cutoff 0.4): 93.9%,<br>NPV (Site1, cutoff 0.4): 99.8%,<br>PPV (Site1, cutoff 0.4): 19.5%,<br>AKI within 48 h AUC (before re-re-fitting, Site3): 0.81 (0.80–0.81),<br>AKI within 48 h AUC (after re-re-fitting, Site3): 0.89 (0.89–0.89),<br>Severe AKI within 48 h AUC (before re-re-fitting, Site3): 0.88 (0.87–0.88),<br>Severe AKI within 48 h AUC (after re-re-fitting, Site3): 0.90 (0.89–0.90),<br>Specificity (AKI detection, cutoff 0.45) improved from: 66.4% to 82.6%,<br>Sensitivity (AKI detection) improved from: 78.7% to 80.8%,<br>PPV improved from: 19.1% to 31.9%,<br>NPV improved from: 96.9% to 97.7% |
| **Application Domains** | Healthcare,<br>Nephrology (acute kidney injury prediction),<br>Hospital operational care / in-hospital clinical decision support,<br>Electronic Health Records (EHR)-based predictive analytics,<br>Clinical risk stratification and preventive medicine |

---


### [217. Machine learning for the physics of climate](https://doi.org/10.1038/s42254-024-00776-3), Nature Reviews Physics *(January 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | ERA5 reanalysis,<br>Landsat-1 / satellite altimeter observations (Jason, ERS, TOPEX/Poseidon),<br>SWOT (Surface Water and Ocean Topography) mission data,<br>CryoSat-2 and SMOS merged product (CS2SMOS),<br>Argo floats (including BGC-Argo),<br>nextsim (neXtSIM) sea-ice thickness product / Arctic sea-ice forecasting,<br>NATL60 (SWOT Data Challenge NATL60),<br>IMERG precipitation product,<br>High-resolution numerical simulations and process-resolving simulation libraries (e.g., large-eddy simulations library) |
| **Models** | Convolutional Neural Network,<br>U-Net,<br>Transformer,<br>ResNet,<br>Graph Neural Network,<br>Recurrent Neural Network,<br>Generative Adversarial Network,<br>Diffusion Model,<br>Normalizing Flow,<br>Gaussian Process |
| **Tasks** | Time Series Forecasting,<br>Image Super-Resolution,<br>Regression,<br>Distribution Estimation,<br>Hyperparameter Optimization,<br>Anomaly Detection,<br>Causal Inference |
| **Learning Methods** | Supervised Learning,<br>Online Learning,<br>Reinforcement Learning,<br>Transfer Learning,<br>Variational Inference,<br>Generative Learning,<br>Self-Supervised Learning,<br>Pre-training,<br>Ensemble Learning |
| **Performance Highlights** | lead_time_months_for_ENSO_skill: up to 17 months (Ham et al. 2019),<br>extended_lead_time_with_loss_and_params: up to 24 months (Patil et al. 2023),<br>lead_time_months_reservoir_methods: reservoir computing methods reached ~21 months for ENSO (Hassanibesheli et al. 2022),<br>lead_time_months_for_ENSO_skill: up to 18 months reported for adaptive graph CNNs (ref. 138),<br>improvement_over_traditional_methods: outperforming traditional algorithms such as kriging (general statement; specific numeric metrics not provided),<br>capability: can generate stochastic high-resolution samples and represent uncertainty (qualitative),<br>examples: GAN-based stochastic super-resolution for precipitation/clouds (refs. 31,33),<br>use_case: diffusion-based ensemble forecasting (Gencast) proposed for medium-range weather,<br>objective: used to objectively tune parameters of parameterization schemes (qualitative benefit),<br>bias_reduction_example: NeuralGCM shows promise in reducing some biases of traditional GCMs (qualitative),<br>stability_issues: offline-trained CNNs can be unstable when coupled; mixed offline–online retraining produced stable QBO in testbed (ref. 81),<br>comparison_with_IFS: global transformer S2S model outperforms IFS in key variables including total precipitation and tropical cyclones (qualitative; no numeric score provided) |
| **Application Domains** | Climate physics,<br>Weather forecasting (nowcasting, medium-range),<br>Sub-seasonal to seasonal forecasting (S2S),<br>Interannual forecasting (ENSO prediction),<br>Decadal forecasting,<br>Oceanography (sea surface height, eddies, ocean turbulence),<br>Cryosphere (sea-ice thickness),<br>Remote sensing / satellite data reconstruction,<br>Parameterization of sub-grid-scale processes in climate models,<br>Data assimilation and reanalysis construction,<br>Model emulation and hybrid physics–ML models |

---


### [215. AI4Materials: Transforming the landscape of materials science and enigneering](https://doi.org/10.1016/j.revmat.2025.100010), Review of Materials Research *(January 01, 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project,<br>Open Quantum Materials Database (OQMD),<br>NOMAD,<br>Materials Data Curation System (MDCS),<br>MGEDATA,<br>Materials Cloud,<br>MATCLOUD / MatCloud,<br>ALKEMIE-Matter Cloud,<br>RXN SMILES / Reaction corpora used by IBM RXN,<br>Corpus for SteelBERT,<br>Hypothetical polymer dataset (polyBERT training),<br>NYU abstracts corpus (skip-gram),<br>DeepMind / GNoME training set (first-principles calculations),<br>Autonomous experiment logs / execution datasets |
| **Models** | Random Forest,<br>Gradient Boosting Tree,<br>Support Vector Machine,<br>Multi-Layer Perceptron,<br>Graph Neural Network,<br>Convolutional Neural Network,<br>Transformer,<br>BERT,<br>GPT,<br>Variational Autoencoder,<br>Generative Adversarial Network,<br>Diffusion Model,<br>Recurrent Neural Network,<br>Bayesian methods (as Active Learning),<br>Evolutionary Learning |
| **Tasks** | Regression,<br>Classification,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Selection,<br>Feature Extraction,<br>Optimization,<br>Experimental Design,<br>Sequence-to-Sequence,<br>Text Classification,<br>Text Generation,<br>Named Entity Recognition,<br>Image Classification,<br>Ranking,<br>Data Generation |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Active Learning,<br>Reinforcement Learning,<br>Transfer Learning,<br>Pre-training,<br>Fine-Tuning,<br>Multi-Task Learning,<br>Self-Supervised Learning,<br>Evolutionary Learning,<br>Federated Learning |
| **Performance Highlights** | prediction_accuracy: 92%,<br>experiments_per_day: approximately 100,<br>parameter_identification_runs: 237 experiments for sensitivity stratification; >600 closed-loop experiments for growth-rate studies,<br>screened_compositions: 19,841 compositions screened,<br>selected_candidates: 151 FPV perovskites identified (downstream regression applied),<br>experimental_workload_reduction: substantial (qualitative),<br>accuracy_improvement: reported improved predictive accuracy over naive approaches,<br>screening_time_reduction: reduced materials screening cycle by approximately ten years (qualitative),<br>scale_of_simulation: molecular dynamics to 100 million atoms,<br>speedup_equivalent: reduced a task that would normally take 60 years to one day (qualitative/inferred by scale),<br>award: 2020 Gordon Bell Prize (team using deep potential approaches),<br>capabilities: predict forward and reverse reactions and yields; rapid and accurate atomic mappings,<br>deployment: used in RXN platform for synthesis planning,<br>R2_on_mechanical_properties: around 80% (yield strength, tensile strength, elongation),<br>fine-tuning_samples: as few as 64 experimental samples for austenitic stainless steels,<br>predicted_steel_properties: yield strength 960 MPa, tensile strength 1138 MPa, elongation 32.5%,<br>A-Lab_success_rate: 41 novel compounds synthesized out of 58 targets (~70.7%) over 17 days,<br>ChatMOF_metrics: text-based searching 96.9%; property predicting 95.7%; structure generating 87.5% (with GPT-4),<br>generated_candidates_reported: e.g., 200 potential high-Tc superconductors proposed; generation of eutectic compositionally complex alloys across quaternary to senary systems (quantities reported in cited works),<br>application_examples: successful inverse design of SMAs, metamaterials, superconductors,<br>applications: GAN used for copper-based metallic glasses, bulk metallic glass inverse design,<br>qualitative_outcomes: enabled generation of novel amorphous solids and metallic glass compositions,<br>improved_prediction: enhanced predictive modeling of creep fracture life (qualitative improvement reported),<br>defect_detection_case: LLM-enabled microstructure optimization combined with ML for defects classification (paper references improved outcomes) |
| **Application Domains** | Materials Science and Engineering,<br>Alloy Design and Metallurgy,<br>Additive Manufacturing / 3D Printing,<br>Catalysis and Photocatalysis,<br>Batteries and Energy Storage (Li-S, solid-state conductors),<br>Superconductors discovery,<br>Metamaterials and Mechanical Metamaterials,<br>Nanotechnology and Quantum Dots,<br>Aerospace materials,<br>Biomedical materials,<br>High-throughput experimental automation |

---


### [214. Computational microscopy with coherent diffractive imaging and ptychography](https://doi.org/10.1038/s41586-024-08278-z), Nature *(January 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | Rotavirus single-particle dataset (~500 particles),<br>Apoferritin cryo-electron ptychography dataset,<br>Mimivirus XFEL single-particle dataset,<br>Twisted bilayer MoS2 electron ptychography dataset,<br>PrScO3 multislice ptychography dataset,<br>Zeolite Socony Mobil-5 catalyst dataset (electron multislice ptychography),<br>La3Ni2O7−δ multislice ptychography + EELS dataset,<br>Lithium- and manganese-rich (LMR) layered-oxide in situ BCDI dataset,<br>Nanoparticle superlattice X-ray ptychography + tomography dataset,<br>Integrated circuit ptychographic X-ray tomography dataset (7-nm commercial IC),<br>Fourier ptychography tissue dataset (digital pathology),<br>Frozen–hydrated mouse brain tissue X-ray ptychographic tomography dataset |
| **Models** | Convolutional Neural Network,<br>Autoencoder,<br>Generative Adversarial Network,<br>Multi-Layer Perceptron |
| **Tasks** | Image-to-Image Translation,<br>Image Generation,<br>Clustering,<br>Experimental Design,<br>3D Reconstruction (mapped to Image-to-Image Translation) |
| **Learning Methods** | Representation Learning,<br>Reinforcement Learning,<br>Maximum Likelihood Estimation,<br>Adversarial Training,<br>Supervised Learning |
| **Performance Highlights** | real-time: claimed (contextual) |
| **Application Domains** | Materials science (atomic-resolution imaging, strain and defect mapping),<br>Quantum materials and magnetism (spin textures, skyrmions, topological defects),<br>Battery and energy materials (electrodes, in situ BCDI during cycling),<br>Nanomaterials (nanoparticle lattices, superlattices),<br>Integrated circuits and nanoelectronics (non-destructive 3D metrology),<br>Structural biology (proteins, viruses, cells, cryo-electron ptychography),<br>Biomedical imaging / Digital pathology (Fourier ptychography, tissue imaging),<br>Ultrafast dynamics (pump–probe HCDI, XFEL single-shot experiments),<br>Computational microscopy / inverse problems (phase retrieval and reconstruction algorithms) |

---


### [213. Probabilistic weather forecasting with machine learning](https://doi.org/10.1038/s41586-024-08252-9), Nature *(January 2025)*

| Category | Items |
|----------|-------|
| **Datasets** | ERA5 reanalysis (analysis),<br>ENS (ECMWF ensemble forecast, TIGGE archive),<br>HRES-fc0 (ECMWF deterministic initial conditions / HRES dataset),<br>Global Power Plant Database (GPPD),<br>IBTrACS (International Best Track Archive for Climate Stewardship),<br>TempestExtremes tracked cyclone outputs (from ERA5, HRES-fc0, GenCast, ENS),<br>Derived pooled verification datasets (pooled CRPS; neighbourhood verification) |
| **Models** | Denoising Diffusion Probabilistic Model,<br>Graph Neural Network,<br>Transformer,<br>Encoder-Decoder,<br>Multi-Layer Perceptron,<br>Message Passing Neural Network,<br>Gaussian Process |
| **Tasks** | Time Series Forecasting,<br>Distribution Estimation,<br>Regression,<br>Sequence-to-Sequence,<br>Synthetic Data Generation,<br>Distribution Estimation |
| **Learning Methods** | Generative Learning,<br>Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Backpropagation,<br>Representation Learning |
| **Performance Highlights** | CRPS_targets_better_than_ENS_pct: 97.2%,<br>CRPS_targets_better_than_ENS_pct_lead_gt_36h: 99.6%,<br>ensemble_mean_RMSE_better_or_equal_pct: 96%,<br>ensemble_mean_RMSE_significantly_better_pct: 78%,<br>runtime_single_15-day_forecast: 8 minutes (on Cloud TPUv5),<br>ensemble_size: 50 members,<br>significance: P < 0.05 for reported significant comparisons,<br>pooled_average_CRPS_better_pct: 98.1% of 5,400 pooled targets,<br>pooled_max_CRPS_better_pct: 97.6% of 5,400 pooled targets,<br>spectral_matching: GenCast samples spectra closely match ERA5 at 1- and 15-day lead times (qualitative; fig.2 and text),<br>CRPS_targets_better_or_competitive_pct_vs_ENS: 82% of scorecard targets,<br>outperformed_by_GenCast_pct: GenCast outperforms GenCast-Perturbed in 99% of targets,<br>sample_sharpness: GenCast-Perturbed ensemble members are blurrier; ensemble-mean-like samples (qualitative),<br>relative_CRPS_improvement_vs_ENS_up_to_2d: ~20% better,<br>relative_CRPS_improvement_vs_ENS_2-4d: 10–20% better,<br>statistically_significant_improvement_out_to: 7 days (P < 0.05),<br>ensemble_mean_track_position_advantage: approx. 12-hour advantage in accuracy between 1 and 4 days ahead (GenCast ensembles mean more accurate than ENS),<br>REV_track_probability: GenCast track probability forecasts outperform ENS (better REV at all cost/loss ratios except when neither model beats climatology); significant improvements out to 7 day lead times (P < 0.05) |
| **Application Domains** | Operational weather forecasting / medium-range global weather prediction,<br>Tropical cyclone track forecasting and hazard prediction,<br>Renewable energy forecasting (regional wind power aggregation and decision support),<br>Meteorological verification and probabilistic forecast evaluation,<br>Climate and atmospheric reanalysis-informed ML model training |

---


### [211. Data extraction from polymer literature using large language models](https://doi.org/10.1038/s43246-024-00708-9), Communications Materials *(December 19, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | corpus of ~2.4 million materials science journal articles,<br>subset of 681,000 polymer-related articles,<br>filtered paragraphs after heuristic and NER filters,<br>manually curated labeled subset for evaluation (630 abstracts),<br>evaluation subset of 1000 polymer-related articles,<br>extracted polymer-property dataset (Polymer Scholar) |
| **Models** | BERT,<br>GPT,<br>Transformer,<br>Long Short-Term Memory,<br>Recurrent Neural Network |
| **Tasks** | Named Entity Recognition,<br>Information Retrieval,<br>Text Generation,<br>Clustering,<br>Feature Extraction |
| **Learning Methods** | Few-Shot Learning,<br>Pre-training,<br>Fine-Tuning,<br>Self-Supervised Learning,<br>Supervised Learning,<br>In-Context Learning,<br>Prompt Learning |
| **Performance Highlights** | F1_Tg: 0.67,<br>F1_bandgap_random: 0.87,<br>F1_bandgap_similar: 0.85,<br>extracted_records_full_corpus: 672,449,<br>extracted_pairs_subset_6179_paragraphs_random_shot: 4706,<br>extracted_pairs_subset_6179_paragraphs_similar_shot: 4589,<br>full_text_Tg_records: 125,585,<br>full_text_bandgap_records: 63,361,<br>api_cost_for_6179_paragraphs: $4.48 (for ~2.9 million tokens, reported in Results),<br>full_corpus_inference_cost: ≈$1,200 for 716,000 paragraphs (Methods),<br>F1_Tg: 0.63,<br>F1_bandgap: 0.66,<br>extracted_records_full_corpus: 390,813,<br>full_text_Tg_records: 75,722,<br>full_text_bandgap_records: 30,732,<br>processing_time_6179_paragraphs: < 30 minutes,<br>monetary_cost: $0 (operated in-house),<br>F1_Tg: 0.64,<br>F1_bandgap: 0.77,<br>extracted_pairs_subset_6179_paragraphs: 3441,<br>inference_time: longest among evaluated (hosted locally on 4x Quadro GP100 GPUs),<br>monetary_cost: $0 (hosted locally but high compute/time cost) |
| **Application Domains** | polymer science,<br>materials science,<br>materials informatics / polymer informatics,<br>natural language processing (applied to scientific literature),<br>scientific data curation / dataset creation for ML |

---


### [210. De novo design of polymer electrolytes using GPT-based and diffusion-based generative models](https://doi.org/10.1038/s41524-024-01470-9), npj Computational Materials *(December 19, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | HTP-MD dataset,<br>PI1M dataset,<br>Generated candidates (this work),<br>Top candidates validated with MD (this work) |
| **Models** | GPT,<br>Transformer,<br>Diffusion Model,<br>Denoising Diffusion Probabilistic Model,<br>BERT,<br>Graph Neural Network,<br>Random Forest,<br>U-Net |
| **Tasks** | Language Modeling,<br>Text Generation,<br>Synthetic Data Generation,<br>Regression,<br>Binary Classification,<br>Data Augmentation |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Transfer Learning,<br>Supervised Learning,<br>Imbalanced Learning,<br>Hyperparameter Optimization,<br>Multi-Objective Learning |
| **Performance Highlights** | unconditional_mean_metric: 0.773,<br>validity: >0.8 (reported for optimal minGPT in unconditional generation),<br>uniqueness: >0.8 (reported),<br>novelty: >0.8 (reported),<br>synthesizability: >0.8 (reported),<br>training_time: 3-4 minutes on Tesla V100 (optimal minGPT model),<br>pretraining_effects: shortened fine-tuning convergence; improved validity & uniqueness especially with limited fine-tuning data,<br>unconditional_mean_metric: 0.736,<br>validity: lower than minGPT and diffusion-LM (qualitative),<br>training_time: ~2 hours on Tesla V100 (optimal 1Ddiffusion model),<br>unconditional_mean_metric: 0.767,<br>validity: >0.8 (reported for optimal diffusion-LM in unconditional generation),<br>training_time: ~2 hours on Tesla V100 (optimal diffusion-LM model),<br>generated_set_size: 100000,<br>selected_for_MD: 50,<br>MD_successful_simulations_obtained: 46,<br>simulations_completed: 206 out of the 250 simulations across the 46 polymers (reported),<br>candidates_surpassing_best_train_conductivity: 17,<br>best_generated_conductivity_S/cm: 0.00113,<br>best_train_conductivity_S/cm: 0.000507,<br>relative_performance: GNN out-performed Random Forest (no numeric values provided),<br>usage: used to screen 100K generated candidates and rank top 50 for MD |
| **Application Domains** | polymer electrolytes for lithium-ion / solid-state batteries,<br>polymer informatics,<br>materials discovery (computational materials),<br>generative molecular design,<br>high-throughput computational screening |

---


### [209. Poseidon: Efficient Foundation Models for PDEs](https://doi.org/), Advances in Neural Information Processing Systems *(December 16, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Pretraining collection: NS-Sines,<br>Pretraining collection: NS-Gauss,<br>Pretraining collection: CE-RP (4-quadrant Riemann problem),<br>Pretraining collection: CE-CRP (curved Riemann partitions),<br>Pretraining collection: CE-KH (Kelvin-Helmholtz),<br>Pretraining collection: CE-Gauss,<br>POSEIDON pretraining aggregate (authors' description),<br>Downstream: NS-PwC (Piecewise-constant vorticity),<br>Downstream: NS-BB (Brownian Bridge initializations),<br>Downstream: NS-SL (Shear Layer),<br>Downstream: NS-SVS (Sinusoidal Vortex Sheet),<br>Downstream: NS-Tracer-PwC (passive tracer transport),<br>Downstream: FNS-KF (Forced Navier-Stokes / Kolmogorov Flow),<br>Downstream: CE-RPUI (Riemann problem with uncertain interfaces),<br>Downstream: CE-RM (Richtmyer–Meshkov),<br>Downstream: GCE-RT (Gravitational compressible Euler, Rayleigh–Taylor),<br>Downstream: Wave-Gauss (wave equation in Gaussian medium),<br>Downstream: Wave-Layer (wave equation in layered medium),<br>Downstream: ACE (Allen–Cahn equation),<br>Downstream: SE-AF (steady Euler flow past airfoil),<br>Downstream: Poisson-Gauss,<br>Downstream: Helmholtz |
| **Models** | Vision Transformer,<br>Transformer,<br>Attention Mechanism,<br>Multi-Head Attention,<br>U-Net,<br>Convolutional Neural Network,<br>Encoder-Decoder,<br>Sequence-to-Sequence |
| **Tasks** | Time Series Forecasting,<br>Sequence-to-Sequence,<br>Image-to-Image Translation,<br>Regression,<br>Time Series Forecasting |
| **Learning Methods** | Pre-training,<br>Fine-Tuning,<br>Transfer Learning,<br>Supervised Learning,<br>Stochastic Gradient Descent,<br>Representation Learning,<br>Batch Learning,<br>Mini-Batch Learning |
| **Performance Highlights** | median_samples_to_match_FNO_1024: 20 (median over downstream tasks, POSEIDON-L as reported),<br>overall_best_performance_count: 14/15 tasks (POSEIDON family best on 14 of 15 tasks; POSEIDON-L top performer overall),<br>pretraining_examples_after_all2all: approx 5.11M training examples (pretraining dataset after all2all pairing),<br>aggregate_median_EG: 49.8 (median Efficiency Gain for POSEIDON-L over tasks),<br>aggregate_mean_AG: 9.58 (mean Accuracy Gain for POSEIDON-L over tasks),<br>EG: 890.6,<br>AG: 24.7,<br>context_metric_baseline: FNO baseline EG/AG normalized to 1 (used as reference),<br>EG: 502.9,<br>AG: 7.3,<br>EG: 552.5,<br>AG: 29.3,<br>EG: 21.9,<br>AG: 5.5,<br>EG: 49.8,<br>AG: 8.7,<br>EG: 62.5,<br>AG: 7.4,<br>EG: 352.2,<br>AG: 6.5,<br>EG: 4.6,<br>AG: 1.2,<br>EG: 3.4,<br>AG: 1.2,<br>EG: 5.3,<br>AG: 2.0,<br>EG: 46.5,<br>AG: 6.1,<br>EG: 62.1,<br>AG: 5.6,<br>EG: 17.0,<br>AG: 11.6,<br>EG: 42.5,<br>AG: 20.5,<br>EG: 78.3,<br>AG: 6.1,<br>reference_model: FNO (baseline), normalized EG=1, AG=1 by construction (used as reference in EG/AG metrics) |
| **Application Domains** | Computational fluid dynamics (incompressible Navier–Stokes),<br>Compressible gas dynamics (Euler equations),<br>Wave propagation / acoustics / seismic (wave equation),<br>Reaction–diffusion systems (Allen–Cahn / material science),<br>Aerofoils / aerodynamic steady-state flow (shape optimization),<br>Astrophysical fluid instabilities (Rayleigh–Taylor),<br>Elliptic problems (Poisson, Helmholtz),<br>Scientific machine learning / operator learning for PDEs |

---


### [206. Invariant Tokenization of Crystalline Materials for Language Model Enabled Generation](https://doi.org/), Advances in Neural Information Processing Systems *(December 16, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Perov-5,<br>Carbon-24,<br>MP-20,<br>MPTS-52,<br>Combined training set (Materials Project + OQMD + NOMAD),<br>JARVIS-DFT |
| **Models** | GPT,<br>Variational Autoencoder,<br>Diffusion Model,<br>Normalizing Flow,<br>Transformer |
| **Tasks** | Language Modeling,<br>Text Generation,<br>Data Generation,<br>Synthetic Data Generation,<br>Regression |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Supervised Learning,<br>Prompt Learning |
| **Performance Highlights** | Perov-5_one_shot_Match: 50.0%,<br>Perov-5_one_shot_RMSE: 0.099,<br>Carbon-24_one_shot_Match: 23.7%,<br>Carbon-24_one_shot_RMSE: 0.169,<br>MP20_one_shot_Match: 61.3%,<br>MP20_one_shot_RMSE: 0.040,<br>MPTS-52_one_shot_Match: 23.1%,<br>MPTS-52_one_shot_RMSE: 0.109,<br>Perov-5_20_shot_Match: 98.5%,<br>Perov-5_20_shot_RMSE: 0.023,<br>Carbon-24_20_shot_Match: 86.0%,<br>Carbon-24_20_shot_RMSE: 0.148,<br>MP20_20_shot_Match: 75.3%,<br>MP20_20_shot_RMSE: 0.037,<br>MPTS-52_20_shot_Match: 36.4%,<br>MPTS-52_20_shot_RMSE: 0.088,<br>Bandgap_condition_low_success_rate: 83.6%,<br>Bandgap_condition_high_success_rate: 90.7%,<br>Validity_overall_towards_low: 88.0%,<br>Uniqueness_towards_low: 98.0%,<br>Novelty_towards_low: 86.2%,<br>Validity_overall_towards_high: 89.8%,<br>Uniqueness_towards_high: 92.2%,<br>Novelty_towards_high: 98.6%,<br>Perov-5_one_shot_Match: 45.3%,<br>Perov-5_one_shot_RMSE: 0.114,<br>Carbon-24_one_shot_Match: 17.1%,<br>Carbon-24_one_shot_RMSE: 0.297,<br>MP20_one_shot_Match: 33.9%,<br>MP20_one_shot_RMSE: 0.105,<br>MPTS-52_one_shot_Match: 5.34%,<br>MPTS-52_one_shot_RMSE: 0.211,<br>Perov-5_20_shot_Match: 88.5%,<br>Perov-5_20_shot_RMSE: 0.046,<br>Carbon-24_20_shot_Match: 88.4%,<br>Carbon-24_20_shot_RMSE: 0.229,<br>MP20_20_shot_Match: 67.0%,<br>MP20_20_shot_RMSE: 0.103,<br>MPTS-52_20_shot_Match: 20.8%,<br>MPTS-52_20_shot_RMSE: 0.209,<br>Perov-5_one_shot_Match: 52.0%,<br>Perov-5_one_shot_RMSE: 0.076,<br>Carbon-24_one_shot_Match: 17.5%,<br>Carbon-24_one_shot_RMSE: 0.276,<br>MP20_one_shot_Match: 51.5%,<br>MP20_one_shot_RMSE: 0.063,<br>MPTS-52_one_shot_Match: 12.2%,<br>MPTS-52_one_shot_RMSE: 0.179,<br>Perov-5_20_shot_Match: 98.6%,<br>Perov-5_20_shot_RMSE: 0.013,<br>Carbon-24_20_shot_Match: 88.5%,<br>Carbon-24_20_shot_RMSE: 0.219,<br>MP20_20_shot_Match: 77.9%,<br>MP20_20_shot_RMSE: 0.049,<br>MPTS-52_20_shot_Match: 34.0%,<br>MPTS-52_20_shot_RMSE: 0.175,<br>CrystaLLM_one_shot_Perov-5_Match: 46.1%,<br>CrystaLLM_one_shot_Perov-5_RMSE: 0.095,<br>CrystaLLM_one_shot_Carbon-24_Match: 20.3%,<br>CrystaLLM_one_shot_Carbon-24_RMSE: 0.176,<br>CrystaLLM_one_shot_MP20_Match: 58.7%,<br>CrystaLLM_one_shot_MP20_RMSE: 0.041,<br>CrystaLLM_one_shot_MPTS-52_Match: 19.2%,<br>CrystaLLM_one_shot_MPTS-52_RMSE: 0.111,<br>CrystaLLM_20_shot_Perov-5_Match: 97.6%,<br>CrystaLLM_20_shot_Perov-5_RMSE: 0.025,<br>CrystaLLM_20_shot_Carbon-24_Match: 85.2%,<br>CrystaLLM_20_shot_Carbon-24_RMSE: 0.151,<br>CrystaLLM_20_shot_MP20_Match: 74.0%,<br>CrystaLLM_20_shot_MP20_RMSE: 0.035,<br>CrystaLLM_20_shot_MPTS-52_Match: 33.8%,<br>CrystaLLM_20_shot_MPTS-52_RMSE: 0.106,<br>FlowMM_validity_%: 83.2%,<br>FlowMM_stability_rate_DFT_%: 96.9%,<br>FlowMM_S.U.N._Rate_%: 2.5%,<br>ComFormer_MAE_bandgap: 0.122 eV,<br>MP-20_test_set_Mat2Seq_large_params: 200M,<br>MP-20_test_set_Mat2Seq_large_RMSE: 0.037,<br>Mat2Seq-small_params: 25M,<br>Mat2Seq-small_RMSE: 0.039,<br>CDVAE_params: 4.5M,<br>CDVAE_RMSE: 0.103,<br>DiffCSP_RMSE: 0.049,<br>generation_speed_Mat2Seq-small_sec_per_crystal: 2.1 s,<br>generation_speed_Mat2Seq-large_sec_per_crystal: 5.7 s,<br>generation_speed_CDVAE_sec_per_crystal: 37.9 s,<br>generation_speed_DiffCSP_sec_per_crystal: 7.3 s,<br>MP-20_exp_observed_MatchRate: 65.2%,<br>MP-20_exp_observed_RMSE: 0.042,<br>MP-20_whole_test_set_MatchRate: 61.3%,<br>MP-20_whole_test_set_RMSE: 0.040 |
| **Application Domains** | Materials discovery / computational materials science,<br>Crystal structure prediction and generation,<br>Band gap materials design and property-driven materials discovery,<br>High-throughput screening of inorganic crystalline materials |

---


### [205. Crystal structure generation with autoregressive large language modeling](https://doi.org/10.1038/s41467-024-54639-7), Nature Communications *(December 06, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | CrystaLLM training set (2.3M unique cell composition-space group pairs, 2,047,889 training CIF files),<br>Challenge set (70 structures: 58 from recent literature unseen in training, 12 from training),<br>Held-out test set (subset of the curated dataset),<br>Perov-5 benchmark,<br>Carbon-24 benchmark,<br>MP-20 benchmark,<br>MPTS-52 benchmark |
| **Models** | Transformer,<br>Graph Neural Network,<br>Variational Autoencoder,<br>Denoising Diffusion Probabilistic Model,<br>U-Net,<br>Transformer |
| **Tasks** | Synthetic Data Generation,<br>Data Generation,<br>Regression |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Supervised Learning,<br>Fine-Tuning,<br>Reinforcement Learning |
| **Performance Highlights** | held-out_test_validity_no_space_group_%: 93.8,<br>held-out_test_validity_with_space_group_%: 94.0,<br>space_group_consistent_no_space_group_%: 98.8,<br>space_group_consistent_with_space_group_%: 99.1,<br>atom_site_multiplicity_consistent_%: 99.4,<br>bond_length_reasonableness_score_mean: 0.988,<br>bond_lengths_reasonable_%: 94.6,<br>average_valid_generated_length_tokens_no_sg: 331.9 ± 42.6,<br>average_valid_generated_length_tokens_with_sg: 339.0 ± 41.4,<br>match_with_test_structure_with_space_group_within_3_attempts_%: 88.1,<br>Perov-5_match_rate_n=20_%_CrystaLLM_a: 98.26,<br>Perov-5_RMSE_CrystaLLM_a_n=20: 0.0236,<br>Carbon-24_match_rate_n=20_%_CrystaLLM_a: 83.60,<br>Carbon-24_RMSE_CrystaLLM_a_n=20: 0.1523,<br>MP-20_match_rate_n=20_%_CrystaLLM_a: 75.14,<br>MP-20_RMSE_CrystaLLM_a_n=20: 0.0395,<br>MPTS-52_match_rate_n=20_%_CrystaLLM_a: 32.98,<br>MPTS-52_RMSE_CrystaLLM_a_n=20: 0.1197,<br>Perov-5_match_rate_n=20_%_CrystaLLM_b: 97.60,<br>Perov-5_RMSE_CrystaLLM_b_n=20: 0.0249,<br>Carbon-24_match_rate_n=20_%_CrystaLLM_b: 85.17,<br>Carbon-24_RMSE_CrystaLLM_b_n=20: 0.1514,<br>MP-20_match_rate_n=20_%_CrystaLLM_b: 73.97,<br>MP-20_RMSE_CrystaLLM_b_n=20: 0.0349,<br>MPTS-52_match_rate_n=20_%_CrystaLLM_b: 33.75,<br>MPTS-52_RMSE_CrystaLLM_b_n=20: 0.1059,<br>MPTS-52_match_rate_n=1_CrystaLLM_c: 28.30,<br>MPTS-52_RMSE_n=1_CrystaLLM_c: 0.0850,<br>MPTS-52_match_rate_n=20_CrystaLLM_c: 47.45,<br>MPTS-52_RMSE_n=20_CrystaLLM_c: 0.0780,<br>unconditional_generation_attempts: 1000,<br>valid_generated_CIFs: 900,<br>unique_structures: 891,<br>novel_structures_vs_training_set: 102,<br>mean_Ehull_of_102_novel_structures_eV_per_atom: 0.40,<br>novel_structures_with_Ehull_<=_0.1_eV_per_atom: 20,<br>novel_structures_with_Ehull_exact_0.00_eV_per_atom: 3,<br>ALIGNN_used_as_predictor: formation energy per atom (used as reward in MCTS),<br>average_ALIGNN_energy_change_after_MCTS_meV_per_atom: -153 ± 15 (prediction decrease across 102 unconditional-generated compositions),<br>MCTS_validity_rate_improvement_no_space_group_%: 95.0,<br>MCTS_validity_rate_improvement_with_space_group_%: 60.0,<br>MCTS_minimum_Ef_improvement_no_space_group_%: 85.0,<br>MCTS_minimum_Ef_improvement_with_space_group_%: 65.0,<br>MCTS_mean_Ef_improvement_no_space_group_%: 70.0,<br>MCTS_mean_Ef_improvement_with_space_group_%: 65.0,<br>mean_Ehull_change_after_ALIGNN-guided_MCTS_meV_per_atom: -56 ± 15 (mean Ehull improved to 0.34 eV/atom across 102 structures); 22 structures within 0.1 eV/atom of hull,<br>successful_generation_rate_small_model_no_sg_%: 85.7,<br>successful_generation_rate_small_model_with_sg_%: 88.6,<br>successful_generation_rate_large_model_no_sg_%: 87.1,<br>successful_generation_rate_large_model_with_sg_%: 91.4,<br>match_rate_seen_small_model_%: 50.0,<br>match_rate_seen_large_model_%: 83.3,<br>match_rate_unseen_small_model_no_sg_%: 25.9,<br>match_rate_unseen_small_model_with_sg_%: 34.5,<br>match_rate_unseen_large_model_no_sg_%: 37.9,<br>match_rate_unseen_large_model_with_sg_%: 41.4,<br>pyrochlore_cell_parameter_R2: 0.62,<br>pyrochlore_cell_parameter_MAE_A: 0.08 Å |
| **Application Domains** | Materials science,<br>Computational materials discovery,<br>Inorganic crystal structure prediction,<br>Materials informatics,<br>Computational chemistry / solid-state physics,<br>High-throughput screening and DFT-accelerated materials design |

---


### [204. Quantifying the use and potential benefits of artificial intelligence in scientific research](https://doi.org/10.1038/s41562-024-02020-5), Nature Human Behaviour *(December 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Microsoft Academic Graph (MAG),<br>USPTO / PatentsView,<br>Open Syllabus Project (OSP) syllabi,<br>Survey of Doctorate Recipients (SDR) |
| **Models** | Convolutional Neural Network,<br>Recurrent Neural Network,<br>Generative Adversarial Network,<br>Random Forest,<br>Support Vector Machine,<br>Decision Tree,<br>Bayesian Network |
| **Tasks** | Feature Extraction,<br>Feature Extraction,<br>Feature Selection,<br>Regression,<br>Object Detection,<br>Image Segmentation,<br>Pose Estimation,<br>Image Generation |
| **Learning Methods** | Reinforcement Learning,<br>Fine-Tuning |
| **Performance Highlights** | CS_direct_AI_use_2000: 0.5%,<br>CS_direct_AI_use_2019: 1.3%,<br>trend_slope_b: 0.00031,<br>trend_P: <0.001,<br>trend_95%_CI: (0.00025, 0.00037),<br>hit_rate_ratio_mean: 1.816,<br>hit_rate_ratio_s.e.: 0.138,<br>hit_rate_ratio_95%_CI: (1.547, 2.086),<br>outside_field_citation_ratio_mean: 1.069,<br>outside_field_citation_ratio_s.e.: 0.028,<br>outside_field_citation_ratio_95%_CI: (1.015, 1.124),<br>direct_vs_potential_percentile_correlation_r: 0.891,<br>direct_vs_potential_percentile_correlation_P: <0.001,<br>direct_vs_potential_percentile_95%_CI: (0.865, 0.913),<br>corr_directAI_collab_r: 0.841,<br>corr_directAI_collab_P: <0.001,<br>corr_directAI_collab_95%_CI: (0.616, 0.939),<br>corr_potentialAI_collab_r: 0.802,<br>corr_potentialAI_collab_P: <0.001,<br>corr_potentialAI_collab_95%_CI: (0.535, 0.923),<br>example_trend_engineering_share_1990: 0.21,<br>example_trend_engineering_share_2019: 0.44,<br>engineering_trend_slope_b: 0.0057,<br>engineering_trend_P: <0.001,<br>engineering_trend_95%_CI: (0.0047, 0.0068),<br>women_vs_directAI_r: -0.555,<br>women_vs_directAI_P: 0.032,<br>women_vs_directAI_95%_CI: (-0.831, -0.059),<br>women_vs_potentialAI_r: -0.593,<br>women_vs_potentialAI_P: 0.020,<br>URM_vs_directAI_r: -0.734,<br>URM_vs_directAI_P: 0.002,<br>URM_vs_directAI_95%_CI: (-0.906, -0.355),<br>URM_vs_potentialAI_r: -0.711,<br>URM_vs_potentialAI_P: 0.003,<br>example_black_vs_white_directAI_ratio_drop: Black score is 78% less than white for direct AI use (paper statement),<br>example_black_vs_white_potentialAI_ratio_drop: Black score is 86% less than white for potential AI benefits (paper statement),<br>hit_rate_ratio_mean: 1.816,<br>hit_rate_ratio_s.e.: 0.138,<br>hit_rate_ratio_95%_CI: (1.547, 2.086),<br>outside_field_citation_ratio_mean: 1.069,<br>outside_field_citation_ratio_s.e.: 0.028,<br>outside_field_citation_ratio_95%_CI: (1.015, 1.124) |
| **Application Domains** | Computer science,<br>Biology,<br>Physics,<br>Economics,<br>Engineering,<br>Medicine,<br>Materials science,<br>Mathematics,<br>Sociology,<br>Psychology,<br>Political science,<br>Geography,<br>Geology,<br>Chemistry,<br>Business,<br>Environmental science,<br>Philosophy,<br>History,<br>Art |

---


### [203. Multifunctional high-entropy materials](https://doi.org/10.1038/s41578-024-00720-y), Nature Reviews Materials *(December 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | HEM literature corpus (natural language analysis),<br>Abstracts corpus used in Pei et al.,<br>Candidate composition space (Pei et al.),<br>Closed-loop active-learning experimental set (Rao et al.),<br>High-throughput electronic calculations and experimental structure data (generic) |
| **Models** | Transformer,<br>Graph Neural Network,<br>Multi-Layer Perceptron,<br>Gaussian Process |
| **Tasks** | Information Retrieval,<br>Text Classification,<br>Optimization,<br>Experimental Design,<br>Regression,<br>Feature Extraction,<br>Ranking |
| **Learning Methods** | Active Learning,<br>Pre-training,<br>In-Context Learning,<br>Supervised Learning |
| **Performance Highlights** | abstracts_analyzed: 6.4 million,<br>promising_compositions_identified: nearly 500 out of 2.6 million candidate compositions,<br>new_alloys_processed: 17,<br>discovered_property: Invar HEMs with extremely low thermal expansion coefficients (~2×10^-6 K^-1 at 300 K) |
| **Application Domains** | Materials science / alloy design,<br>Magnetic materials and hard/soft magnets,<br>Thermoelectrics,<br>Electrocatalysis and heterogeneous catalysis,<br>Photovoltaics / optoelectronics,<br>Hydrogen storage and hydrides,<br>Radiation-resistant materials,<br>Shape-memory and multicaloric materials,<br>Biomedical implant materials,<br>High-throughput computational materials discovery |

---


### [202. Learning spatiotemporal dynamics with a pretrained generative model](https://doi.org/10.1038/s42256-024-00938-z), Nature Machine Intelligence *(December 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Kuramoto–Sivashinsky equation (KSE) simulation dataset,<br>Kolmogorov turbulent flow simulation dataset,<br>ERA5 reanalysis subset (u,v,Temp,P),<br>Cylinder flow PIV experimental dataset,<br>Additional PDE datasets (Burgers', Korteweg–de Vries, compressible Navier–Stokes) |
| **Models** | Diffusion Model,<br>Denoising Diffusion Probabilistic Model,<br>U-Net,<br>Convolutional Neural Network,<br>Graph Neural Network |
| **Tasks** | Image Super-Resolution,<br>Image-to-Image Translation,<br>Time Series Forecasting,<br>Sequence-to-Sequence,<br>Regression,<br>Distribution Estimation |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Zero-Shot Learning,<br>Unsupervised Learning |
| **Performance Highlights** | nRMSE: S3GM attains lower nRMSE than baselines (FNO, U-Net, LNO, DeepONets) across downsampling factors (1× to 64×) as shown in Fig.2c,<br>uncertainty: s.d. fields computed from five different predictions; uncertainty reported qualitatively (uncertainty fields shown),<br>nRMSE: S3GM shows low relative errors and uncertainty when reconstructing from spectral measurements; outperforms end-to-end baselines trained for spectral-to-physical mapping (Fig.2e,f),<br>nRMSE: S3GM produces more accurate and stable long-term predictions than baselines (FNO, U-Net); error accumulation lower over long horizons (see Fig.2i and Fig.3f),<br>spectral_consistency: Kinetic energy spectrum of S3GM matches reference better than baselines (Fig.3g),<br>nRMSE: nRMSE decreases as the number of observed points increases (Fig.4d) — S3GM yields acceptable nRMSE even with 1% observations plus 10% Gaussian noise,<br>Pearson_correlation: Correlation between predictions and ground truth increases toward ~1 as observed portion increases (Fig.4e),<br>nRMSE: S3GM statistically achieves lower nRMSE than PINN across Reynolds numbers (109.3,159.0,248.5) and as number of sensor positions varies (Fig.5d),<br>spectral_match: Fourier features and vortex shedding frequency recovered accurately (Fig.5h),<br>nRMSE: Baselines (including U-Net) often have higher nRMSE than S3GM across many experiments (Figs.2c,3c,3f) |
| **Application Domains** | fluid dynamics (turbulence, Kolmogorov flow, Navier–Stokes),<br>spatiotemporal chaotic systems (Kuramoto–Sivashinsky equation, KdV, Burgers'),<br>climate and atmospheric science (ERA5 reanalysis),<br>experimental fluid mechanics / laboratory PIV measurements,<br>general scientific and engineering full-field reconstruction from sparse sensors |

---


### [201. Towards the holistic design of alloys with large language models](https://doi.org/10.1038/s41578-024-00726-6), Nature Reviews Materials *(December 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | corpus of six million texts (literature mining),<br>scientific corpora (publications, patents, conference abstracts and other corpora),<br>structured data sets generated from automated laboratories,<br>ImageNet (cited as an analogy / example),<br>Protein Data Bank (cited as an analogy / example) |
| **Models** | Transformer,<br>BERT,<br>GPT,<br>Convolutional Neural Network |
| **Tasks** | Named Entity Recognition,<br>Regression,<br>Recommendation,<br>Optimization,<br>Text Summarization,<br>Information Retrieval,<br>Ranking |
| **Learning Methods** | Fine-Tuning,<br>Pre-training,<br>Prompt Learning,<br>Domain Adaptation,<br>Unsupervised Learning,<br>Representation Learning,<br>Supervised Learning |
| **Performance Highlights** | gpu_hours: 1,700,000,<br>CO2_equivalent_tons: 291 |
| **Application Domains** | Alloy design,<br>Metallurgy / metallic materials,<br>Materials science,<br>Additive manufacturing,<br>Sustainability assessment for materials,<br>Materials mining and literature-based discovery |

---


### [199. An automatic end-to-end chemical synthesis development platform powered by large language models](https://doi.org/10.1038/s41467-024-54457-x), Nature Communications *(November 23, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Semantic Scholar academic literature database,<br>High-throughput screening (HTS) experimental dataset (this work),<br>Kinetics time-course datasets (this work),<br>Reaction optimization experimental dataset (this work),<br>Repository of code and processed data |
| **Models** | GPT,<br>Transformer,<br>Gaussian Process |
| **Tasks** | Information Retrieval,<br>Optimization,<br>Regression,<br>Time Series Forecasting,<br>Feature Extraction,<br>Information Retrieval,<br>Planning,<br>Control,<br>Optimization |
| **Learning Methods** | In-Context Learning,<br>Multi-Agent Learning,<br>Model-Based Learning,<br>Pre-training |
| **Performance Highlights** | agreement_with_manual_analysis: nearly consistent,<br>highest_identified_yield: 94.5%,<br>PI_stopping_met_after_experiments: 36,<br>photoredox_optimum_yield: 87%,<br>DMSO_R2: 0.996,<br>MeCN_R2: 0.994,<br>DMSO_k1: 22.34,<br>DMSO_k2: 2.84e-3,<br>DMSO_k3: 2.51e-4,<br>MeCN_k1: 16.29,<br>MeCN_k2: 6.0e-3,<br>MeCN_k3: 5.30e-4,<br>SNAr_model: r15 = 0.2 [13][14]^2,<br>R2: 0.995,<br>scale_up_isolated_yield: 86%,<br>scale_up_purity: >98%,<br>ResultInterpreter_stop_suggestion_experiment_number: 26 (heuristic),<br>PI_stop_experiment_number: 36 (statistical) |
| **Application Domains** | Chemical synthesis / synthetic chemistry,<br>Organic chemistry (method development),<br>Medicinal chemistry / drug discovery (reaction types relevant to drug discovery: SNAr, cross-coupling),<br>Catalyst development,<br>Process development and scale-up,<br>Photoelectrochemistry / heterogeneous photoelectrocatalysis,<br>Automation and laboratory robotics (automated HTS, OT-2, Unchained Big Kahuna) |

---


### [198. Sequence modeling and design from molecular to genome scale with Evo](https://doi.org/10.1126/science.ado9336), Science *(November 15, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | OpenGenome (GTDB + IMG/VR + IMG/PR compilation),<br>CRISPR-Cas fine-tuning dataset,<br>IS200/IS605 fine-tuning dataset,<br>Deep Mutational Scanning (DMS) datasets — prokaryotic protein DMS,<br>Deep Mutational Scanning (DMS) datasets — human proteins,<br>ncRNA DMS datasets,<br>Promoter / RBS expression datasets (supervised and evaluation),<br>Gene essentiality studies (DEG + phage screens),<br>Genome-scale generation / evaluation set |
| **Models** | Transformer,<br>Attention Mechanism,<br>Multi-Head Attention,<br>Convolutional Neural Network,<br>Linear Model,<br>Self-Attention Network |
| **Tasks** | Language Modeling,<br>Regression,<br>Binary Classification,<br>Synthetic Data Generation,<br>Feature Extraction,<br>Data Generation,<br>Clustering |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Zero-Shot Learning,<br>Supervised Learning,<br>Transfer Learning |
| **Performance Highlights** | evaluation_perplexity_range_for_baselines: eval PPL ~3.2 - 3.8 (reported across architectures in scaling laws analysis),<br>CNN_on_one-hot_mean_Spearman_r: 0.44,<br>CNN_on_Evo_embeddings_mean_Spearman_r: 0.56,<br>Ridge_on_Evo_embeddings_mean_Spearman_r: not explicitly numeric in text (reported as lower than CNN); zero-shot Evo likelihood mean Spearman r = 0.43 for promoters,<br>relative_performance: Evo’s zero-shot performance exceeded other nucleotide models and was competitive with leading protein-specific language models (per-figure; exact per-dataset Spearman values in supplemental tables),<br>example_Spearman_r_on_5S_rRNA: Spearman r = 0.60 (two-sided t-distributed P = 1.9 × 10^-3),<br>promoter_zero_shot_mean_Spearman_r: 0.43,<br>promoter_GC_content_mean_Spearman_r: 0.35,<br>GenSLM_zero_shot_mean_Spearman_r_on_promoters: 0.09,<br>protein_expression_promoter-RBS_zero_shot_Spearman_r: 0.61,<br>RBS_Calculator_performance_Spearman_r: 0.39,<br>selected_candidates_tested: 11 Evo-generated Cas9 systems selected for validation; 1 generation (EvoCas9-1) exhibited robust in vitro cleavage activity comparable to SpCas9,<br>EvoCas9-1_sequence_identity_to_closest_Cas_database: 79.9%,<br>EvoCas9-1_sequence_identity_to_SpCas9: 73.1%,<br>Evo_generated_sgRNA_identity_to_SpCas9_sgRNA: 91.1%,<br>AlphaFold3_cofold_mean_pLDDT: 90.01 (EvoCas9-1 cofold),<br>tested_designs: 24 IS200-like and 24 IS605-like designs experimentally tested,<br>successful_IS200_like: 11 out of 24 (≈45.8%) demonstrated excision and insertion in vitro,<br>successful_IS605_like: 3 out of 24 (12.5%) demonstrated excision and insertion in vitro,<br>sequence_identity_minimum_successful_examples: functional designs encoded proteins with sequence identity as low as 67% to fine-tuning database,<br>significant_association_with_essentiality: Evo log-likelihood changes with 66k context significantly associated (Bonferroni-corrected P < 0.05) with gene essentiality in 49 of 58 genomes tested,<br>AUROC_lambda_phage: 0.90 (P < 1×10^-5),<br>AUROC_Pseudomonas_aeruginosa: 0.84 (P < 1×10^-5),<br>generated_sequences_count_and_size: 16 sequences ~1 Mb each (total ~16 Mb),<br>tRNA_count_generated: 128 tRNA sequences encoding anticodons for all canonical amino acids across generated sequences,<br>coding_density_comparison: Generated sequences have coding densities nearly the same as natural genomes and substantially higher than random sequences |
| **Application Domains** | Genomics,<br>Molecular biology,<br>Protein engineering,<br>Synthetic biology / genome design,<br>Metagenomics and sequence mining,<br>Biotechnology (CRISPR and transposon tool design),<br>Functional genomics (gene essentiality prediction) |

---


### [197. Learning the language of DNA](https://doi.org/10.1126/science.adt3007), Science *(November 15, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | 2.7 million evolutionarily diverse prokaryotic and phage genomes (300 billion nucleotides),<br>Experimental validation set: 11 Evo-generated DNA sequences |
| **Models** | Transformer,<br>Attention Mechanism |
| **Tasks** | Language Modeling,<br>Text Generation,<br>Synthetic Data Generation,<br>Regression,<br>Classification,<br>Sequence-to-Sequence |
| **Learning Methods** | Pre-training,<br>Self-Supervised Learning,<br>Prompt Learning,<br>Generative Learning,<br>Representation Learning |
| **Performance Highlights** | perplexity: improved (no numeric value reported); improvement of Evo over StripedHyena alone reported; perplexity improves with increasing context size,<br>functional_validation: 1 new Cas9 (Evo-Cas9-1) functionally validated in vitro among sequences generated; 11 generated sequences were selected for experimental validation,<br>predictive_ability: qualitative statement: 'can predict critical features of DNA sequence, including the effect of mutations' (no numeric metrics reported) |
| **Application Domains** | Genomics (prokaryotic and phage genomes),<br>Synthetic biology,<br>CRISPR-Cas design and protein-RNA complex design,<br>DNA/RNA/protein function prediction,<br>Genome-scale sequence generation and design |

---


### [196. Deep learning generative model for crystal structure prediction](https://doi.org/10.1038/s41524-024-01443-y), npj Computational Materials *(November 12, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | MP60-CALYPSO,<br>MP60 (selected from Materials Project),<br>CALYPSO (selected),<br>MP20,<br>MP experimental test subset (ICSD labeled),<br>MP experimental subset with <20 atoms,<br>Randomly generated structures (CALYPSO) used for comparison |
| **Models** | Variational Autoencoder,<br>Diffusion Model,<br>Graph Neural Network,<br>Message Passing Neural Network,<br>Multi-Layer Perceptron,<br>Variational Autoencoder (CDVAE baseline) |
| **Tasks** | Synthetic Data Generation,<br>Data Generation,<br>Optimization,<br>Representation Learning |
| **Learning Methods** | Unsupervised Learning,<br>Representation Learning,<br>Batch Learning |
| **Performance Highlights** | reconstruction_match_rate_on_MP60-CALYPSO_test (%): 16.58,<br>reconstruction_N-RMSD_on_MP60-CALYPSO: 0.2066,<br>success_rate_on_3547_MP_experimental_structures_within_800_samplings (%): 59.3,<br>success_rate_on_2062_structures_with_<20_atoms_within_800_samplings (%): 83.2,<br>reconstruction_match_rate_on_MP60-CALYPSO_test (%): 13.01,<br>reconstruction_N-RMSD_on_MP60-CALYPSO: 0.2093,<br>success_rate_on_3547_MP_experimental_structures_within_800_samplings (%): 46.4 (plateau approx.),<br>success_rate_on_2062_structures_with_<20_atoms_within_800_samplings (%): 69.9,<br>DFT_local_optimization_convergence_rate_at_0GPa (%): 94.60,<br>DFT_local_optimization_convergence_rate_at_100GPa (%): 97.00,<br>average_ionic_steps_at_0GPa (#): 44.73,<br>average_RMSD_after_relaxation_at_0GPa (Å): 0.79,<br>reconstruction_match_rate_on_MP20_test (%): 45.43,<br>reconstruction_N-RMSD_on_MP20: 0.0356,<br>Li_cI16 (16 atoms, 50 GPa) Nmodel (avg samplings to ground state): 566.0,<br>Li_cI16 Runs_successful_Nmodel/runs: 2/5,<br>Li_cI16 CALYPSO_NCSP (avg samplings): 50.0 (3/3),<br>B_alpha-B12 (36 atoms, 0 GPa) Nmodel: 74.0 (1/5 successful runs),<br>B_gamma-B28 (28 atoms, 50 GPa) Nmodel: 341.0 (5/5 successful runs); CALYPSO: 0/3 success within 1000 samplings,<br>SiO2_alpha-quartz (9 atoms, 0 GPa) Nmodel: 62.8 (5/5) vs CALYPSO 189.0 (3/3),<br>SiO2_coesite (24 atoms, 5 GPa) Nmodel: 328.0 (3/5) vs CALYPSO failed within 1000 samplings |
| **Application Domains** | Computational materials science,<br>Crystal structure prediction (CSP),<br>High-pressure materials discovery,<br>Materials design and discovery (superconductors, superhard materials),<br>Atomistic structure generation and sampling |

---


### [195. Crystal Structure Determination from Powder Diffraction Patterns with Generative Machine Learning](https://doi.org/10.1021/jacs.4c10244), Journal of the American Chemical Society *(November 06, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | mp-20 (Materials Project subset),<br>RRUFF database,<br>American Mineralogist Crystal Structure Database,<br>Powder Diffraction File (PDF) database,<br>High-pressure experimental PXRD data (authors' lab / APS HPCAT beamline) |
| **Models** | Denoising Diffusion Probabilistic Model,<br>Variational Autoencoder,<br>Convolutional Neural Network,<br>Multi-Layer Perceptron,<br>Graph Neural Network |
| **Tasks** | Structured Prediction,<br>Regression,<br>Feature Extraction,<br>Synthetic Data Generation |
| **Learning Methods** | Generative Learning,<br>Denoising Diffusion Probabilistic Model,<br>Variational Inference,<br>Supervised Learning,<br>Data Augmentation,<br>Gradient Descent |
| **Performance Highlights** | simulated_match_rate_1_attempt: 30.4%,<br>simulated_match_rate_32_attempts: 61.8%,<br>simulated_match_rate_64_attempts: 66.6% (paper also reports up to 67% elsewhere),<br>experimental_RRUFF_match_rate_64_attempts_with_augmentation: 41.8%,<br>experimental_RRUFF_match_rate_64_attempts_without_augmentation: 17.9%,<br>experimental_RRUFF_match_rate_1_attempt_with_augmentation: 8.2%,<br>experimental_RRUFF_match_rate_1_attempt_without_augmentation: 2.2%,<br>average_best_RMSD_over_64_attempts: 0.0397,<br>implicit: VAE used as encoder/latent sampler in the overall model; performance metrics aggregated under Crystalyze diffusion results (see diffusion metrics),<br>post-StructSnap_lattice_tolerance_match_rate_1%: up to 22.3%,<br>post-StructSnap_angle_tolerance_0.5deg_match_rate: up to 26.7%,<br>see_overall_diffusion_performance: metrics reported under primary diffusion model (e.g., up to 66.6% simulated match rate; 41.8% experimental with augmentation) |
| **Application Domains** | Materials science,<br>Solid-state chemistry / crystallography,<br>Powder X-ray diffraction (PXRD) analysis,<br>High-pressure materials synthesis and discovery (diamond anvil cell experiments),<br>Automated / high-throughput materials discovery |

---


### [193. Physics-Informed Inverse Design of Programmable Metasurfaces](https://doi.org/10.1002/advs.202406878), Advanced Science *(November 06, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Simulated LC metasurface designs (On-state and Off-state reflections with retrieved mode parameters),<br>Experimental measurement data from fabricated sample (112 x 112 units) |
| **Models** | Multi-Layer Perceptron,<br>Feedforward Neural Network,<br>Autoencoder,<br>Variational Autoencoder,<br>Generative Adversarial Network,<br>ResNet |
| **Tasks** | Regression,<br>Optimization |
| **Learning Methods** | Supervised Learning,<br>Gradient Descent,<br>Backpropagation |
| **Performance Highlights** | training_dataset_size: 12000,<br>validation_dataset_size: 3000,<br>convergence_speed_vs_conventional_MLP: 2.5x faster,<br>hyperparameter_scale_vs_conventional_MLP: 10x larger hyperparameters,<br>MSE: statistically much smaller MSE distribution when physics layers are integrated (no absolute numeric provided),<br>inverse_design_candidates_generated: 500 candidates,<br>inverse_design_runtime: 10 s to generate 500 candidates,<br>simulated_phase_tuning: ≈300°,<br>simulated_reflection_amplitude_at_465GHz: >0.9 (over 90%),<br>experimental_measured_phase_at_intersection: 264.8° (measured),<br>experimental_measured_max_phase_change: 253.0° at 465.4 GHz (measured under 10.5° incidence),<br>experimental_reflection_amplitude_at_intersection: ≈0.7 (measured at 467 GHz),<br>beam_steering_max_deflection: 68° at 442 GHz (measured),<br>2-bit_deflection: ≈31.6° at 457 GHz with amplitude 0.26 (measured),<br>tri-state_deflection: 23° at 457 GHz (measured) |
| **Application Domains** | Terahertz programmable metasurfaces / metamaterials,<br>Beam steering for telecommunications (including 6G wireless communications),<br>Hyperspectral imaging,<br>Holographic displaying / holography,<br>Photonic device design / metasurface inverse design |

---


### [192. Reproducibility in automated chemistry laboratories using computer science abstractions](https://doi.org/10.1038/s44160-024-00649-8), Nature Synthesis *(November 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | _None_ |
| **Models** | Transformer,<br>GPT |
| **Tasks** | Machine Translation,<br>Optimization,<br>Experimental Design,<br>Decision Making,<br>Control,<br>Experimental Design |
| **Learning Methods** | Reinforcement Learning,<br>Active Learning,<br>Transfer Learning,<br>Fine-Tuning |
| **Performance Highlights** | yield_labA: 87%,<br>yield_labB: 47% |
| **Application Domains** | Chemistry,<br>Materials Science,<br>Automated Laboratory / Laboratory Robotics,<br>Molecular Discovery,<br>Photochemistry / Quantum Dots,<br>Catalysis,<br>Flow Chemistry and Batch Chemistry workflows,<br>Analytical Chemistry (HPLC, chromatography, NMR) |

---


### [191. Autonomous mobile robots for exploratory synthetic chemistry](https://doi.org/10.1038/s41586-024-08173-7), Nature *(November 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Parallel divergent synthesis screening and diversification reactions (thio)ureas,<br>Supramolecular host–guest discovery reaction library,<br>Photocatalyst screening for decarboxylative conjugate addition,<br>Autonomous analytical measurement database (UPLC–MS and 1H NMR spectra) |
| **Models** | _None_ |
| **Tasks** | Binary Classification,<br>Feature Extraction,<br>Novelty Detection |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | Synthetic organic chemistry,<br>Medicinal chemistry / Parallel diversification,<br>Supramolecular chemistry (metal–organic assemblies, host–guest binding),<br>Photochemical synthesis / Photocatalysis,<br>Autonomous laboratories / Mobile robotics integration |

---


### [189. Transforming science labs into automated factories of discovery](https://doi.org/10.1126/scirobotics.adm6991), Science Robotics *(October 23, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | historical and online data,<br>massive quantities of experimental data (generated by automated labs),<br>experimental runs / datasets produced by autonomous systems cited (e.g., AlphaFlow, mobile robotic chemist) |
| **Models** | Transformer,<br>Multi-Layer Perceptron |
| **Tasks** | Experimental Design,<br>Optimization,<br>Decision Making,<br>Policy Learning,<br>Control,<br>Planning,<br>Regression,<br>Experimental Design |
| **Learning Methods** | Reinforcement Learning,<br>Supervised Learning,<br>Prompt Learning,<br>Representation Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | chemistry,<br>biochemistry,<br>materials science,<br>energy,<br>catalysis,<br>biotechnology,<br>sustainability,<br>electronics,<br>drug design,<br>semiconductor materials,<br>batteries,<br>photocatalysis,<br>organic light-emitting devices (OLEDs) |

---


### [188. Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models](https://doi.org/10.48550/arXiv.2410.12771), Preprint *(October 16, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | OMat24 (Open Materials 2024),<br>MPtrj (Materials Project trajectories),<br>Alexandria,<br>sAlexandria (subset of Alexandria),<br>OC20 / OC22 (referenced),<br>WBM dataset (used by Matbench-Discovery),<br>Matbench-Discovery (benchmark) |
| **Models** | Graph Neural Network,<br>Transformer |
| **Tasks** | Regression,<br>Binary Classification,<br>Classification |
| **Learning Methods** | Pre-training,<br>Fine-Tuning,<br>Transfer Learning,<br>Supervised Learning,<br>Self-Supervised Learning |
| **Performance Highlights** | F1: 0.916,<br>MAE_energy: 0.020 eV/atom (20 meV/atom),<br>RMSE: 0.072 eV/atom (72 meV/atom),<br>Accuracy: 0.974,<br>Precision: 0.923,<br>Recall: 0.91,<br>R2: 0.848,<br>F1: 0.823,<br>MAE_energy: 0.035 eV/atom (35 meV/atom),<br>RMSE: 0.082 eV/atom (82 meV/atom),<br>Accuracy: 0.944,<br>Precision: 0.792,<br>Recall: 0.856,<br>R2: 0.802,<br>Energy_MAE_validation: 9.6 meV/atom,<br>Forces_MAE_validation: 43.1 meV/Å,<br>Stress_MAE_validation: 2.3 (units consistent with meV/Å^3),<br>Test_splits_energy_MAE_range: ≈9.7 - 14.6 meV/atom depending on test split (ID/OOD/WBM),<br>F1: 0.86,<br>MAE_energy: 0.029 eV/atom (29 meV/atom),<br>RMSE: 0.078 eV/atom (78 meV/atom),<br>Accuracy: 0.957,<br>Precision: 0.862,<br>Recall: 0.858,<br>R2: 0.823,<br>Validation_energy_MAE_on_MPtrj: 10.58 - 12.4 meV/atom depending on model variant; (Table 9: eqV2-L-DeNS energy 10.58 meV/atom; eqV2-S 12.4 meV/atom),<br>Validation_forces_MAE_on_MPtrj: ≈30 - 32 meV/Å |
| **Application Domains** | Materials discovery,<br>Inorganic materials / solid-state materials,<br>DFT surrogate modeling (predicting energies, forces, stress),<br>Computational screening for stable materials (thermodynamic stability / formation energy prediction),<br>Catalyst discovery and related atomistic simulations,<br>Molecular dynamics / non-equilibrium structure modeling (potential downstream application) |

---


### [187. MatGPT: A Vane of Materials Informatics from Past, Present, to Future](https://doi.org/10.1002/adma.202306733), Advanced Materials *(October 09, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Inorganic Crystal Structure Database (ICSD),<br>Materials Project,<br>AFLOW,<br>OQMD,<br>NOMAD,<br>C2DB,<br>JARVIS,<br>ESP,<br>OpenKIM,<br>OMDB,<br>ChEMBL,<br>ZINC,<br>GDB,<br>Dataset for HER NN model (first-principles adsorption configurations),<br>Dataset of 6,531 2D materials,<br>Adsorption energies at 198 active sites,<br>CO2 reduction / Cu–Al dataset (DFT + active learning example),<br>Liquid electrolyte dataset for Coulomb efficiency,<br>Garnet candidate dataset,<br>Mechanical property dataset from MP,<br>Perovskite synthesisability dataset,<br>MOF pretraining set,<br>Elpasolite compositions dataset,<br>ZeoGAN / zeolite dataset,<br>HEA dataset (high-entropy alloys),<br>HMFP / IonML high-fidelity Li+ conductor dataset,<br>Adsorption dataset for HMIs on g-C3N4 (transfer learning example) |
| **Models** | Decision Tree,<br>Support Vector Machine,<br>Random Forest,<br>XGBoost,<br>K-Means,<br>Agglomerative Hierarchical Clustering,<br>Spectral Clustering,<br>Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>Recurrent Neural Network,<br>Long Short-Term Memory,<br>Autoencoder,<br>Deep Belief Network,<br>Generative Adversarial Network,<br>Variational Autoencoder,<br>Transformer,<br>Deep Reinforcement Learning,<br>Gaussian Process,<br>Graph Neural Network,<br>Graph Convolutional Network,<br>Message Passing Neural Network,<br>Gradient Boosting Tree,<br>Deep Convolutional GAN |
| **Tasks** | Regression,<br>Classification,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Extraction,<br>Clustering,<br>Hyperparameter Optimization,<br>Graph Generation,<br>Sequence-to-Sequence,<br>Image Generation,<br>Optimization,<br>Regression,<br>Classification |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Semi-Supervised Learning,<br>Contrastive Learning,<br>Transfer Learning,<br>Active Learning,<br>Data Augmentation,<br>Ensemble Learning,<br>Pre-training,<br>Fine-Tuning,<br>Reinforcement Learning,<br>Self-Supervised Learning |
| **Performance Highlights** | accuracy: 95.2%,<br>out_of_sample_true_positive_rate: 0.957,<br>RMSE_initial_discharge_capacity: 16.66 mAh/g,<br>RMSE_50th_cycle_discharge_capacity: 18.59 mAh/g,<br>RMSE: < 0.1 eV,<br>selected_candidates: 38 out of 6531 2D materials screened,<br>prediction_performance: high prediction accuracy reported for PV parameters and ablation loss (quantitative numbers not specified),<br>state_of_the_art: claimed state-of-the-art results for a variety of properties (no single numeric metric reported),<br>generated_structures_count: e.g., ZeoGAN generated 121 crystalline porous materials; GAN for Elpasolites and Mg–Mn–O predicted multiple novel candidate structures,<br>success_rate_first_principles_verification: between 7.1% and 38.9% (for FTCP framework cases),<br>classification_accuracy_double_perovskites: 92%,<br>general_success: reported good predictive performance across many applications (specific metrics vary per study),<br>speedup: improved the efficiency of constructing macromolecular force fields by >10^4 times (order-of-magnitude claims); ITLFF improved force-field construction efficiency by >10^4 |
| **Application Domains** | materials science (general),<br>catalysis / heterogeneous catalysis,<br>battery materials / electrolytes / solid-state electrolytes / electrode materials,<br>photovoltaic materials / perovskites,<br>environmental materials (adsorbents, membranes for pollutant removal),<br>high-entropy alloys and alloy design,<br>2D materials,<br>metal–organic frameworks (MOFs),<br>organic macromolecular materials (proteins, enzymes),<br>crystal structure prediction,<br>autonomous experimentation / experimental robots / high-throughput synthesis |

---


### [186. Machine learning for data-centric epidemic forecasting](https://doi.org/10.1038/s42256-024-00895-7), Nature Machine Intelligence *(October 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | United States COVID-19 Forecast Hub dataset,<br>Google COVID-19 Community Mobility Reports,<br>Facebook-sampled online symptomatic surveys,<br>Wastewater SARS-CoV-2 RNA measurements,<br>Remote-sensing satellite imagery (hospital parking lot vacancies),<br>Pharmaceutical and retail/supermarket sales records,<br>CDC FluSight challenge datasets (seasonal influenza surveillance data),<br>Reinhart et al. open repository of real-time COVID-19 indicators,<br>Genomics / pathogen lineage datasets (phylodynamics) |
| **Models** | ARIMA Model,<br>Gaussian Process,<br>Recurrent Neural Network,<br>Transformer,<br>Long Short-Term Memory,<br>Variational Autoencoder,<br>Convolutional Neural Network,<br>Graph Neural Network,<br>Latent Dirichlet Allocation |
| **Tasks** | Time Series Forecasting,<br>Regression,<br>Density Estimation,<br>Distribution Estimation,<br>Anomaly Detection,<br>Decision Making,<br>Resource Allocation,<br>Clustering |
| **Learning Methods** | Supervised Learning,<br>Transfer Learning,<br>Meta-Learning,<br>Multi-Task Learning,<br>Ensemble Learning,<br>Variational Inference,<br>Representation Learning |
| **Performance Highlights** | evaluation_metrics_mentioned: log score; interval score; weighted interval score; mean absolute error,<br>qualitative: Ensembles consistently outperform individual models in multiple CDC forecasting competitions (influenza, Ebola, COVID-19); adaptive and equally weighted ensembles cited as effective |
| **Application Domains** | Epidemic forecasting / epidemiology,<br>Public health decision-making,<br>Pandemic preparedness,<br>Healthcare resource allocation (e.g., ventilators, hospital capacity),<br>Policy evaluation (lockdowns, travel restrictions, vaccination strategies),<br>Supply chain planning for medical supplies |

---


### [185. Generative deep learning for the inverse design of materials](https://doi.org/10.48550/arXiv.2409.19124), Preprint *(September 27, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Open Quantum Materials Database (OQMD),<br>Computational 2D Materials Database (C2DB) / Computational 2D Materials Database,<br>Inorganic Crystal Structure Database (ICSD),<br>NFFA-EUROPE SEM Dataset,<br>ASM Micrograph Database,<br>UHCSDB (UHCSDB / UHCSDB steel SEM dataset),<br>DoITPoMS micrograph collection,<br>Synthetic microstructure images by GRF (Gaussian Random Field),<br>Ferrite-martensite SEM dataset,<br>HMX SEM image (single large image),<br>U-10Mo SEM-BSE dataset,<br>Dual-phase steel synthetic micrographs,<br>AZ80 magnesium alloy components dataset,<br>SAOED composites micro-CT images,<br>Large DDPM microstructure training set (Düreth et al.),<br>2D database of structural features and 3D porous material database (Lyu et al.),<br>SOFC anode PFIB-SEM dataset (Hsu et al.),<br>Li-ion battery cathode XCT and SOFC anode XCT (Gayon-Lombardo et al.),<br>Crystal datasets used in various crystal generative works (e.g., V-O binary, Heusler, chalcogenides) |
| **Models** | Variational Autoencoder,<br>Generative Adversarial Network,<br>Denoising Diffusion Probabilistic Model,<br>Convolutional Neural Network,<br>Graph Neural Network,<br>Crystal Graph Convolutional Neural Networks,<br>U-Net,<br>Random Forest,<br>Support Vector Machine,<br>Gaussian Process,<br>Transformer,<br>SE(3)-equivariant GNN (GemNet-dT),<br>Diffusion Model (general) |
| **Tasks** | Image Generation,<br>Image-to-Image Translation,<br>Synthetic Data Generation,<br>Regression,<br>Classification,<br>Data Generation,<br>Graph Generation,<br>Image Denoising,<br>Hyperparameter Optimization,<br>Image Super-Resolution,<br>Structured Prediction |
| **Learning Methods** | Unsupervised Learning,<br>Supervised Learning,<br>Active Learning,<br>Representation Learning,<br>Fine-Tuning,<br>Self-Supervised Learning |
| **Performance Highlights** | generation_success_rate: less than 1%,<br>generation_efficiency_improvement: 100x (when adding property constraints in loss) |
| **Application Domains** | Materials science,<br>Crystal structure inverse design,<br>Microstructure generation and design,<br>Porous materials design,<br>Metal alloys (e.g., high-entropy alloys, NiTi shape memory alloys),<br>Battery electrode microstructures (Li-ion cathode),<br>Solid Oxide Fuel Cell (SOFC) anode microstructures,<br>Metal additive manufacturing and process-structure optimization,<br>Mechanoluminescent composites,<br>MOF (metal-organic frameworks) / nanoporous crystalline materials,<br>Magnetic materials |

---


### [183. Are LLMs Ready for Real-World Materials Discovery?](https://doi.org/10.48550/arXiv.2402.05200), Preprint *(September 25, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | MaScQA,<br>Battery Device QA,<br>MatSciNLP,<br>OpticalTable / OpticalTable-SQA,<br>SustainableConcrete,<br>Cambridge Structural Database (CSD),<br>Publisher machine-readable MatSci corpus (Springer/Elsevier APIs),<br>S2ORC (Semantic Scholar Open Research Corpus),<br>RedPajama (open dataset) |
| **Models** | GPT,<br>BERT,<br>Transformer,<br>Variational Autoencoder,<br>Diffusion Model,<br>Graph Neural Network,<br>Message Passing Neural Network |
| **Tasks** | Question Answering,<br>Named Entity Recognition,<br>Information Retrieval,<br>Text Generation,<br>Structured Prediction,<br>Sequence Labeling,<br>Feature Extraction,<br>Image Classification,<br>Clustering |
| **Learning Methods** | Pre-training,<br>Fine-Tuning,<br>In-Context Learning,<br>Transfer Learning,<br>Few-Shot Learning,<br>Weakly Supervised Learning |
| **Performance Highlights** | accuracy_overall: 62%,<br>numerical_questions_accuracy: 39%,<br>relative_to_top_humans: ≈50% of top-performing humans,<br>code_generation_accuracy: 71%,<br>table_property_extraction_recall: ≈55% |
| **Application Domains** | Materials Science (core),<br>Crystallography / inorganic materials design,<br>Chemistry (adjacent domain; synthesis/retrosynthesis),<br>Energy storage (batteries, electrolytes),<br>Cement / concrete (sustainable concrete design),<br>Glass science and optical materials,<br>Robotics / autonomous laboratories (experimental execution),<br>Healthcare / biomedical materials (medical implants),<br>Manufacturing / materials processing |

---


### [182. Electronic descriptors for dislocation deformation behavior and intrinsic ductility in bcc high-entropy alloys](https://doi.org/10.1126/sciadv.adp7670), Science Advances *(September 20, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | DFT dislocation core and vibrational dataset for selected bcc HEAs,<br>DFT bulk DOS descriptor dataset (cubic 54-atom cells) across >50 bcc complex concentrated alloys,<br>Surface and unstable stacking fault (USF) energy dataset,<br>Experimental compressive fracture strain dataset (literature compilation) |
| **Models** | _None_ |
| **Tasks** | Feature Extraction,<br>Ranking,<br>Clustering,<br>Data Generation |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | Materials Science,<br>Computational Materials Design,<br>Alloy Discovery / High-Entropy Alloys,<br>Mechanical Behavior / Deformation Mechanisms |

---


### [181. Scalable crystal structure relaxation using an iteration-free deep generative model with uncertainty quantification](https://doi.org/10.1038/s41467-024-52378-3), Nature Communications *(September 17, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | X-Mn-O dataset,<br>Materials Project (MP) dataset,<br>C2DB (Computational 2D Materials Database),<br>Layered van der Waals (vdW) crystals dataset,<br>MoS2 defect dataset,<br>DeepMind 2.2 million hypothetical crystals (referenced) |
| **Models** | Graph Neural Network |
| **Tasks** | Regression,<br>Data Generation |
| **Learning Methods** | Supervised Learning,<br>Transfer Learning,<br>Pre-training,<br>Fine-Tuning,<br>Ensemble Learning,<br>Gradient Descent |
| **Performance Highlights** | X-Mn-O_coordinates_MAE_A: 0.116,<br>X-Mn-O_bond_length_MAE_A: 0.136,<br>X-Mn-O_lattice_MAE_A: 0.063,<br>X-Mn-O_cell_volume_MAE_A3: 3.4,<br>X-Mn-O_match_rate_percent: 84.7,<br>MP_coordinates_MAE_A: 0.066,<br>MP_bond_length_MAE_A: 0.094,<br>MP_lattice_MAE_A: 0.041,<br>MP_cell_volume_MAE_A3: 9.6,<br>C2DB_coordinates_MAE_A_DeepRelax: 0.196,<br>C2DB_bond_length_MAE_A_DeepRelax: 0.268,<br>C2DB_lattice_MAE_A_DeepRelax: 0.085,<br>C2DB_cell_volume_MAE_A3_DeepRelax: 60.2,<br>C2DB_coordinates_MAE_A_DeepRelaxT: 0.185,<br>C2DB_bond_length_MAE_A_DeepRelaxT: 0.165,<br>C2DB_lattice_MAE_A_DeepRelaxT: 0.082,<br>C2DB_cell_volume_MAE_A3_DeepRelaxT: 56.7,<br>X-Mn-O_PAINN_coordinates_MAE_A: 0.159,<br>X-Mn-O_PAINN_bond_length_MAE_A: 0.175,<br>X-Mn-O_PAINN_lattice_MAE_A: 0.066,<br>X-Mn-O_PAINN_cell_volume_MAE_A3: 3.8,<br>X-Mn-O_PAINN_match_rate_percent: 81.2,<br>X-Mn-O_EGNN_coordinates_MAE_A: 0.166,<br>X-Mn-O_EGNN_bond_length_MAE_A: 0.189,<br>X-Mn-O_EGNN_lattice_MAE_A: 0.066,<br>X-Mn-O_EGNN_cell_volume_MAE_A3: 4.2,<br>X-Mn-O_EGNN_match_rate_percent: 77.5,<br>X-Mn-O_Cryslator_coordinates_MAE_A: 0.127,<br>X-Mn-O_Cryslator_cell_volume_MAE_A3: 6.2,<br>X-Mn-O_Cryslator_match_rate_percent: 83.7,<br>Ablation_Dummy_coordinates_MAE_A: 0.314,<br>Ablation_Vanilla_coordinates_MAE_A: 0.155,<br>Ablation_DeepRelax(UCOE)_coordinates_MAE_A: 0.121,<br>Ablation_DeepRelax(BLDU)_coordinates_MAE_A: 0.142,<br>Ablation_DeepRelax_full_coordinates_MAE_A: 0.116,<br>Spearman_correlation_system_uncertainty_vs_error_X-Mn-O: 0.95,<br>Spearman_correlation_system_uncertainty_vs_error_MP: 0.83,<br>Spearman_correlation_system_uncertainty_vs_error_C2DB: 0.88,<br>Energy_MAE_reduction_X-Mn-O_samples_kJ?or_eV?: energy MAE reduced from 32.51 to 5.97 (units: unclear in text but reported as energy MAE),<br>DFT_ionic_steps_reduction_example: Starting DFT from DeepRelax-predicted structures significantly reduces number of ionic steps vs starting from unrelaxed structures (Fig.4f).,<br>Speedup_vs_M3GNet_factor: 100 |
| **Application Domains** | Materials science (crystal structure relaxation),<br>Computational chemistry / computational materials discovery,<br>2D materials and defect engineering,<br>High-throughput virtual screening of hypothetical materials |

---


### [180. De novo design of high-affinity protein binders with AlphaProteo](https://doi.org/10.48550/arXiv.2409.08022), Preprint *(September 12, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Protein Data Bank (PDB) — structural examples used for training and benchmarking,<br>Distillation set of AlphaFold predictions,<br>Cao et al. de novo binder dataset (retrospective benchmark),<br>Published RFdiffusion designs (downloaded set),<br>This paper: experimental screening dataset (yeast surface display + follow-up expression/KD),<br>Experimental neutralization assay data (SARS-CoV-2) |
| **Models** | Diffusion Model,<br>Graph Neural Network,<br>Convolutional Neural Network,<br>Transformer |
| **Tasks** | Synthetic Data Generation,<br>Binary Classification,<br>Regression,<br>Ranking,<br>Clustering |
| **Learning Methods** | Generative Learning,<br>Pre-training,<br>Knowledge Distillation,<br>Zero-Shot Learning,<br>Supervised Learning,<br>Representation Learning |
| **Performance Highlights** | experimental_success_rate_BHRF1_percent: 88,<br>experimental_success_rate_SC2RBD_percent: 12,<br>experimental_success_rate_IL-7RA_percent: 25,<br>experimental_success_rate_PD-L1_percent: 15,<br>experimental_success_rate_TrkA_percent: 9,<br>experimental_success_rate_IL-17A_percent: 14,<br>experimental_success_rate_VEGF-A_percent: 33,<br>experimental_success_rate_TNFα_percent: 0,<br>best_per_target_KD_BHRF1_nM: 8.5,<br>best_per_target_KD_SC2RBD_nM: 26,<br>best_per_target_KD_IL-7RA_nM: 0.082,<br>best_per_target_KD_PD-L1_nM: 0.18,<br>best_per_target_KD_TrkA_nM: 0.96,<br>best_per_target_KD_IL-17A_nM: 8.4,<br>best_per_target_KD_VEGF-A_nM: 0.48,<br>overall_best_KD_nM: 0.082,<br>fold_improvement_vs_best_previous_unoptimized_range: 3x to 300x (per abstract/main results),<br>notable_effect: improved filter increased experimental success on SC2RBD and PD-L1 (see Section S2 and Table S2),<br>Example_SC2RBD_success_with_improved_filters_percent (AlphaProteo v1 improved filters): 29,<br>Example_PD-L1_success_with_v2_percent (AlphaProteo v2 on PD-L1): 26.5,<br>AF3-optimized_filter_thresholds: min pae interaction <1.5; ptm binder >0.8; rmsd <2.5 (derived in Section S2.2),<br>retrospective_enrichment: AF3-based filters enriched for experimental success more strongly than AF2-based filters on Cao et al. dataset (Section S2.2, Figure S1C),<br>role: ProteinMPNN used to redesign sequences with low sampling temperature (0.0001) during AF2-based benchmark reproduction,<br>contextual_performance: used as part of AF2 benchmark reproduction pipeline; not reported as a standalone performance metric in paper |
| **Application Domains** | Protein design / structural biology,<br>Therapeutics (binder design for therapeutic targets such as PD-L1, TrkA, IL-7RA, VEGF-A, IL-17A, TNFα),<br>Virology (SARS-CoV-2 neutralization),<br>Biotechnology / research reagents (ready-to-use binders for imaging, signaling modulation, etc.),<br>Cryo-EM and X-ray structural validation workflows |

---


### [179. ChemOS 2.0: An orchestration architecture for chemical self-driving laboratories](https://doi.org/10.1016/j.matt.2024.04.022), Matter *(September 04, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | DFT calculations (ChemOS 2.0 DFT database uploaded to ioChem-BD),<br>Experimental database (internal ChemOS 2.0 experimental RDBMS),<br>Enumerated BSBCz derivative search space (products of double Suzuki-Miyaura coupling with 38 commercially available dihalides),<br>Frozen ChemOS 2.0 code/version and experimental data (GitHub repository / Zenodo snapshot) |
| **Models** | Gaussian Process |
| **Tasks** | Optimization,<br>Experimental Design,<br>Regression |
| **Learning Methods** | Active Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | Materials discovery,<br>Chemistry (synthetic organic chemistry),<br>Optoelectronic materials / organic laser molecules,<br>Automated experimentation / self-driving laboratories,<br>Computational chemistry (DFT simulations) |

---


### [178. Closed-loop transfer enables artificial intelligence to yield chemical knowledge](https://doi.org/10.1038/s41586-024-07892-1), Nature *(September 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | 2,200-molecule design space (donor–bridge–acceptor combinatorial space),<br>BO-synthesized experimental rounds (Phase I): 30 molecules,<br>Full experimental photostability dataset (CLT campaign),<br>Predicted photostabilities across 2,200 molecules (DFT+RDKit featurizations) |
| **Models** | Support Vector Machine,<br>Linear Model |
| **Tasks** | Regression,<br>Optimization,<br>Feature Selection,<br>Feature Extraction,<br>Dimensionality Reduction |
| **Learning Methods** | Supervised Learning |
| **Performance Highlights** | LOOV_R2: 0.86,<br>Spearman_R2_on_validation_batches: 0.54,<br>Mann-Whitney_p_value: 0.026,<br>Top7_avg_photostability: 165,<br>Bottom7_avg_photostability: 97,<br>Most_predictive_models_R2_threshold: >0.70,<br>Top5_avg_photostability_improvement: >500%,<br>sampling_fraction: <1.5% of 2,200 space |
| **Application Domains** | molecular photostability / photodegradation for light-harvesting small molecules,<br>organic electronics (organic photovoltaics, organic light-emitting diodes),<br>dyed polymers and photo-active coatings,<br>solar fuels and photosynthetic system analogues,<br>organic laser emitters (mentioned as further application),<br>stereoselective aluminium complexes for ring-opening polymerization (mentioned as further application) |

---


### [177. AI-driven research in pure mathematics and theoretical physics](https://doi.org/10.1038/s42254-024-00740-1), Nature Reviews Physics *(September 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | arXiv,<br>viXra,<br>Lean MathLib,<br>MathPile,<br>CICY threefolds / Complete Intersection Calabi–Yau (CICY) datasets,<br>Kreuzer–Skarke Calabi–Yau database,<br>Sliding-window binary sequence dataset (synthetic example in Box 2) |
| **Models** | Decision Tree,<br>Support Vector Machine,<br>Feedforward Neural Network,<br>Multi-Layer Perceptron,<br>Perceptron,<br>Transformer,<br>Random Forest |
| **Tasks** | Binary Classification,<br>Classification,<br>Image Classification,<br>Language Modeling,<br>Clustering,<br>Optimization,<br>Sequence-to-Sequence,<br>Language Modeling |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Reinforcement Learning,<br>Pre-training,<br>Fine-Tuning |
| **Performance Highlights** | accuracy_first_sequence: 100%,<br>accuracy_second_sequence: about 80%,<br>accuracy_third_sequence: about 50%,<br>accuracy_second_sequence: about 80%,<br>separation_found: qualitative (separation between simple and non-simple finite groups observed; no numeric metric provided),<br>sequence_task_accuracy_first: 100%,<br>sequence_task_accuracy_second: about 80%,<br>algebraic_variety_task_accuracy: >99.9%,<br>algorithm_discovery: discovered faster matrix multiplication algorithm (reported), later superseded by a human-derived algorithm |
| **Application Domains** | Pure mathematics,<br>Theoretical physics,<br>Algebraic geometry,<br>Number theory,<br>String theory / string landscape,<br>Representation theory and algebraic structures,<br>Combinatorics and graph theory,<br>Knot theory,<br>Quantum field theory,<br>Theoretical cosmology,<br>Symbolic mathematics / automated theorem proving |

---


### [175. Machine learning enables the discovery of 2D Invar and anti-Invar monolayers](https://doi.org/10.1038/s41467-024-51379-6), Nature Communications *(August 14, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | C2DB (computational 2D materials database) - selected stable subset,<br>Full 2D materials collections (contextual),<br>Subset for ZA-mode contribution analysis,<br>Training/validation splits used in ML experiments |
| **Models** | Random Forest,<br>Support Vector Machine |
| **Tasks** | Classification,<br>Regression,<br>Feature Selection,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Bagging,<br>Active Learning |
| **Performance Highlights** | accuracy: 100% (all training data perfectly classified),<br>alpha500K_regression_RMSE_train: 1.67 × 10^-6 K^-1,<br>alpha500K_regression_RMSE_test: 1.35 × 10^-6 K^-1,<br>alpha500K_regression_R2_train: 0.91,<br>alpha500K_regression_R2_test: 0.93,<br>D_prediction_RMSE: 1.34 eV |
| **Application Domains** | Materials science,<br>2D materials,<br>Thermal expansion / thermal management,<br>Nanotechnology / nanoelectronics,<br>Computational materials discovery / high-throughput materials screening |

---


### [174. Accurate prediction of protein function using statistics-informed graph networks](https://doi.org/10.1038/s41467-024-50955-0), Nature Communications *(August 04, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | PDB-derived GO benchmark (41,896 protein chains),<br>PDB-derived EC benchmark (20,215 protein chains),<br>CAFA3 dataset,<br>Independent hold-out set (RCSB PDB post-2022),<br>UniClust30 multiple sequence alignments (MSAs),<br>UniProt / UniRef50 (ESM-1b pretraining corpora),<br>BioLiP (semi-manually curated ligand-protein interactions),<br>PhiGnet application output on UniProt |
| **Models** | Graph Convolutional Network,<br>Transformer,<br>Convolutional Neural Network,<br>Multi-Layer Perceptron,<br>Triplet Network,<br>Graph Neural Network |
| **Tasks** | Multi-label Classification,<br>Multi-class Classification,<br>Sequence Labeling,<br>Representation Learning,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Pre-training,<br>Transfer Learning,<br>Fine-Tuning,<br>Mini-Batch Learning,<br>Representation Learning,<br>Contrastive Learning,<br>End-to-End Learning |
| **Performance Highlights** | AUPR_overall: 0.70,<br>F_max_overall: 0.80,<br>AUPR_CC: 0.64,<br>F_max_CC: 0.82,<br>AUPR_BP: 0.65,<br>F_max_BP: 0.75,<br>AUPR_MF: 0.80,<br>F_max_MF: 0.81,<br>MCC_average: 0.76,<br>AUPR: 0.89,<br>F_max: 0.88,<br>robust_F_max_at_30%_seqid: 0.61,<br>robust_F_max_at_40%_seqid: 0.72,<br>residue_level_accuracy_on_9_proteins: >=75% (average),<br>qualitative_examples: near-perfect predictions for cPLA2α, Ribokinase, αLA, TmpK, and Ecl18kI,<br>EC_F_max: 0.37,<br>EC_AUPR: 0.21,<br>EC_F_max: 0.69,<br>EC_AUPR: 0.70,<br>EC_F_max: 0.76,<br>EC_AUPR: 0.70,<br>CAFA3_F_max_BP: 0.458,<br>CAFA3_F_max_CC: 0.493,<br>CAFA3_F_max_MF: 0.470,<br>CAFA3_AUPR_BP: 0.378,<br>CAFA3_AUPR_CC: 0.361,<br>CAFA3_AUPR_MF: 0.323 |
| **Application Domains** | Protein function prediction / bioinformatics,<br>Structural biology (residue-level functional site identification),<br>Genomics / proteomics (large-scale UniProt annotation),<br>Drug discovery and biomedical research (interpretation of functional residues and disease variants),<br>Evolutionary biology (leveraging evolutionary couplings and residue communities) |

---


### [173. Accelerated discovery of perovskite solid solutions through automated materials synthesis and characterization](https://doi.org/10.1038/s41467-024-50884-y), Nature Communications *(August 02, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Experimental dataset from ICSD (perovskites and non-perovskites),<br>Candidate pool of disordered compositions,<br>Materials Project ABO3 subset (stable/metastable perovskites),<br>Experimental data and code repository |
| **Models** | Gradient Boosting Tree,<br>Variational Autoencoder |
| **Tasks** | Binary Classification,<br>Ranking,<br>Recommendation,<br>Feature Extraction,<br>Representation Learning |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Representation Learning,<br>Feature Learning |
| **Performance Highlights** | accuracy: 94% |
| **Application Domains** | Materials science (perovskite oxide discovery),<br>Automated materials synthesis (self-driving laboratory / robotic synthesis),<br>High-frequency dielectric characterization (microwave dielectric materials),<br>Wireless communications (tunable devices, antennas) and biosensors (application areas motivating materials discovery) |

---


### [172. The power and pitfalls of AlphaFold2 for structure prediction beyond rigid globular proteins](https://doi.org/10.1038/s41589-024-01638-w), Nature Chemical Biology *(August 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | AlphaFold Protein Structure Database,<br>Protein Data Bank (PDB),<br>Human proteome models (AF2 coverage),<br>McDonald et al. peptide benchmark (588 peptides),<br>Yin et al. heterodimer benchmark (152 heterodimeric complexes),<br>Bryant et al. heterodimer benchmark (dataset used in their study),<br>Terwilliger et al. molecular-replacement benchmark (215 structures),<br>Membrane protein benchmarks (various sets),<br>NMR ensemble datasets (general),<br>SAXS / SANS datasets and Small-Angle Scattering Biological Data Bank derived datasets |
| **Models** | Transformer,<br>Attention Mechanism |
| **Tasks** | Structured Prediction,<br>Sequence-to-Sequence,<br>Clustering |
| **Learning Methods** | Pre-training,<br>Fine-Tuning,<br>Ensemble Learning,<br>Representation Learning,<br>Transfer Learning,<br>Stochastic Learning |
| **Performance Highlights** | human_proteome_coverage: 98.5% modeled,<br>high_confidence_residue_fraction: ~50% of residues across all proteins predicted with high confidence (cited average),<br>pLDDT_thresholds: pLDDT > 70 interpreted as higher confidence; pLDDT > 90 as very high,<br>peptide_benchmark_size: 588 peptides (McDonald et al.),<br>peptide_prediction_note: AF2 predicts many α-helical and β-hairpin peptide structures with surprising accuracy (no single numeric accuracy given in paper excerpt),<br>heterodimer_success_Yin: 51% success rate (AF2 and AlphaFold2-Multimer on 152 heterodimeric complexes),<br>heterodimer_success_Bryant: 63% success rate (Bryant et al. study),<br>molecular_replacement_success: 187 of 215 structures solved using AlphaFold-guided molecular replacement (Terwilliger et al.),<br>alternative_conformation_sampling_note: modifications (reduced recycles, shallow MSAs, MSA clustering, enabling dropout) allow sampling of alternative conformations (no single numeric accuracy provided),<br>AlphaMissense_note: AlphaMissense provides probability of missense variant pathogenicity; AF2 itself 'has not been trained or validated for predicting the effect of mutations' (authors' caution) |
| **Application Domains** | Structural biology (protein 3D structure prediction and validation),<br>Proteomics (proteome-scale modeling; human proteome coverage),<br>Integrative structural methods (integration with SAXS, NMR, cryo-EM, X-ray diffraction),<br>Drug discovery / therapeutics (identifying therapeutic candidates, ligand/cofactor modeling),<br>Membrane protein biology (transmembrane protein modeling),<br>Intrinsically disordered proteins (IDPs/IDRs) and conformational ensembles,<br>Peptide biology and peptide–protein interactions,<br>De novo protein design |

---


### [171. OpenFold: retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization](https://doi.org/10.1038/s41592-024-02272-z), Nature Methods *(August 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | OpenProteinSet (replication of AlphaFold2 training set),<br>Protein Data Bank (PDB),<br>Uniclust MSAs,<br>CAMEO validation set,<br>CASP15 domains,<br>CATH-derived domain splits (topologies/architectures/classes),<br>Subsampled training sets (ablation experiments),<br>Rosetta decoy ranking dataset (subset) |
| **Models** | Transformer,<br>Attention Mechanism,<br>Self-Attention Network,<br>Multi-Head Attention |
| **Tasks** | Regression,<br>Sequence Labeling,<br>Binary Classification,<br>Ranking |
| **Learning Methods** | Supervised Learning,<br>Knowledge Distillation,<br>Fine-Tuning,<br>Pre-training,<br>Self-Supervised Learning,<br>Distributed Learning,<br>Ensemble Learning |
| **Performance Highlights** | OpenFold (mean lDDT-Cα on CAMEO, main comparison): 0.911,<br>AlphaFold2 (mean lDDT-Cα on CAMEO, main comparison): 0.913,<br>OpenFold final replication (after clamping change): 0.902 lDDT-Cα (on CAMEO validation set),<br>Full data model peak (early reported value): 0.83 lDDT-Cα (after 20,000 steps),<br>10,000-sample subsample after 7,000 steps: exceeded 0.81 lDDT-Cα,<br>1,000-chain ablation (short run, ~7,000 steps): 0.64 lDDT-Cα,<br>Inference speedup (overall OpenFold vs AlphaFold2): up to 3-4x faster (single A100 GPU),<br>FlashAttention effect on short sequences (<1000 residues): up to 15% additional speedup in OpenFold when applicable,<br>Sequence length robustness: OpenFold runs successfully on sequences and complexes exceeding 4,000 residues; AlphaFold2 crashes beyond ~2,500 residues on single GPU,<br>Secondary structure learning order (qualitative): α-helices learned first, then β-sheets, then less common SSEs (measured by F1 over DSSP categories); final high F1 scores for SSEs,<br>Contact F1 (for fragments / SSEs): improves earlier for shorter helices and narrower sheets; specific numbers are plotted in Fig. 5b and Extended Data Fig. 8 (no single consolidated numeric value in text),<br>Number of models in final ensemble: 10 distinct models (seven snapshots from main run + additional models from branch),<br>Effect: Ensemble used at prediction time to generate alternate structural hypotheses; explicit ensemble metric improvement not numerically summarized in a single value in main text |
| **Application Domains** | Protein structural biology,<br>Biomolecular modeling (protein complexes, peptide–protein interactions),<br>Evolutionary sequence analysis / MSA-based modeling,<br>RNA structure prediction (discussed as potential application),<br>Spatial reasoning over polymers and arbitrary molecules (structure module / invariant point attention) |

---


### [170. Large-scale foundation model on single-cell transcriptomics](https://doi.org/10.1038/s41592-024-02305-7), Nature Methods *(August 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Pretraining corpus of human single-cell RNA-seq (scRNA-seq) profiles,<br>Validation split of pretraining data,<br>Read-depth enhancement test set (unseen cells),<br>Zheng68K,<br>Segerstolpe dataset,<br>Baron (pancreatic islet) dataset processed by SAVER,<br>Cancer drug response (CDR) dataset (DeepCDR preprocessed),<br>Single-cell drug response classification dataset (SCAD),<br>Perturbation datasets (Perturb-seq resources),<br>Simulated reference and query dataset; organoid and in vivo data |
| **Models** | Transformer,<br>Multi-Layer Perceptron,<br>Graph Neural Network,<br>Variational Autoencoder,<br>BERT,<br>GPT,<br>Attention Mechanism |
| **Tasks** | Regression,<br>Clustering,<br>Multi-class Classification,<br>Binary Classification,<br>Graph Generation,<br>Link Prediction,<br>Representation Learning |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Transfer Learning,<br>Fine-Tuning,<br>Domain Adaptation,<br>Supervised Learning,<br>Representation Learning,<br>Pre-training |
| **Performance Highlights** | MAE: about 50% reduction (text: 'notable reduction of half the MAE'),<br>MRE: about 50% reduction (text: 'notable reduction of half the ... MRE'),<br>PCC: substantial increase (visual reported; specific case examples given in Fig. 2b),<br>PCC_best_case: 0.93 (overall example: Pearson: 0.93, Spearman: 0.87, N = 73),<br>PCC_WZ-1-84: Pearson: 0.94, Spearman: 0.95, N = 8,<br>Leave-one-drug-out_improvement: Top drug PHA-793887: PCC improved from 0.07 to 0.73; example drug zibotentan improved from 0.49 to 0.64,<br>AUCs_scFoundation: 0.84, 0.84, 0.66, 0.68 (four drugs reported in figure),<br>AUCs_baseline: 0.62, 0.56, 0.38, 0.66 (baseline SCAD with raw gene expression),<br>Spearman_correlations: For NVP-TAE684: scFoundation 0.56 vs baseline 0.24; for sorafenib: scFoundation -0.55 vs baseline -0.06,<br>MSE: scFoundation-based GEARS achieved lower averaged MSE compared with baseline GEARS and CPA across tested perturbation datasets (numerical MSEs reported in figures / Supplementary),<br>Averaged_MSE_two-gene_0/2_unseen: lowest averaged MSE for scFoundation-based model (text statement),<br>PCC_magnitude_score: scFoundation Pearson: 0.18; baseline Pearson: 0.01 (Fig. 5e),<br>macro_F1: scFoundation achieved the highest macro F1 score on both Zheng68K and Segerstolpe datasets (exact numeric values provided in Supplementary Table 4),<br>improvement_on_rare_types: improved performance on rare cell types such as CD4+ T helper 2 and CD34+,<br>NMI/ARI/SIL: scFoundation outperformed baseline downsampled data and other imputation methods (scImpute, scVI, SAVER, MAGIC) across these clustering metrics (see Fig. 2c and 2f),<br>SIL_on_Zheng68K: scFoundation had a higher silhouette coefficient compared to scVI and raw data (text: 'scFoundation had a higher SIL score') |
| **Application Domains** | single-cell transcriptomics / computational biology,<br>cancer pharmacogenomics (drug response prediction),<br>single-cell perturbation analysis (Perturb-seq),<br>cell type annotation and atlas construction,<br>gene module discovery and gene regulatory network inference,<br>bioinformatics data integration and batch correction |

---


### [169. Sequential closed-loop Bayesian optimization as a guide for organic molecular metallophotocatalyst formulation discovery](https://doi.org/10.1038/s41557-024-01546-5), Nature Chemistry *(August 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Virtual library of 560 CNP molecules,<br>First experimental CNP dataset (BO-synthesized),<br>Expanded experimental CNP dataset (BO + diversity baseline),<br>Predicted set of 100 new CNPs,<br>Reaction-condition space (encoded),<br>Experimental reaction-condition dataset (BO + initial) |
| **Models** | Gaussian Process,<br>Radial Basis Function Network |
| **Tasks** | Optimization,<br>Regression,<br>Experimental Design,<br>Dimensionality Reduction,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Batch Learning |
| **Performance Highlights** | max_yield_first_bo_campaign: 67%,<br>samples_evaluated_first_campaign: 55 of 560 CNPs,<br>max_yield_reaction_condition_optimization: 88%,<br>samples_evaluated_reaction_conditions: 107 of 4,500 possible conditions,<br>BO_vs_random_max_yield: 88% (BO) vs 75% (random baseline),<br>high_activity_fraction_BO: 39 of 88 (44%) samples had yield >67%,<br>high_activity_fraction_random: 2 of 44 (4.5%) samples had yield >=67%,<br>prediction_accuracy_examples: Predicted yields for new CNPs matched experimental yields for three synthesized candidates (CNP-561, CNP-565, CNP-577) 'close to their predicted values' (no numeric error given in text) |
| **Application Domains** | organic chemistry,<br>photoredox catalysis,<br>metallaphotocatalysis (photoredox + nickel dual catalysis),<br>catalyst discovery,<br>reaction-condition optimization,<br>closed-loop experiment design / autonomous/algorithm-guided experimentation |

---


### [168. Compositional design of multicomponent alloys using reinforcement learning](https://doi.org/10.1016/j.actamat.2024.120017), Acta Materialia *(August 01, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | 112 composition-enthalpy dataset (initial training data),<br>Candidate search space of multicomponent alloys,<br>Experimental dataset synthesized in this work,<br>Nine large-enthalpy samples (thermal diffusivity measurements),<br>Synthetic benchmark functions (Ackley, Rastrigin, Levy) |
| **Models** | Gaussian Process,<br>Gradient Boosting Tree,<br>Multi-Layer Perceptron |
| **Tasks** | Regression,<br>Decision Making,<br>Optimization,<br>Experimental Design,<br>Feature Selection / Feature Extraction |
| **Learning Methods** | Reinforcement Learning,<br>Supervised Learning,<br>Active Learning,<br>Pre-training,<br>Mini-Batch Learning,<br>Evolutionary Learning |
| **Performance Highlights** | training_size: 112 samples,<br>example_prediction_vs_experiment_error_J_per_g: -31.94 (predicted) vs -31.1 (experimental) for Ti53Ni47 (error ~0.84 J/g),<br>diagnostics_reported: R2 increased and RMSE decreased as iterations proceed (exact numerical R2/RMSE values shown in Fig. 2b in paper),<br>training_episodes_to_converge: ≈3,000 (converges in less than 4,000 episodes across seeds),<br>surrogate_calls: agent needs less than 4,000 ×5 surrogate model interactions to propose compositions with maximum cumulative rewards (calls per episode: 5),<br>final_experimental_result: Ti27.2Ni47Hf13.8Zr12 with transformation enthalpy ΔH = -37.1 J/g (corrected to -39.0 J/g after calibration),<br>success_rate_experiment: 33 out of 40 synthesized compositions exhibited martensitic transformation,<br>selected_features: Pettifer chemical scale, valence electron numbers of average atomic number, difference of atomic radii, configurational entropy,<br>contextual_note: GB used to rank feature importance prior to GP training,<br>pretraining_effect: Pretrained agent accesses regions of higher reward values more frequently and learns high-rewarding states with greater probability than agent without pretraining (t-SNE, entropy reduction reported in supplemental figures),<br>scaling_reduction: Interactions with surrogate reduced from O(N) in BGO to ~ O(5255 × N^0.06) for ordered-substitution case (and ~ O(715 × N^0.176) if order is random) estimated by fitting required interactions vs N,<br>black-box_benchmarks: RL outperforms BGO and GA for Ackley, Rastrigin, Levy functions across dimensionalities up to 10 when function call budget is large (3,000); with small budgets (100) RL underperforms,<br>relative_performance_with_budget_3000: GA and BGO are outperformed by RL on most tested functions/dimensionalities up to 10,<br>relative_performance_with_budget_100: With fewer experimental iterations (100), BGO and GA outperform RL for certain problems (e.g., Rastrigin and higher-dimensional Ackley) |
| **Application Domains** | Materials science (alloy design),<br>Phase change materials (PCMs),<br>Shape memory alloys (SMAs),<br>Thermal energy storage / thermal management,<br>Autonomous experimentation / closed-loop experimental design,<br>Black-box optimization and algorithm benchmarking |

---


### [166. Autonomous chemistry: Navigating self-driving labs in chemical and material sciences](https://doi.org/10.1016/j.matt.2024.06.003), Matter *(July 03, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | literature reaction / chemistry datasets (general),<br>Adam platform measurements (bioinformatics / enzyme search),<br>A-lab generated inorganic compounds dataset,<br>High-throughput experimental datasets (HPLC, NMR, IR, mass spec outputs) |
| **Models** | Gaussian Process,<br>Decision Tree,<br>Random Forest,<br>Multi-Layer Perceptron,<br>Transformer |
| **Tasks** | Optimization,<br>Optimization,<br>Optimization,<br>Data Generation,<br>Optimization |
| **Learning Methods** | Gradient Descent,<br>Reinforcement Learning,<br>Evolutionary Learning,<br>Multi-Task Learning,<br>Active Learning,<br>Supervised Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | synthetic chemistry / reaction optimization,<br>materials science / material property optimization,<br>analytical chemistry (e.g., chromatography method optimization),<br>drug discovery / compound discovery,<br>biological discovery (gene/enzyme identification),<br>automation/robotics integration and lab orchestration |

---


### [165. Has generative artificial intelligence solved inverse materials design?](https://doi.org/10.1016/j.matt.2024.05.017), Matter *(July 03, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | MatterGen pretraining dataset (~1 million unique bulk crystal structures),<br>public materials datasets (general),<br>stable materials subset (used by CDVAE) |
| **Models** | Variational Autoencoder,<br>Generative Adversarial Network,<br>Denoising Diffusion Probabilistic Model,<br>Transformer,<br>Graph Neural Network,<br>Graph Convolutional Network,<br>Convolutional Neural Network,<br>Conditional GAN,<br>GPT,<br>DALL-E |
| **Tasks** | Synthetic Data Generation,<br>Graph Generation,<br>Text Generation,<br>Language Modeling,<br>Regression,<br>Optimization,<br>Distribution Estimation,<br>Synthetic Data Generation |
| **Learning Methods** | Unsupervised Learning,<br>Self-Supervised Learning,<br>Adversarial Training,<br>Pre-training,<br>Fine-Tuning,<br>Representation Learning,<br>Adversarial Training |
| **Performance Highlights** | reconstruction_loss: not numerically specified (qualitative: FTCP suffers heavy reconstruction losses; WYCRYST demonstrates lower reconstruction losses than FTCP),<br>validity: WYCRYST demonstrates higher structure validity than FTCP (qualitative),<br>precision: VAE showed slightly higher precision and was easier to train than a Wasserstein GAN in one elpasolite composition study (qualitative, no numeric value),<br>structure_validity: diffusion models outperform GANs on structure validity (qualitative),<br>distribution_quality: learned distribution of crystal structure parameters better for diffusion (C RYSTENS) than GANs (qualitative),<br>low_energy_targeting: CRYSTAL-LLM (fine-tuned LLAMA-2) outperforms early diffusion-based models (e.g., CDVAE) in targeting low-energy configurations (qualitative) |
| **Application Domains** | Inverse materials design for inorganic crystalline materials,<br>Crystal structure generation and prediction,<br>Property-targeted materials design (e.g., band gap, bulk modulus),<br>Porous materials / zeolite design,<br>Surface structure discovery (oxide formation on surfaces),<br>Multi-component alloys and composition design,<br>Bulk metallic glasses design,<br>Thin-film synthesis / processing parameter design,<br>General materials discovery workflows (screening + generative proposals) |

---


### [164. Promising directions of machine learning for partial differential equations](https://doi.org/10.1038/s43588-024-00643-2), Nature Computational Science *(July 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | 2D simulations of fluid flow past a circular cylinder,<br>Large-scale mesoscale ocean model outputs (LES closure training data),<br>High-fidelity particle-in-cell simulations,<br>Kolmogorov 2D flow dataset (Kochkov et al.) / turbulent flow fields,<br>Forced turbulence training dataset (illustrated in Fig. 5),<br>PDEBench (benchmark problems) |
| **Models** | Autoencoder,<br>Convolutional Neural Network,<br>Long Short-Term Memory,<br>Transformer,<br>Graph Neural Network,<br>Gaussian Process,<br>Support Vector Machine,<br>ResNet,<br>Feedforward Neural Network,<br>Multi-Layer Perceptron |
| **Tasks** | Regression,<br>Dimensionality Reduction,<br>Time Series Forecasting,<br>Image Super-Resolution,<br>Feature Extraction,<br>Clustering,<br>Optimization |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Ensemble Learning,<br>Representation Learning,<br>Transfer Learning,<br>Bayesian / Probabilistic Methods (mapped to available list via Gaussian Process),<br>End-to-End Learning |
| **Performance Highlights** | Reynolds_number_identification_error: within 1% of true value,<br>speedup: ≈86× (figure text) ; described as 'two orders of magnitude acceleration' in accuracy vs computational cost comparison |
| **Application Domains** | Fluid dynamics / turbulence modeling,<br>Climate science / oceanography,<br>Plasma physics,<br>Weather forecasting,<br>Neuroscience (spatiotemporal brain data),<br>Epidemiology,<br>Materials science,<br>Biology (collective dynamics, bacterial colonies, organized biological matter) |

---


### [163. Prediction of DNA origami shape using graph neural network](https://doi.org/10.1038/s41563-024-01846-8), Nature Materials *(July 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Graph dataset of DNA origami designs (labelled),<br>Augmented unlabelled designs (automatically generated variants),<br>Collected designs (Methods section),<br>Supramolecular assemblies (hierarchical graphs) |
| **Models** | Graph Neural Network,<br>Gated Recurrent Unit,<br>Transformer,<br>Multi-Head Attention,<br>Ensemble Learning,<br>Residual Network (as residual blocks) |
| **Tasks** | Regression,<br>Structured Prediction,<br>Graph Generation,<br>Optimization,<br>Data Augmentation |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Ensemble Learning,<br>Data Augmentation,<br>Gradient Descent,<br>Backpropagation,<br>Evolutionary Learning,<br>Meta-Learning |
| **Performance Highlights** | RMSD_mean: 5.56 nm,<br>Orientation_Score_mean: 0.92,<br>RMSD_mean: 6.11 nm,<br>Orientation_Score_mean: 0.92,<br>RMSD_mean: 7.50 nm,<br>Orientation_Score_mean: 0.88,<br>RMSD_mean: 10.10 nm,<br>Orientation_Score_mean: 0.73,<br>RMSD_mean: 3.31 nm,<br>Orientation_Score_mean: 0.95,<br>Inference_time_mean: 0.98 s,<br>Total_energy_ratio_initial_to_GT: 182.3 -> predicted 4.1,<br>Computed_radii_vs_experiment_tetrahedron: 131 nm (pred) vs 125 nm (exp),<br>Computed_radii_vs_experiment_hexahedron: 151 nm (pred) vs 150 nm (exp),<br>Computed_radii_vs_experiment_dodecahedron: 217 nm (pred) vs 210 nm (exp),<br>Predicted_twist_angle: 3.5° (pred) vs ~4.0° (exp),<br>Computation_time_max: up to 66 h (CPU),<br>Number_of_predictions_for_optimization: 5,000 to 100,000 predictions per optimization,<br>Optimization_time_range: 7.8 min to 10.6 h (using pretrained DGNN),<br>SNUPI_alternative_time_estimate: would take hundreds to tens of thousands of hours for same optimization |
| **Application Domains** | DNA origami / structural DNA nanotechnology,<br>Molecular architecture design,<br>Supramolecular assembly analysis,<br>Inverse design and automated design optimization of nanoscale DNA structures,<br>Rapid virtual prototyping of DNA nanostructures |

---


### [162. From bulk effective mass to 2D carrier mobility accurate prediction via adversarial transfer learning](https://doi.org/10.1038/s41467-024-49686-z), Nature Communications *(June 25, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Collected 2D carrier mobility dataset (training/testing),<br>Bulk effective mass data (source-domain properties),<br>C2DB + 2DMatPedia (prediction / screening set),<br>DFT validation data (effective-mass approximation and deformation potential approximation) |
| **Models** | Multi-Layer Perceptron,<br>XGBoost |
| **Tasks** | Regression,<br>Regression,<br>Dimensionality Reduction,<br>Feature Extraction,<br>Ranking,<br>Feature Selection |
| **Learning Methods** | Transfer Learning,<br>Adversarial Training,<br>Domain Adaptation,<br>Backpropagation,<br>Supervised Learning,<br>Ensemble Learning,<br>Boosting,<br>Representation Learning,<br>Pre-training |
| **Performance Highlights** | R2_electron: 0.88,<br>R2_hole: 0.90,<br>MAE_electron: 0.19,<br>MAE_hole: 0.19,<br>R2_electron: 0.89,<br>MAE_electron: 0.11,<br>R2_hole: 0.89,<br>MAE_hole: 0.13,<br>R2_electron: 0.95,<br>MAE_electron: 0.12,<br>R2_hole: 0.89,<br>MAE_hole: 0.28,<br>R2_overall: >0.82,<br>MAE_overall: <0.22 |
| **Application Domains** | Materials discovery,<br>2D materials (semiconductor) design,<br>Electronic materials and device engineering (carrier mobility prediction for semiconductors / transistor scaling),<br>High-throughput screening and materials screening,<br>Catalysis and photovoltaic materials (potential applications of screened materials) |

---


### [161. LLMatDesign: Autonomous Materials Discovery with Large Language Models](https://doi.org/10.48550/arXiv.2406.13163), Preprint *(June 19, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project (structures used to curate datasets and starting materials),<br>mpgap (MatBench),<br>mpform (MatBench),<br>MLFF training dataset (curated),<br>Experimental runs: 10 starting materials (random selection) |
| **Models** | GPT,<br>Transformer,<br>Attention Mechanism |
| **Tasks** | Regression,<br>Optimization,<br>Language Modeling,<br>Text Generation,<br>Decision Making,<br>Data Generation |
| **Learning Methods** | Zero-Shot Learning,<br>Pre-training,<br>Supervised Learning,<br>Prompt Learning |
| **Performance Highlights** | average_number_of_modifications_to_reach_band_gap_target: 10.8,<br>average_final_band_gap_eV: 1.39,<br>average_formation_energy_eV_per_atom (ML surrogate runs): -1.97,<br>minimum_formation_energy_eV_per_atom (ML surrogate runs): -2.72,<br>average_number_of_modifications_with_history: 13.7,<br>average_number_of_modifications_historyless: 14.8,<br>MLFF_training_dataset_size: 187687,<br>MLFF_training_epochs: 400,<br>MLPP_band_gap_dataset_size: 106113,<br>MLPP_formation_energy_dataset_size: 132752,<br>MLPP_training_epochs_each: 200,<br>average_number_of_modifications_GPT-4o_Refined: 8.69,<br>average_number_of_modifications_Persona: 9.11,<br>average_number_of_modifications_history_baseline: 10.8,<br>average_number_of_modifications_with_history_without_self-reflection: 23.4,<br>average_number_of_modifications_with_history_with_self-reflection: 10.8,<br>average_number_of_modifications_historyless: 26.6,<br>random_baseline_average_number_of_modifications: 27.4,<br>random_baseline_average_final_band_gap_eV: 1.06,<br>DFT_validation_average_formation_energy_per_atom_GPT-4o_with_history: -2.31,<br>DFT_validation_average_formation_energy_per_atom_random: -1.51,<br>DFT_job_success_rate_GPT-4o_with_history_percent: 73.3,<br>DFT_job_success_rate_random_percent: 40.0,<br>constraint_do_not_use_Ba_or_Ca_percent_compliant: 100,<br>constraint_do_not_modify_Sr_percent_compliant: 100,<br>constraint_do_not_have_more_than_4_distinct_elements_percent_compliant: 99.02 |
| **Application Domains** | Materials Science,<br>Computational Chemistry,<br>Materials Discovery / Inorganic Crystal Design,<br>Photovoltaics (band gap target selection example),<br>Autonomous Laboratories / Self-driving labs,<br>Scientific Decision Making (LLM-assisted) |

---


### [160. Deep learning probability flows and entropy production rates in active matter](https://doi.org/10.1073/pnas.2318106121), Proceedings of the National Academy of Sciences *(June 18, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | N=2 illustrative system (torus),<br>N=64 swimmers in a harmonic trap,<br>N=4,096 MIPS training dataset (packing fraction φ = 0.5),<br>Transfer / test datasets: N=8,192; N=16,384; N=32,768 (various packing fractions φ=0.01..0.9),<br>Datasets for training/optimization (general) |
| **Models** | Transformer,<br>Multi-Layer Perceptron,<br>Attention Mechanism,<br>Denoising Diffusion Probabilistic Model |
| **Tasks** | Distribution Estimation,<br>Feature Extraction,<br>Representation Learning |
| **Learning Methods** | Unsupervised Learning,<br>Self-Supervised Learning,<br>Transfer Learning,<br>Stochastic Gradient Descent,<br>Mini-Batch Learning,<br>Online Learning,<br>Representation Learning |
| **Performance Highlights** | generalization_system_size: predictive transfer from N=4,096 training to N up to 32,768 without retraining,<br>packing_fraction_transfer_range: predictions consistent for packing fraction φ from 0.01 to 0.9 (evaluated on N=8,192),<br>qualitative_accuracy_checks: convergence verified via FPE residual and stationarity condition Eq. 13; authors report 'high accuracy' though no numeric values provided |
| **Application Domains** | active matter,<br>statistical mechanics,<br>stochastic thermodynamics,<br>scientific computing / computational physics,<br>many-body nonequilibrium systems |

---


### [159. Generative learning facilitated discovery of high-entropy ceramic dielectrics for capacitive energy storage](https://doi.org/10.1038/s41467-024-49170-8), Nature Communications *(June 10, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | 77 sets of experimental results (initial dataset),<br>Generated candidate dataset (GM output),<br>Phase-field simulation results (P-E loops and energy density vs Sconfig),<br>Augmented compositional search space (sampling discretization) |
| **Models** | Encoder-Decoder,<br>Multi-Layer Perceptron,<br>LightGBM,<br>Gaussian Mixture Model,<br>Encoder-Decoder |
| **Tasks** | Generative Learning,<br>Unsupervised Learning,<br>Classification,<br>Binary Classification,<br>Regression,<br>Data Augmentation,<br>Dimensionality Reduction,<br>Representation Learning,<br>Sampling / Monte Carlo Learning,<br>Optimization,<br>Experimental Design,<br>Feature Extraction |
| **Learning Methods** | Generative Learning,<br>Unsupervised Learning,<br>Supervised Learning,<br>Monte Carlo Learning,<br>Active Learning,<br>Representation Learning,<br>Dimensionality Reduction |
| **Performance Highlights** | generated_candidates: 2144,<br>predicted_Ue_threshold: >65 J cm−3,<br>candidates_sampled: 2144 (from latent space region),<br>top_selected_for_experiment: 5 compositions (C-1...C-5),<br>generated_candidates_with_predicted_Ue>65: 2144 (from regression/classifier pipeline),<br>downstream_experimental_success: 5 targeted experiments selected; best experimental Ue = 156 J cm−3 (C-3),<br>use_case: filtering generated compositions to a candidate pool (Ue > 65 J cm−3 predicted),<br>numeric_accuracy: not reported,<br>visualized_components: latent z projected by PCA (Fig. 2b),<br>observations: generated candidates (purple) cluster in latent space; five chosen compositions located in middle dense region |
| **Application Domains** | Materials Science,<br>Dielectric Materials / Dielectric Capacitors,<br>High-Entropy Ceramics,<br>Capacitive Energy Storage,<br>Thin Film Ceramics |

---


### [158. Machine learning-guided realization of full-color high-quantum-yield carbon quantum dots](https://doi.org/10.1038/s41467-024-49172-6), Nature Communications *(June 06, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | CQD synthesis dataset (initial + augmented) |
| **Models** | XGBoost,<br>Gradient Boosting Tree,<br>Decision Tree |
| **Tasks** | Regression,<br>Optimization,<br>Experimental Design,<br>Hyperparameter Optimization,<br>Feature Selection,<br>Dimensionality Reduction |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Active Learning |
| **Performance Highlights** | MSE_PLQY_initial: 0.45,<br>MSE_PLQY_after_4_iterations: ≈0.15,<br>MSE_PLQY_stabilized: ≈0.1,<br>MSE_PL_wavelength: <0.1 (throughout iterations),<br>Objective_utility_improvement: from 39.27% to 75.44 (reported improvement range),<br>PLQY_all_colors: >60% (peak PLQY for all seven colors exceeds 60%),<br>Notable_color_PLQYs: cyan 94% (max), some colors near 100% (cyan, green, orange); several colors approach 70% (purple, blue, red),<br>Iterations_to_meet_goal: 20 iterations (40 guided experiments) to achieve PLQY >50% across all seven colors,<br>Total_experiments_for_final_result: 63 experiments,<br>General_model_note: Used as the algorithmic family via XGBoost; specific performance reported under XGBoost entries |
| **Application Domains** | Materials Science,<br>Nanochemistry,<br>Luminescent materials / Optoelectronics,<br>LEDs,<br>Bioimaging / Life medicine,<br>Solar cells |

---


### [157. Closed-Loop Multi-Objective Optimization for Cu–Sb–S Photo-Electrocatalytic Materials’ Discovery](https://doi.org/10.1002/adma.202304269), Advanced Materials *(June 04, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Cu–Sb–S HTE experimental dataset (this work),<br>Initial sampling prior (Latin hypercube sampling runs),<br>Materials Project phase diagram (Cu–Sb–S subspaces) |
| **Models** | Gaussian Process |
| **Tasks** | Regression,<br>Optimization,<br>Feature Selection,<br>Multi-objective Classification |
| **Learning Methods** | Active Learning,<br>Batch Learning |
| **Performance Highlights** | RMSE_Y: 0.05,<br>RMSE_bandgap: 0.19,<br>RMSE_Cu1+/Cu_ratio: 0.17,<br>GPRU_RMSE: high (not numerical; model uncertainty remained high for uniformity),<br>R2_improvements: R2 scores improved over iterations (not all numerical values specified; improvements noted from iterations 2–4 and 6–8),<br>photocurrent_optimum: -186 μA cm^-2 at 0 V vs RHE,<br>photocurrent_baseline: -86 μA cm^-2 (batch 1),<br>relative_improvement: 2.3x |
| **Application Domains** | photo-electrochemical water splitting (photo-electrocatalysis / photocathode discovery),<br>materials discovery,<br>high-throughput experimentation (automated synthesis and characterization),<br>closed-loop autonomous experimentation |

---


### [156. ChatMOF: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models](https://doi.org/10.1038/s41467-024-48998-4), Nature Communications *(June 03, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | CoREMOF (Computation-ready experimental MOFs),<br>QMOF,<br>MOFkey,<br>DigiMOF,<br>hMOF / hypothetical MOFs (pre-training corpus),<br>Fine-tuned_MOFTransformer_model (released),<br>Database_for_ChatMOF (hMOF used in genetic algorithm) |
| **Models** | GPT,<br>Transformer,<br>Generative Adversarial Network,<br>Diffusion Model,<br>Variational Autoencoder |
| **Tasks** | Information Retrieval,<br>Regression,<br>Graph Generation,<br>Optimization,<br>Ranking,<br>Data Generation |
| **Learning Methods** | Pre-training,<br>Fine-Tuning,<br>Transfer Learning,<br>Prompt Learning,<br>Few-Shot Learning,<br>Zero-Shot Learning,<br>Supervised Learning,<br>Evolutionary Learning,<br>Reinforcement Learning |
| **Performance Highlights** | accuracy: 96.9%,<br>token_limit_exceeded_counts: 4/100 (search task),<br>accuracy: 95.7%,<br>token_limit_exceeded_counts: 6/100 (prediction task),<br>accuracy: 87.5%,<br>token_limit_exceeded_counts: 2/10 (generation task),<br>parent-child_overlap_percentage: ~30% overlap (GPT-4 generated children vs parents),<br>example_predicted_accessible_surface_area: 6411.28 m^2/g (predicted),<br>calculated_accessible_surface_area_after_optimization: 7647.62 m^2/g,<br>example_predicted_H2_uptake: 499.998 cm^3/cm^3 (predicted),<br>calculated_H2_uptake_after_optimization: 495.823 cm^3/cm^3,<br>example_predicted_hydrogen_diffusivity: 0.0030176841738998412 cm^2/s (BAZGAM_clean),<br>search_accuracy: 95% (GPT-3.5-turbo, excluding token-limit instances),<br>prediction_accuracy: 91% (GPT-3.5-turbo, excluding token-limit instances),<br>generation_accuracy: 77.8% (GPT-3.5-turbo, excluding token-limit instances),<br>note_on_16k_variant: GPT-3.5-turbo-16k (higher max tokens) did not significantly reduce token-limit errors due to suboptimal code generation,<br>example_generation_convergence: Accessible surface area population average increased from ~3748 m^2/g (initial) to 5554 m^2/g by the 3rd generation,<br>example_target_match: H2 uptake target 500 cm^3/cm^3 achieved: predicted 499.998 cm^3/cm^3, calculated after optimization 495.823 cm^3/cm^3,<br>ranking: Generated rtl+N535+N234 structure's calculated ASA ranks third-highest in CoREMOF |
| **Application Domains** | Materials science,<br>Metal–organic frameworks (MOFs) research,<br>Computational materials discovery,<br>Chemistry (porous materials / gas adsorption),<br>Materials informatics / AI for science |

---


### [154. Three million images and morphological profiles of cells treated with matched chemical and genetic perturbations](https://doi.org/10.1038/s41592-024-02241-6), Nature Methods *(June 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | CPJUMP1,<br>Drug Repurposing Hub (subset used to select compounds),<br>RxRx3 (mentioned for comparison),<br>JUMP Cell Painting dataset (larger consortium dataset, referenced) |
| **Models** | Convolutional Neural Network |
| **Tasks** | Information Retrieval,<br>Ranking,<br>Anomaly Detection,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Extraction,<br>Image Generation,<br>Style Transfer |
| **Learning Methods** | Supervised Learning,<br>Representation Learning,<br>Multi-View Learning,<br>Transfer Learning,<br>Domain Adaptation |
| **Performance Highlights** | training_samples: 7,077 single cells,<br>input_feature_dimension: 3,019 features (reduced non-redundant CellProfiler features),<br>output_image_size: 128 x 128 x 5 channels,<br>evaluation_metrics: Average Precision (AP), mean Average Precision (mAP), fraction retrieved (FR = fraction of perturbations with q < 0.05),<br>statistical_testing: Permutation testing (100,000 shuffles) to assign P values to AP, Benjamini–Hochberg FDR to obtain q-values; similarity metric: cosine similarity (or absolute cosine similarity when both positive and negative correlations considered matches),<br>metric: Average Precision (AP) per perturbation; mean Average Precision (mAP) summarized per task; fraction retrieved (FR = fraction perturbations with q < 0.05),<br>stat_test: Permutation test (100,000 shuffles) to obtain P for each AP, then Benjamini–Hochberg FDR to compute q-values,<br>metric: Cosine similarity distributions and counts beyond percentile thresholds vs null; Fisher’s exact test for directionality,<br>reported_counts: n = 3,728 biologically independent ORF and CRISPR reagents (for ORF vs CRISPR comparison); n = 1,864 independent pairs of compounds and genetic perturbations for compound–genetic comparisons |
| **Application Domains** | Cell biology (cell morphology profiling),<br>Drug discovery and chemical biology (compound mechanism-of-action identification, virtual screening),<br>Functional genomics (gene function via genetic perturbations),<br>Bioimage analysis / high-content screening,<br>Representation learning and machine learning research (benchmarking new methods on biological imaging data) |

---


### [153. Machine intelligence-accelerated discovery of all-natural plastic substitutes](https://doi.org/10.1038/s41565-024-01635-z), Nature Nanotechnology *(June 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Initial robot-prepared dataset (boundary definition),<br>Active-learning experimental dataset (stagewise fabricated samples),<br>Combined real + virtual training dataset (after data augmentation),<br>SVM classifier test set,<br>Model expansion experiments (chitosan incorporation),<br>Model-predicted/combinatorial design space outputs,<br>Public data/code repository |
| **Models** | Support Vector Machine,<br>Multi-Layer Perceptron,<br>Linear Model,<br>Decision Tree,<br>Gradient Boosting Tree,<br>Random Forest |
| **Tasks** | Multi-class Classification,<br>Regression,<br>Optimization,<br>Clustering,<br>Data Augmentation,<br>Model Interpretation (not a listed ML task; included because performed) |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Ensemble Learning,<br>Data Augmentation,<br>Cross-Validation (fivefold) |
| **Performance Highlights** | accuracy: 94.3%,<br>A-grade success_rate: >94%,<br>mean_relative_error_(MRE): ≈17% (champion model after 14 active learning loops),<br>ANN_without_data_augmentation_MRE: >55%,<br>model_expansion_MRE_change: from 107% to 21% after three expansion loops,<br>Not_directly_applicable: committee prediction variance used in acquisition function (A_score = L_hat × sigma_hat),<br>MRE_relative_performance: All had higher MREs than ANN (exact values not reported in main text),<br>context_note: Compared to ANN, other models yielded larger MREs (Fig. 2e summary),<br>predicted_and_experimental_strengths: Model-suggested MMT-rich sample: average σu = 114 ± 18 MPa (n=5); CNF-rich sample: average σu = 98 ± 7 MPa (n=5); after two-step treatments: improved σu to 468.6 ± 52.6 MPa (max 520.7 MPa) and 463.0 ± 35.7 MPa (max 521.0 MPa),<br>context_note: Experimental validation of model-suggested high-strength compositions |
| **Application Domains** | Materials science / nanocomposite materials,<br>Biodegradable plastics / sustainable materials,<br>Polymer engineering / packaging materials,<br>Structural materials (high-strength bio-based materials),<br>Robotics-assisted experimentation / autonomous materials discovery,<br>Materials informatics / ML-driven materials design |

---


### [152. Machine learning-aided generative molecular design](https://doi.org/10.1038/s42256-024-00843-5), Nature Machine Intelligence *(June 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | ChEMBL,<br>GuacaMol,<br>MOSES,<br>Reaxys (reaction corpus used for SCScore),<br>Ultralarge make-on-demand virtual libraries (examples: 11 billion synthon-based, VirtualFlow 69 billion),<br>Chemistry42 (proprietary platform/data),<br>AlphaFold-predicted protein structures (used as structural input),<br>Benchmarks and oracle collections (sample-efficiency benchmark referenced) |
| **Models** | Variational Autoencoder,<br>Generative Adversarial Network,<br>Normalizing Flow,<br>Diffusion Model,<br>Recurrent Neural Network,<br>Long Short-Term Memory,<br>Gated Recurrent Unit,<br>Transformer,<br>Graph Neural Network,<br>Seq2Seq,<br>Gradient Boosting Tree,<br>Gaussian Process,<br>Autoencoder,<br>Variational Autoencoder,<br>Denoising Diffusion Probabilistic Model,<br>Genetic Algorithm,<br>Reinforcement Learning,<br>Markov chain Monte Carlo |
| **Tasks** | Graph Generation,<br>Sequence-to-Sequence,<br>Synthetic Data Generation,<br>Optimization,<br>Regression,<br>Classification,<br>Graph Classification,<br>Data Generation |
| **Learning Methods** | Reinforcement Learning,<br>Fine-Tuning,<br>Pre-training,<br>Bayesian Optimization,<br>Evolutionary Learning,<br>Monte Carlo Learning,<br>Gradient Descent,<br>Active Learning,<br>Contrastive Learning,<br>Transfer Learning,<br>Few-Shot Learning,<br>Representation Learning |
| **Performance Highlights** | hit_rate: 4/5 (80%),<br>outcome: nM agonist (RXR) reported,<br>hit_rate: 2/4 (50%),<br>outcome: μM agonist (RXR),<br>hit_rate: 7/7 (100%),<br>outcome: nM inhibitor (JAK1),<br>hit_rate: 2/6 (33%),<br>outcome: nM inhibitor (Nurr1γ),<br>hit_rate: 9/43 (21%),<br>outcome: nM inhibitor (CDK8),<br>hit_rate: 0/1 (0%),<br>outcome: μM inhibitor (Bacteria),<br>hit_rate: 2/2 (100%),<br>outcome: nM inhibitor (DDR1),<br>hit_rate: 1/1 (100%),<br>outcome: nM inhibitor (TBK1),<br>hit_rate: 17/23 (74%),<br>outcome: cnM inhibitor (CDK2) reported,<br>hit_rate: 2/2 (100%),<br>outcome: μM agonist (PPARγ),<br>outcome: design, synthesis and experimental validation of DDR1 inhibitor within 21 days (nanomolar or notable potency reported in ref. 19),<br>hit_rate: 4/6 (67%),<br>outcome: nM inhibitor (DDR1) reported in ref. 108 and Table 4,<br>hit_rate: 7/7 (100%),<br>outcome: nM inhibitor (CDK2) reported in 2024 (ref.173) - lead optimization |
| **Application Domains** | Drug discovery,<br>Small-molecule design,<br>Structure-based drug design (SBDD),<br>Ligand-based drug design (LBDD),<br>Medicinal chemistry,<br>Pharmacology,<br>Antibiotic discovery,<br>Protein–ligand binding and docking,<br>Automated synthesis / self-driving labs,<br>Materials design (suggested potential extensions: polysaccharides, proteins, nucleic acids, crystals, polymers) |

---


### [151. High-Entropy Photothermal Materials](https://doi.org/10.1002/adma.202400920), Advanced Materials *(June 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | co-sputtered material libraries (combinatorial material libraries),<br>comprehensive databases of material properties (unnamed) |
| **Models** | _None_ |
| **Tasks** | Experimental Design |
| **Learning Methods** | Supervised Learning,<br>Representation Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | Solar water evaporation,<br>Personal thermal management (warming textiles),<br>Solar thermoelectric generation,<br>Photocatalysis (CO2 reduction, water splitting),<br>Photothermal catalysis (biomass conversion, CO production),<br>Biomedical applications (photothermal therapy, antibacterial/antibiofilm),<br>Broadband solar absorbers,<br>Spectrally selective absorbers |

---


### [150. Diffusion-based deep learning method for augmenting ultrastructural imaging and volume electron microscopy](https://doi.org/10.1038/s41467-024-49125-z), Nature Communications *(June 01, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | In-house mouse brain cortex dataset (denoising),<br>In-house mouse brain cortex dataset (super-resolution),<br>In-house mouse liver, heart, bone marrow, cultured HeLa cell images,<br>OpenOrganelle mouse liver dataset (jrc_mus-liver),<br>OpenOrganelle mouse kidney dataset (jrc_mus-kidney),<br>OpenOrganelle T-cell dataset (jrc_ctl-id8-2),<br>EPFL mouse brain dataset,<br>MICrONS multi-area dataset,<br>FANC dataset,<br>MANC dataset (downsampled) |
| **Models** | Denoising Diffusion Probabilistic Model,<br>U-Net |
| **Tasks** | Image Denoising,<br>Image Super-Resolution,<br>Image-to-Image Translation,<br>Image Generation,<br>Semantic Segmentation |
| **Learning Methods** | Supervised Learning,<br>Self-Supervised Learning,<br>Transfer Learning,<br>Fine-Tuning,<br>Ensemble Learning,<br>Pre-training |
| **Performance Highlights** | LPIPS: EMDiffuse-n outperformed baselines (lower LPIPS values across test set; test points n=960),<br>FSIM: EMDiffuse-n outperformed baselines (higher FSIM),<br>Resolution Ratio: EMDiffuse-n demonstrated superior resolution ratio versus baselines,<br>Uncertainty threshold: 0.12 (predictions with uncertainty below 0.12 considered reliable),<br>Acquisition speedup: up to 18× reduction in acquisition time reported (denoising restoring high-quality images from noisy images),<br>LPIPS: EMDiffuse-r outperformed CARE, PSSR, and RCAN across LPIPS,<br>FSIM: EMDiffuse-r outperformed baselines (higher FSIM),<br>Resolution Ratio: EMDiffuse-r outperformed baselines (higher resolution ratio),<br>Fourier Ring Correlation: Indicates EMDiffuse-r captures high-frequency details present in ground truth,<br>Uncertainty threshold: 0.12 (predictions with uncertainty below threshold considered reliable),<br>Acquisition speedup: 36× increase in EM imaging speed (by super-resolving 6 nm pixel to 3 nm pixel),<br>LPIPS: vEMDiffuse-i produced lower LPIPS than 3D-SRU-Net and ITK-cubic interpolation (quantitative improvement reported),<br>FSIM: vEMDiffuse-i produced higher FSIM than 3D-SRU-Net and ITK-cubic interpolation,<br>Resolution Ratio: Improved versus baselines (higher resolution ratio),<br>IoU (organelle segmentation): vEMDiffuse-i generated volume achieved IoU scores similar to ground truth isotropic volume; for some experiments ER and mitochondria IoU > 0.9 (up to axial resolution = 64 nm),<br>Uncertainty: Predictions had uncertainty values below the uncertainty threshold (0.12) indicating reliability,<br>LPIPS/FSIM/Resolution Ratio: vEMDiffuse-a outperformed CARE and ITK-cubic interpolation on LPIPS, FSIM, and resolution ratio in downsampled OpenOrganelle experiments (Supplementary Fig. 23c).,<br>IoU (organelle segmentation): Reported example IoU values in figure captions: 0.60, 0.98, 0.62, 0.94 (these illustrate anisotropic vs vEMDiffuse-a vs isotropic comparisons across datasets/organelle types); vEMDiffuse-a improved IoU and 3D reconstruction fidelity.,<br>Uncertainty: Generated volumes had uncertainty values below threshold indicating reliability for MICrONS and FANC demonstrations.,<br>IoU: Segmentation models trained on isotropic OpenOrganelle liver volume applied to interpolated, anisotropic, vEMDiffuse-i generated, and ground truth isotropic volumes; vEMDiffuse-i generated volume achieved IoU similar to isotropic volume (Fig. 3e). |
| **Application Domains** | Electron microscopy (EM) ultrastructural imaging,<br>Volume electron microscopy (vEM) / Connectomics,<br>Cell biology (organelle-level 3D ultrastructural analysis),<br>Large-scale tissue array tomography datasets (e.g., MICrONS, FANC) |

---


### [147. MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures and Pressures](https://doi.org/10.48550/arXiv.2405.04967), Preprint *(May 10, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | MatterSim training dataset (~17M structures),<br>MPF-TP,<br>Random-TP,<br>MPF-Alkali-TP,<br>MPF2021,<br>MPtrj (variants: MPTrj-random-1k, MPTrj-highest-stress-1k),<br>Alexandria (and Alexandria-1k, Alexandria-MP-ICSD),<br>PhononDB (MDR phonon database),<br>MatBench (MatBench Discovery and other MatBench tasks),<br>RSS-generated dataset (this work) / Random Structure Search outputs,<br>Experimental Gibbs free energy references (FactSage / SISSO-derived analytical forms) |
| **Models** | Graph Neural Network,<br>Message Passing Neural Network,<br>Transformer,<br>Multi-Layer Perceptron |
| **Tasks** | Regression,<br>Binary Classification,<br>Ranking |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Ensemble Learning,<br>Pre-training,<br>Fine-Tuning,<br>Zero-Shot Learning,<br>Uncertainty Quantification,<br>End-to-End Learning,<br>Knowledge Distillation |
| **Performance Highlights** | MAE_energy_on_MPF-TP: 36 meV/atom,<br>chemical_accuracy_equivalent: 43 meV/atom,<br>MP_Gap_MAE: 0.1290 eV,<br>logGVRH_MAE: 0.0608 (log GPa),<br>logKVRH_MAE: 0.0488 (log GPa),<br>Dielectric_MAE: 0.2516 (unitless),<br>Phonons_MAE: 26.022 cm^-1,<br>jdft2d_MAE: 32.762 meV/atom,<br>MAE_max_phonon_frequency: 0.87 THz,<br>MAE_average_phonon_frequency: 0.76 THz,<br>MAE_DOS: 0.64 (integrated MAE of DOS; units as in SI),<br>Li2B12H12_max_force_error_reduction: Model reproduces similar accuracy with only 15% of data compared to training from scratch,<br>uncertainty_reduction: Maximum error notably reduced after active learning (no single scalar provided),<br>data_efficiency_gain: finetune-30 (30 configurations) achieves similar performance to scratch-900 (900 configurations) — 97% reduction in data,<br>self_diffusion_D_finetune_corrected: 1.862×10^-5 cm^2/s (error below 20% vs experiment 2.3–2.4×10^-5 cm^2/s),<br>MatBench_Discovery_F1: 0.83,<br>formation_energy_MAE: 0.026 eV/atom (reported ~0.03 eV/atom in Table S2),<br>MD_success_rate_across_families: >90% success rate across tested material families,<br>snapshot_energy_error: <50 meV/atom (mean error of energy lower than 50 meV/atom for example MD snapshots),<br>MAE_vs_PBE_QHA_free_energy_up_to_1000K: <10 meV/atom (sub-10 meV/atom),<br>MAE_vs_experiment_over_200_materials: 15 meV/atom (0–1000K range comparison to experimental in SI),<br>MgO_B1-B2_transition_pressure_at_300K: 584 GPa predicted (compared to experimental 429–562 GPa and first-principles 520 GPa) |
| **Application Domains** | Materials discovery / computational materials science,<br>Atomistic simulations and molecular dynamics,<br>High-pressure / high-temperature materials (earth mantle conditions, high-pressure synthesis),<br>Inorganic solids (bulk crystals, MOFs, 2D materials, molecular crystals, polymers, interfaces, surfaces),<br>Thermodynamics (Gibbs free energy and phase diagram prediction),<br>Electronic and mechanical property prediction (band gaps, elastic moduli, dielectric constants),<br>Liquid water simulation (ab initio level properties and dynamics),<br>High-throughput screening and random structure search (RSS) |

---


### [146. Robotic synthesis decoded through phase diagram mastery](https://doi.org/10.1038/s44160-024-00500-0), Nature Synthesis *(May 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | 35 quaternary oxides (containing 27 unique elements) |
| **Models** | _None_ |
| **Tasks** | Experimental Design,<br>Ranking,<br>Optimization,<br>Data Generation,<br>Decision Making |
| **Learning Methods** | _None_ |
| **Performance Highlights** | success_rate: 32/35 (91%),<br>similar_or_greater_purity_count: 31 compounds,<br>unique_successes_by_predicted_precursors: 6 targets,<br>examples_of_metastable_products_and_energies_meV_per_atom: LiNbWO6 (10 meV/atom), LiZnBO3 (8 meV/atom), KTiNbO5 (1 meV/atom), Li3Y2(BO3)3 (39 meV/atom),<br>ranking_criterion: lowest point on the convex hull prioritized; high inverse-hull energy favoured,<br>failure_modes_count: 3 targets not synthesized by either predicted or conventional precursors |
| **Application Domains** | Materials science,<br>Inorganic oxide synthesis,<br>Battery cathodes and solid-state electrolytes,<br>Automated/high-throughput experimentation,<br>Self-driving labs (closed-loop materials discovery) |

---


### [145. Navigating phase diagram complexity to guide robotic inorganic materials synthesis](https://doi.org/10.1038/s44160-024-00502-y), Nature Synthesis *(May 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project phases and formation energies (used to construct convex hulls),<br>ASTRAL robotic laboratory experimental dataset,<br>Text-mined database of traditional solid-state synthesis recipes (Kononova et al.),<br>Candidate reaction list (Supplementary Data 1),<br>Shared experimental data on figshare |
| **Models** | Graph Neural Network,<br>None (physics-based convex hull / DFT-driven precursor selection — not in provided model list) |
| **Tasks** | Experimental Design,<br>Optimization,<br>Recommendation,<br>Hyperparameter Optimization |
| **Learning Methods** | Active Learning,<br>Supervised Learning |
| **Performance Highlights** | total_experiments: 224,<br>targets_tested: 35,<br>predicted_precursors_success_rate: 32/35 (91%),<br>targets_with_>=20%_higher_phase_purity_using_predicted_precursors: 15,<br>targets_only_synthesized_by_predicted_precursors: 6,<br>targets_with_similar_yields: 16,<br>targets_where_traditional_better: 4,<br>example_reaction_energy_LiBaBO3_overall: -336 meV per atom,<br>LiBO2 + BaO reaction_energy: -192 meV per atom,<br>LiBaBO3_inverse_hull_energy: -153 meV per atom,<br>observed_failure_energy_region: ΔE_reaction > -70 meV per atom; inverse hull energy > -50 meV per atom (region where predicted precursors often failed or were less certain) |
| **Application Domains** | Inorganic materials synthesis,<br>Solid-state synthesis of multicomponent oxides (oxides, phosphates, borates),<br>Intercalation battery cathode materials,<br>Solid-state electrolytes,<br>Robotic/automated laboratory experimentation |

---


### [144. Designing semiconductor materials and devices in the post-Moore era by tackling computational challenges with data-driven strategies](https://doi.org/10.1038/s43588-024-00632-5), Nature Computational Science *(May 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Carolina materials database (hypothetical materials databases),<br>Merchant et al. expanded training dataset,<br>Schmidt et al. training dataset (stepwise transfer learning),<br>Public computational materials databases (generic / unspecified),<br>Multi-fidelity datasets (DFT, hybrid functionals, GW, experimental) |
| **Models** | Generative Adversarial Network,<br>Variational Autoencoder,<br>Diffusion Model,<br>Transformer,<br>Graph Neural Network,<br>Message Passing Neural Network,<br>Convolutional Neural Network,<br>Multi-Layer Perceptron,<br>XGBoost,<br>Support Vector Machine,<br>Naive Bayes,<br>Random Forest,<br>Gaussian Process |
| **Tasks** | Graph Generation,<br>Data Generation,<br>Regression,<br>Optimization,<br>Feature Selection,<br>Classification,<br>Anomaly Detection,<br>Hyperparameter Optimization |
| **Learning Methods** | Active Learning,<br>Transfer Learning,<br>Fine-Tuning,<br>Pre-training,<br>Reinforcement Learning,<br>Ensemble Learning,<br>Supervised Learning,<br>Unsupervised Learning |
| **Performance Highlights** | DFT_optimization_success_rate: 93.5%,<br>convex_hull_threshold_fraction: 75% below 0.1 eV per atom,<br>training_set_size_growth: from 1e4 to ~1e7 (dataset expansion reported),<br>explored_design_space_size: up to 1 billion compounds (as part of stepwise transfer learning),<br>performance_note: progressively enhanced chemical and structural diversity of training set,<br>speedup_BSE: 1 to 2 orders of magnitude (computational efficiency),<br>nanowire_length_applicability: predicted electrical conductivity for systems with lengths up to 17.5 nm |
| **Application Domains** | Semiconductor materials discovery / design,<br>Crystal structure prediction (CSP),<br>Device fabrication optimization (growth conditions, interface design),<br>Thermal transport prediction and thermal management,<br>Electrical transport prediction (carrier mobility, conductivity),<br>Optical properties and excited-state properties prediction (bandgaps, BSE/GW acceleration),<br>Autonomous / closed-loop experimental synthesis and optimization (perovskite solar cells example),<br>Semiconductor manufacturing testing (wafer defect detection, ML-guided design-for-test, lithography mask optimization),<br>Spintronic materials and superconductors (materials design applications) |

---


### [143. Universal chemical programming language for robotic synthesis repeatability](https://doi.org/10.1038/s44160-023-00473-6), Nature Synthesis *(April 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | χDL files (.xdl) and Chemputer graph files (.json),<br>HPLC / NMR / MS experimental data (spectra, chromatograms, conversions, yields),<br>Experimental result records for specific syntheses (yields, purities for compounds 3,4,5,7,8,9,13, etc.) |
| **Models** | _None_ |
| **Tasks** | Optimization,<br>Experimental Design,<br>Data Generation,<br>Automation (framed as Experimental Design / Optimization on platforms) |
| **Learning Methods** | Online Learning,<br>Active Learning,<br>Transfer Learning |
| **Performance Highlights** | yield_compound_8_after_workup_optimization: 71%,<br>yield_compound_8_counter-validated: 72%,<br>TIDA_protection_initial_canada: 59%,<br>TIDA_protection_scotland_final_validated: 59% (first attempt at Scotland), later 71% after workup optimization,<br>CDI_amide_coupling_Kinova_conversion: 83% conversion (without final purification),<br>CDI_amide_coupling_Chemputer_complete_conversion: complete conversion by endpoint HPLC,<br>CDI_amide_coupling_Chemputer_Scotland_yield: 93% yield,<br>CDI_amide_coupling_Chemputer_upscaled_benzylamine_yield: 74% yield,<br>compound_3_yield_original: 45%,<br>compound_4_yield_from_3_original: 82%,<br>compound_3_yield_optimized_chemputer: 88%,<br>compound_4_yield_from_3_optimized: 47% |
| **Application Domains** | Synthetic organic chemistry,<br>Automated chemical synthesis,<br>Laboratory robotics / laboratory automation,<br>Analytical chemistry (online HPLC, NMR, MS integration),<br>Digital chemistry / chemical informatics |

---


### [141. Crystal Structure Assignment for Unknown Compounds from X-ray Diffraction Patterns with Deep Learning](https://doi.org/10.1021/jacs.3c11852), Journal of the American Chemical Society *(March 27, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | ICSD top-100 structure types (simulated XRD patterns),<br>ICSD overall (metadata reported),<br>RRUFF experimental XRD patterns (test set),<br>ICSD top-101−110 structure types (used for OOD test) |
| **Models** | Convolutional Neural Network,<br>ResNet,<br>Ensemble (union of submodels) |
| **Tasks** | Multi-class Classification,<br>Classification,<br>Binary Classification,<br>Out-of-Distribution Learning,<br>Dimensionality Reduction,<br>Clustering |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Mini-Batch Learning,<br>Backpropagation,<br>Ensemble Learning,<br>Out-of-Distribution Learning |
| **Performance Highlights** | test_accuracy_on_same_dataset: 99.89%,<br>test_accuracy_on_top-100_structure_types: 80.0%,<br>test_accuracy_with_only_confidence_combination: 65.7% (combinatorial classification using confidence value alone),<br>experimental_accuracy_on_RRUFF: 81.3% (on 80 experimental patterns),<br>OOD_detection_accuracy_at_R0=0.6: 89.1%,<br>in-distribution_confidence_range: 0.9–1.0 (for RCNet tested on same dataset),<br>out-of-distribution_confidence_range: 0–0.1 (for RCNet tested on different subsets), but occasional C>0.9 leading to misclassification,<br>optimized_alpha_test_accuracy: 80.0% at α = 0.7,<br>ablation_effect_of_removing_RCNet#2: accuracy increase of ~17% for α=0; drop to ~4% for α=0.7 |
| **Application Domains** | Materials science,<br>Inorganic crystallography / crystal structure identification,<br>X-ray diffraction (XRD) pattern analysis,<br>High-throughput experimentation / autonomous (self-driving) laboratories,<br>Mineralogy (experimental validation using RRUFF) |

---


### [140. Machine-Learning Assisted Screening Proton Conducting Co/Fe based Oxide for the Air Electrode of Protonic Solid Oxide Cell](https://doi.org/10.1002/adfm.202309855), Advanced Functional Materials *(March 18, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | PAA database (proton absorption amount) - 792 samples, 29 features |
| **Models** | XGBoost,<br>Random Forest,<br>Decision Tree |
| **Tasks** | Regression |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Bagging,<br>Boosting |
| **Performance Highlights** | RMSE: 0.021,<br>MAE: 0.01,<br>R-squared: 0.901,<br>RMSE: 0.022,<br>MAE: 0.012,<br>R-squared: 0.892 |
| **Application Domains** | Proton-conducting solid oxide cells (P-SOCs),<br>Materials discovery and screening for air electrode materials,<br>Perovskite oxide materials (Co/Fe-based ABO3),<br>Electrochemistry: fuel cells and electrolysis (hydrogen production) |

---


### [139. Autonomous reaction Pareto-front mapping with a self-driving catalysis laboratory](https://doi.org/10.1038/s44286-024-00033-5), Nature Chemical Engineering *(March 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Fast-Cat in-house experimental dataset (autonomously generated reaction data) |
| **Models** | Feedforward Neural Network,<br>Gaussian Process |
| **Tasks** | Regression,<br>Optimization,<br>Hyperparameter Optimization,<br>Feature Selection |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Active Learning,<br>Batch Learning,<br>Hyperparameter Optimization |
| **Performance Highlights** | L1_experiments_to_pareto_front: 60,<br>L2-L6_experiments_per_ligand: 40,<br>resource_use_for_L1_campaign: less than 500 ml solvent, 2 mmol total ligand,<br>L1_SN_range_obtained: 0.37–0.85 (l/b 0.8–5.5) |
| **Application Domains** | Homogeneous catalysis,<br>Hydroformylation of olefins (1-octene),<br>Catalyst and ligand discovery/benchmarking,<br>Flow chemistry (gas–liquid segmented flow reactors),<br>Process development for fine/specialty chemicals,<br>Autonomous experimentation / self-driving laboratories |

---


### [137. Digital twins in medicine](https://doi.org/10.1038/s43588-024-00607-6), Nature Computational Science *(March 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Contrast-enhanced MRI cohort (Arevalo et al.),<br>MRI time-course datasets for glioblastoma (Swanson and colleagues),<br>Breast tumor imaging dataset (Wu et al.),<br>Study of cardiac sarcoidosis patients (Shade et al.),<br>Hypertrophic cardiomyopathy cohort (O'Hara et al.),<br>Archimedes diabetes model / UVA/Padova type 1 diabetes simulator datasets,<br>The Cancer Genome Atlas (TCGA),<br>Human Cell Atlas,<br>Electronic health record (EHR) datasets and wearable data (general references) |
| **Models** | Bayesian Network,<br>Multi-Layer Perceptron,<br>Feedforward Neural Network |
| **Tasks** | Binary Classification,<br>Survival Analysis,<br>Time Series Forecasting,<br>Regression,<br>Feature Extraction,<br>Image Segmentation,<br>Synthetic Data Generation,<br>Clustering,<br>Control |
| **Learning Methods** | Supervised Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | Cardiology (arrhythmia risk prediction, catheter ablation planning, atrial fibrillation management),<br>Oncology (tumor progression modeling, treatment planning, predictive oncology),<br>Critical care / ICU (sepsis modeling, scoring systems, decision support),<br>Endocrinology (type 1 diabetes closed-loop insulin delivery),<br>Infectious disease (immune-system-focused digital twins, SARS-CoV-2 response),<br>Pharmacology / Drug development (virtual clinical trials, pharmacometrics, virtual populations),<br>Preventive medicine / wearable-based monitoring |

---


### [135. A comprehensive transformer-based approach for high-accuracy gas adsorption predictions in metal-organic frameworks](https://doi.org/10.1038/s41467-024-46276-x), Nature Communications *(March 01, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | pre-training MOF/COF structures (collected + generated, total >631,000),<br>hMOF_MOFX_DB,<br>CoRE_MOFX_DB,<br>CoRE_MAP_DB (RASPA generated),<br>ToBaCCo.3.0 generated MOFs,<br>Other structure collections referenced (hMOF, CoRE MOF, CCDC, CoRE COF, GCOFs),<br>COF CH4 adsorption dataset (CoRE COFs at 300 K),<br>Other generated property data (Zeo++ outputs) |
| **Models** | Transformer,<br>Attention Mechanism,<br>Multi-Head Attention,<br>Feedforward Neural Network,<br>Graph Neural Network |
| **Tasks** | Regression,<br>Ranking,<br>Feature Extraction,<br>Dimensionality Reduction,<br>Clustering |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Supervised Learning,<br>Transfer Learning,<br>Representation Learning,<br>Multi-Task Learning |
| **Performance Highlights** | pre-training_representation_accuracy_R2: 0.98,<br>R2_hMOF_MOFX_DB: 0.98,<br>R2_CoRE_MOFX_DB: 0.92,<br>R2_CoRE_MAP_DB: 0.83,<br>R2_Kr_unknown: 0.85,<br>R2_Xe_unknown: 0.41,<br>R2_all_unknown_gases_min: 0.35 (paper states prediction accuracy above 0.35 for all unknown gases),<br>R2_CH4_in_COFs: 0.76,<br>R2_structural_features_hMOF: >0.99,<br>R2_CoRE_MAP_DB_without_pretraining: 0.70,<br>R2_CoRE_MAP_DB_with_pretraining: 0.83 |
| **Application Domains** | Materials science,<br>Nanoporous materials / metal-organic frameworks (MOFs),<br>Covalent-organic frameworks (COFs),<br>Gas separation and adsorption (industrial gas separation, carbon capture, noble gas purification),<br>High-throughput materials screening / computational materials discovery |

---


### [134. Automated synthesis of oxygen-producing catalysts from Martian meteorites by a robotic AI chemist](https://doi.org/10.1038/s44160-023-00424-1), Nature Synthesis *(March 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | DFT-simulated dataset of high-entropy hydroxides (29,902 compositions),<br>Robot-driven experimental dataset (243 experiments),<br>LIBS spectral dataset (elemental analysis),<br>Search space of candidate compositions |
| **Models** | Multi-Layer Perceptron,<br>Gaussian Process |
| **Tasks** | Regression,<br>Optimization,<br>Data Generation |
| **Learning Methods** | Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Transfer Learning,<br>Backpropagation |
| **Performance Highlights** | Pearson_r_for_ΔGOH*_prediction: 0.998,<br>Pearson_r_for_ΔGO*−OH*_prediction: 0.998,<br>Pearson_r_for_Δq_prediction: 0.996,<br>Pearson_r_for_predicted_vs_measured_overpotential: 0.963,<br>train_test_split: 80% train / 20% test,<br>trained_on: 243 experimental datasets (re-training) + simulated descriptors,<br>Model-guided_OPT_η10: 445.1 mV,<br>Improvement_vs_best_experimental: 37.1 mV lower than best pilot experiment (482.2 mV),<br>Exp-guided_OPT_η10: 467.4 mV,<br>Bayesian_iterations: 280 iterations |
| **Application Domains** | Materials science,<br>Electrocatalysis (oxygen evolution reaction, OER),<br>Automated chemical synthesis / robotics,<br>Planetary science / in-situ resource utilization for Mars,<br>Computational materials discovery (MD, DFT, ML integration) |

---


### [133. Data-Driven Design for Metamaterials and Multiscale Systems: A Review](https://doi.org/10.1002/adma.202305254), Advanced Materials *(February 22, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | J-CFID (CFID),<br>OQMD-8 (OQMD),<br>Orthotropic mechanical metamaterials dataset (freeform pixelated unit cells),<br>Large orthotropic dataset (Wang et al.),<br>TPMS dataset,<br>Multiclass lattice dataset,<br>MetaMine / NanoMine data repositories (meta-materials collection),<br>Pixel/voxelated freeform unit cell datasets (examples),<br>Photonic/metasurface datasets (class-based and freeform) |
| **Models** | Convolutional Neural Network,<br>Multi-Layer Perceptron,<br>Variational Autoencoder,<br>Autoencoder,<br>Generative Adversarial Network,<br>Conditional GAN,<br>Denoising Diffusion Probabilistic Model,<br>Gaussian Process,<br>Decision Tree,<br>Recurrent Neural Network,<br>Graph Neural Network,<br>Encoder-Decoder,<br>Autoencoder -> PCA / Latent-space visualization (Dimensionality Reduction) [model family],<br>Variational Autoencoder-coupled regressor (VAE + regressor),<br>Fourier Neural Operator (related operator learning mention),<br>Gaussian Mixture Models (in density-based inverse methods) |
| **Tasks** | Regression,<br>Classification,<br>Dimensionality Reduction,<br>Clustering,<br>Image Generation,<br>Synthetic Data Generation,<br>Optimization,<br>Decision Making,<br>Experimental Design,<br>Feature Extraction,<br>Data Augmentation |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Semi-Supervised Learning,<br>Reinforcement Learning,<br>Active Learning,<br>Representation Learning,<br>Dimensionality Reduction,<br>Adversarial Training,<br>End-to-End Learning,<br>Fine-Tuning,<br>Supervised + Generative (conditional generation) |
| **Performance Highlights** | example_training_size: can be effective with small datasets (example: trained on local patches to achieve field-prediction with only 10 design samples in one work),<br>latent_dim_examples: 16D latent representation (Wang et al.) used and visualized via PCA; VAE trained on large datasets (e.g., 240k),<br>qualitative: hPINNs achieved same objective value but produced simpler/smoother solutions with faster convergence (qualitative) compared to PDE-constrained adjoint-based optimization. |
| **Application Domains** | Metamaterials (general),<br>Mechanical metamaterials,<br>Optical / photonic metasurfaces,<br>Acoustic / phononic metamaterials,<br>Thermal metamaterials / thermal design,<br>Magneto-mechanical metamaterials,<br>Multiscale structural design (engineering: compliant mechanisms, cloaking, morphing, implants),<br>Materials science (crystal datasets, formation energy analysis) |

---


### [132. Extracting accurate materials data from research papers with conversational language models and prompt engineering](https://doi.org/10.1038/s41467-024-45914-8), Nature Communications *(February 21, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Bulk modulus test dataset (hand-curated),<br>Critical cooling rate dataset (R_c1 ground truth),<br>Critical cooling rate dataset (R_c2 ChatExtract),<br>Bulk modulus tables/figures subset,<br>Yield strength of high-entropy alloys dataset (ChatExtract outputs) |
| **Models** | GPT,<br>Transformer |
| **Tasks** | Classification,<br>Binary Classification,<br>Information Retrieval,<br>Structured Prediction,<br>Text Classification,<br>Named Entity Recognition |
| **Learning Methods** | Zero-Shot Learning,<br>Prompt Learning,<br>Pre-training,<br>Fine-Tuning |
| **Performance Highlights** | precision_overall: 90.8%,<br>recall_overall: 87.7%,<br>single-valued_precision: 100%,<br>single-valued_recall: 100%,<br>multi-valued_precision: 100%,<br>multi-valued_recall: 82.7%,<br>precision_overall: 70.1%,<br>recall_overall: 65.4%,<br>single-valued_precision: 100%,<br>single-valued_recall: 88.5%,<br>multi-valued_precision: 97.3%,<br>multi-valued_recall: 55.9%,<br>precision_overall: 61.5%,<br>recall_overall: 62.9%,<br>single-valued_precision: 74.1%,<br>single-valued_recall: 87.7%,<br>multi-valued_precision: 87.3%,<br>multi-valued_recall: 53.5%,<br>precision_overall: 42.7%,<br>recall_overall: 98.9%,<br>single-valued_precision: 100%,<br>single-valued_recall: 100%,<br>multi-valued_precision: 99.2%,<br>multi-valued_recall: 98.4%,<br>precision_overall: 70.0%,<br>recall_overall: 54.7%,<br>single-valued_precision: 100%,<br>single-valued_recall: 76.9%,<br>multi-valued_precision: 86.6%,<br>multi-valued_recall: 45.7%,<br>table_classification_precision: 95%,<br>table_classification_recall: 98%,<br>datapoint_extraction_precision: 91%,<br>datapoint_extraction_recall: 89%,<br>precision: 80%,<br>recall: 82%,<br>standardized_precision: 91.9%,<br>standardized_recall: 84.2%,<br>R_c2_raw_extracted: 634 values,<br>R_c1_ground_truth: 721 entries (raw) |
| **Application Domains** | Materials science,<br>Automated literature data extraction,<br>Materials property database construction (bulk modulus, critical cooling rates for metallic glasses, yield strength for high-entropy alloys),<br>Scientific NLP (information extraction from papers: text, tables, figure captions) |

---


### [130. A generative artificial intelligence framework based on a molecular diffusion model for the design of metal-organic frameworks for carbon capture](https://doi.org/10.1038/s42004-023-01090-2), Communications Chemistry *(February 14, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | hMOF dataset (hypothetical MOF dataset),<br>GEOM dataset,<br>OChemDb (Open Chemistry Database),<br>Generated GHP-MOFassemble artifacts (linker and MOF pools produced by this paper),<br>CoRE DB and Cambridge Structural Database (references) |
| **Models** | Diffusion Model,<br>Graph Neural Network,<br>Graph Neural Network |
| **Tasks** | Synthetic Data Generation,<br>Graph Generation,<br>Regression,<br>Classification,<br>Feature Extraction,<br>Data Generation |
| **Learning Methods** | Generative Learning,<br>Pre-training,<br>Supervised Learning,<br>Ensemble Learning |
| **Performance Highlights** | initial_linker_samples: 64,800 (540 fragments × 20 samples × 6 connection-atom counts),<br>after_hydrogen_addition: 56,257,<br>with_dummy_atoms: 16,162,<br>after_element_filter (remove S, Br, I): 12,305,<br>Model1_R2: 0.932,<br>Model1_MAE: 0.098 m mol g−1,<br>Model1_RMSE: 0.171 m mol g−1,<br>Model2_R2: 0.937,<br>Model2_MAE: 0.100 m mol g−1,<br>Model2_RMSE: 0.170 m mol g−1,<br>Model3_R2: 0.936,<br>Model3_MAE: 0.099 m mol g−1,<br>Model3_RMSE: 0.170 m mol g−1,<br>Ensemble_MAE (reported): 0.093 m mol g−1 (on 10% test set),<br>Ensemble_std_threshold: 0.2 m mol g−1 (approximately 96% of test predictions had std < 0.2),<br>Classification_accuracy (threshold 2 mmol g−1): 98.4% (13,551/13,765 on test set),<br>Balanced_accuracy: 90.7%,<br>training_dataset_for_pretraining: GEOM dataset (~37M conformations / 450k molecules),<br>diversity_indicator: Tanimoto similarity distribution between generated linkers and hMOF linkers: majority around 0.3–0.4 (indicating novelty); internal diversity (IntDiv1/IntDiv2) increased with sampled atoms,<br>screened_structures_input_to_regressor: 18,770 assembled MOFs,<br>predicted_high_performing_by_ensemble: 364 MOFs (predicted > 2 mmol g−1 via ensemble mean+std criterion),<br>downstream_stable_after_MD: 102 MOFs (lattice parameter change <5%),<br>GCMC_confirmed_high_performing: 6 MOFs with CO2 adsorption > 2 mmol g−1 |
| **Application Domains** | Materials Science,<br>Computational Chemistry,<br>Metal-Organic Framework (MOF) design,<br>Carbon capture / CO2 adsorption,<br>Molecular generative design (method transfer from drug discovery to materials) |

---


### [129. Autonomous execution of highly reactive chemical transformations in the Schlenkputer](https://doi.org/10.1038/s44286-023-00024-y), Nature Chemical Engineering *(February 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Zenodo repository: .xdl and .json procedure files and full analytical data for each synthesis |
| **Models** | _None_ |
| **Tasks** | Control,<br>Planning,<br>Experimental Design,<br>Optimization,<br>Decision Making |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | Synthetic chemistry,<br>Organometallic chemistry,<br>Inert-atmosphere (air-/moisture-sensitive) synthesis,<br>Laboratory automation / robotic chemical synthesis,<br>Analytical chemistry (inline NMR and UV–vis),<br>Chemical process safety (automated handling of pyrophoric reagents),<br>Chemical engineering / apparatus design |

---


### [128. Automated self-optimization, intensification, and scale-up of photocatalysis in flow](https://doi.org/10.1126/science.adj1817), Science *(January 26, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | RoboChem Datasets (Zenodo, Robochem Datasets v1),<br>Robochem campaign — HAT alkylation campaign dataset,<br>Robochem campaign — Decatungstate-enabled trifluoromethylthiolation datasets,<br>Robochem campaign — Oxytrifluoromethylation campaign dataset,<br>Robochem campaign — Aryl trifluoromethylation datasets,<br>Robochem campaign — C(sp2)–C(sp3) cross-electrophile coupling datasets |
| **Models** | Gaussian Process,<br>No explicit ML architecture specified (classical surrogate-based Bayesian optimization) |
| **Tasks** | Optimization,<br>Experimental Design,<br>Data Generation,<br>Hyperparameter Optimization |
| **Learning Methods** | Model-Based Learning,<br>Active Learning |
| **Performance Highlights** | yield_percent_HAT_alkylation: >95,<br>isolated_yield_percent_HAT_scaleup: 99,<br>n_experiments_HAT: 19,<br>convergence_in_runs: 9,<br>time_HAT_campaign: 4 hours,<br>productivity_increase_trifluoromethylthiolation: 70-100x (productivity vs reported batch or literature model systems),<br>n_experiments_per_substrate: 18-36,<br>scale_up: 5 mmol (isolated yields closely matched NMR yields),<br>max_space_time_yield_increase_oxytrifluoromethylation: up to 565-fold,<br>n_experiments_per_substrate: 14-25,<br>residence_time_min: as low as 10 s,<br>internal_volume_for_short_residence: 0.26 mL,<br>n_experiments_per_substrate_aryl_trifluoromethylation: 17-35,<br>time_per_campaign: 11-24 hours,<br>observed_improvement: substantial enhancement in yield and productivity vs literature,<br>yield_increase_example_compound_17: from 37% (literature) to 77% (after 60 experiments),<br>n_experiments_compound_17: 60,<br>time_compound_17: 58 hours |
| **Application Domains** | Photocatalysis,<br>Synthetic organic chemistry,<br>Flow chemistry / continuous-flow photochemistry,<br>Medicinal chemistry (drug-like molecules and functionalization),<br>Agrochemical chemistry,<br>Chemical process intensification and scale-up / process development |

---


### [127. A dynamic knowledge graph approach to distributed self-driving laboratories](https://doi.org/10.1038/s41467-023-44599-9), Nature Communications *(January 23, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Aldol condensation closed-loop optimisation dataset (Cambridge & Singapore),<br>Knowledge graph provenance triples (experiment provenance) |
| **Models** | Gaussian Process |
| **Tasks** | Optimization,<br>Experimental Design,<br>Data Generation |
| **Learning Methods** | Evolutionary Learning,<br>Online Learning |
| **Performance Highlights** | highest_yield_%: 93,<br>number_of_runs: 65,<br>best_environment_factor: 26.17,<br>best_space-time_yield_g_per_L_per_h: 258.175 |
| **Application Domains** | chemical reaction optimisation,<br>flow chemistry,<br>laboratory automation / self-driving laboratories (SDLs),<br>digital twin / knowledge graph representations for scientific labs,<br>experiment provenance and FAIR data in chemical sciences |

---


### [125. Universal machine learning aided synthesis approach of two-dimensional perovskites in a typical laboratory](https://doi.org/10.1038/s41467-023-44236-5), Nature Communications *(January 02, 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | High-throughput experimental synthesis dataset (training/test),<br>Prediction set from PubChem,<br>Experimental validation set (selected predicted compounds),<br>Literature-reported successful sample (collected as successful data) |
| **Models** | Support Vector Machine,<br>Decision Tree,<br>Gradient Boosting Tree,<br>Generalized Linear Model |
| **Tasks** | Binary Classification,<br>Classification,<br>Feature Extraction,<br>Feature Selection,<br>Dimensionality Reduction |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning |
| **Performance Highlights** | AUC: 0.85,<br>Reported accuracy (stated): 85%,<br>Misclassification_note: Only 1 out of 10 molecules of 2D perovskites is misclassified by the ML model (quoted in text),<br>Experimental validation success rate (ML-guided): 61.5% (8 out of 13 predicted compounds successfully synthesized),<br>Chemical intuition success rate (baseline): 16.4% (13/79? reported),<br>Improvement factor: Approximately 4× increase in success rate relative to traditional chemical intuition |
| **Application Domains** | Materials discovery / materials science,<br>Two-dimensional hybrid organic–inorganic perovskites (2D HOIPs), specifically 2D AgBi iodide perovskites,<br>Experimental synthesis planning and acceleration,<br>Optoelectronic materials (photodetectors, LEDs),<br>X-ray imaging materials / detectors,<br>Chemoinformatics and molecular descriptor-based ML |

---


### [124. Active learning guides discovery of a champion four-metal perovskite oxide for oxygen evolution electrocatalysis](https://doi.org/10.1038/s41563-023-01707-w), Nature Materials *(January 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Initial 30 resynthesized ABO3 four-metal perovskite oxides,<br>Augmentation set: 10 additional synthesized perovskite oxides,<br>Combined training set (30 initial + 10 augmented),<br>Enumerated candidate set (in silico): 10,101 candidate structures,<br>Mechanism-labelled dataset (AEM vs LOM): curated from literature,<br>Experimental XRD vs simulated/generated XRD pairs |
| **Models** | Message Passing Neural Network,<br>Graph Neural Network,<br>Feedforward Neural Network / (general) Neural Network (bootstrap ensemble) |
| **Tasks** | Regression,<br>Binary Classification,<br>Clustering,<br>Synthetic Data Generation,<br>Feature Extraction,<br>Representation Learning |
| **Learning Methods** | Active Learning,<br>Supervised Learning,<br>Unsupervised Learning,<br>Ensemble Learning,<br>Data Augmentation,<br>Synthetic Data Generation,<br>Representation Learning |
| **Performance Highlights** | accuracy (5-fold CV, OO-1): 0.882,<br>r.m.s.e. (5-fold CV, OO-1): 0.033,<br>r.m.s.e. (simulated vs experimental XRD positions & intensities): 0.0874,<br>accuracy (5-fold CV, OO-2): 0.897,<br>r.m.s.e. (5-fold CV, OO-2): 0.025,<br>uncertainty criterion coverage: 100% of 10,101 candidates have prediction uncertainty < 0.020 V (OO-2),<br>accuracy (5-fold CV, mechanism classifier): 0.917,<br>AUC (area under ROC): 0.932,<br>perovskite subset accuracy & AUC: 1.0 (both),<br>predicted overpotential for champion (CPCF): 396 mV (predicted),<br>experimental overpotential for champion (CPCF): 391 mV (measured),<br>prediction vs experiment agreement (paired t-test p-value for simulated vs experimental XRD predictions): 0.4805,<br>initial % candidates below uncertainty threshold (OO-1): 2.87%,<br>final % candidates below uncertainty threshold (OO-2): 100% |
| **Application Domains** | Materials discovery,<br>Electrocatalysis,<br>Oxygen evolution reaction (OER),<br>Perovskite oxides / inorganic functional materials,<br>Catalyst design and optimization,<br>Materials characterization (XRD-driven representations) |

---


### [123. Self-driving laboratories to autonomously navigate the protein fitness landscape](https://doi.org/10.1038/s44286-023-00002-4), Nature Chemical Engineering *(January 2024)*

| Category | Items |
|----------|-------|
| **Datasets** | Cytochrome P450 dataset,<br>GH1 combinatorial sequence space,<br>SAMPLE experimental dataset (GH1 runs),<br>Reproducibility test set: four diverse GH1 enzymes from Streptomyces species |
| **Models** | Gaussian Process,<br>Gaussian Process |
| **Tasks** | Binary Classification,<br>Regression,<br>Optimization,<br>Data Generation,<br>Batch Learning |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Batch Learning |
| **Performance Highlights** | classification_accuracy: 0.83,<br>Pearson_r: 0.84,<br>measurements_to_find_thermostable_P450s: 26 (on average for UCB positive and Expected UCB in simulation),<br>sample_efficiency_vs_UCB_and_random: 3- to 4-fold fewer samples required compared to standard UCB and random,<br>thermostability_gain: >= 12 °C (agents discovered GH1 sequences at least 12 °C more stable than initial seed sequences),<br>search_fraction: < 2% of full combinatorial landscape searched,<br>observed_batch_effect: slight benefit to running experiments in smaller batches (as reported; evaluated via 10,000 simulated trials) |
| **Application Domains** | Protein engineering,<br>Synthetic biology,<br>Enzyme engineering (thermostability optimization),<br>Automated/self-driving laboratories,<br>High-throughput experimental design / closed-loop experimentation |

---


### [122. Learning skillful medium-range global weather forecasting](https://doi.org/10.1126/science.adi2336), Science *(December 22, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | ERA5,<br>HRES (and HRES-fc0),<br>TIGGE archive (HRES operational tracks),<br>IBTrACS (International Best Track Archive for Climate Stewardship) Version 4,<br>WeatherBench |
| **Models** | Graph Neural Network,<br>Message Passing Neural Network,<br>Encoder-Decoder,<br>Convolutional Neural Network,<br>Transformer |
| **Tasks** | Time Series Forecasting,<br>Regression,<br>Binary Classification |
| **Learning Methods** | Supervised Learning,<br>Backpropagation,<br>Gradient Descent,<br>Mini-Batch Learning,<br>End-to-End Learning |
| **Performance Highlights** | RMSE skill score on Z500: improvement of around 7 to 14% (GraphCast vs HRES),<br>Percent of targets where GraphCast outperforms HRES: 90.3% of 1380 targets (89.9% significant at P ≤ 0.05),<br>Outperform Pangu-Weather: GraphCast outperformed Pangu-Weather on 99.2% of 252 targets,<br>Model parameters: 36.7 million,<br>Runtime: produces a 10-day forecast in under 1 minute on a single Google Cloud TPU v4 device,<br>RMSE, ACC comparisons vs HRES: GraphCast has better RMSE and ACC skill curves across lead times for Z500 (plots show superior RMSE and ACC),<br>RMSE-optimized blurring analysis: RMSE-optimized blurring applied to GraphCast still has greater skill than analogous blurring applied to HRES on 88.0% of 1380 targets,<br>Extreme heat precision-recall: GraphCast precision-recall curves above HRES for 5- and 10-day lead times; HRES better at 12-hour lead time (no single scalar AUC provided in main text),<br>Tropical cyclone median track error: GraphCast has lower median track error than HRES over 2018-2021; GraphCast significantly better for lead times 18 hours to 4.75 days (bootstrapped 95% CIs reported in Fig. 3A/B),<br>Atmospheric river IVT RMSE: GraphCast improves prediction of IVT compared with HRES, improvement from ~25% at short lead times to ~10% at longer horizons |
| **Application Domains** | Medium-range global weather forecasting (meteorology),<br>Severe event prediction (tropical cyclone tracking, atmospheric rivers, extreme temperature),<br>Climate-related forecasting and monitoring (implications for climate trends and retraining recency),<br>Potential applications in other geospatial-temporal forecasting problems (climate, ecology, energy, agriculture, human and biological activity) - suggested |

---


### [121. Data-driven analysis and prediction of stable phases for high-entropy alloy design](https://doi.org/10.1038/s41598-023-50044-0), Scientific Reports *(December 18, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Collected HEA experimental dataset (raw),<br>Cleaned HEA dataset (after data cleaning/outlier removal),<br>Augmented / balanced HEA dataset (after synthetic data generation),<br>Per-category cleaned counts (Table 1) |
| **Models** | XGBoost,<br>Random Forest,<br>Support Vector Machine,<br>Multi-Layer Perceptron |
| **Tasks** | Multi-class Classification,<br>Classification,<br>Feature Selection,<br>Anomaly Detection,<br>Data Augmentation,<br>Hyperparameter Optimization,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Ensemble Learning,<br>Bagging,<br>Boosting,<br>Backpropagation,<br>Stochastic Gradient Descent |
| **Performance Highlights** | accuracy_SS_phases_(BCC,FCC,BCC+FCC): 97%,<br>accuracy_with_AM_included_(4-class_set?): ≈94%,<br>accuracy_with_IM_included_(7 categories): ≈89%,<br>accuracy_all_11_phases: 86%,<br>impact_of_synthetic_data: improvement <1% (all models),<br>accuracy_SS_phases_(BCC,FCC,BCC+FCC): 97%,<br>accuracy_with_AM_included: 93%,<br>accuracy_with_IM_included_(7 categories): ≈89%,<br>accuracy_all_11_phases: 85%,<br>n_trees: 100 (model hyperparameter),<br>accuracy_SS_phases_(BCC,FCC,BCC+FCC): 95%,<br>accuracy_with_AM_included: 95%,<br>accuracy_with_IM_included_(7 categories): just above 84%,<br>accuracy_all_11_phases: 83%,<br>training_epochs: 128,<br>batch_size: 32,<br>learning_rate: 5.0e-4,<br>accuracy_SS_phases_(BCC,FCC,BCC+FCC): 94%,<br>accuracy_with_AM_included: ≈94%,<br>accuracy_with_IM_included_(7 categories): just above 84%,<br>accuracy_all_11_phases: 82%,<br>kernel: 6-degree polynomial (c=5.0, γ=1.0 optimized via grid search) |
| **Application Domains** | Materials Science,<br>Alloy design,<br>High‑entropy alloy (HEA) discovery and phase prediction,<br>Metallurgy,<br>Materials informatics for clean energy / extreme environment applications |

---


### [120. Automated classification of big X-ray diffraction data using deep learning models](https://doi.org/10.1038/s41524-023-01164-8), npj Computational Materials *(December 04, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | 171k (derived from Inorganic Crystal Structure Database, ICSD),<br>Baseline synthetic dataset,<br>Mixed synthetic dataset,<br>Large synthetic dataset,<br>RRUFF dataset,<br>Materials Project (MP) dataset,<br>Lattice Augmentation dataset,<br>Seven synthetic datasets (peak-shape/noise variants) |
| **Models** | Convolutional Neural Network,<br>Convolutional Neural Network,<br>Multi-Layer Perceptron,<br>Deep Neural Network |
| **Tasks** | Multi-class Classification,<br>Multi-class Classification,<br>Multi-class Classification,<br>Synthetic Data Generation,<br>Data Augmentation,<br>Domain Adaptation |
| **Learning Methods** | Supervised Learning,<br>Domain Adaptation,<br>Data Augmentation,<br>Synthetic Data Generation |
| **Performance Highlights** | 7-way_accuracy_on_synthetic_baseline: ≈96%,<br>230-way_accuracy_on_synthetic_baseline: ≈94%,<br>7-way_accuracy_RRUFF_trained_on_baseline: 12%,<br>230-way_accuracy_RRUFF_trained_on_baseline: 12%,<br>7-way_accuracy_RRUFF_trained_on_baseline: 22%,<br>230-way_accuracy_RRUFF_trained_on_baseline: 17%,<br>7-way_accuracy_RRUFF_trained_on_mixed: 35%,<br>230-way_accuracy_RRUFF_trained_on_mixed: 22%,<br>7-way_accuracy_RRUFF_trained_on_mixed: 64%,<br>230-way_accuracy_RRUFF_trained_on_mixed: 53%,<br>7-way_accuracy_RRUFF_trained_on_large: 74%,<br>230-way_accuracy_RRUFF_trained_on_large: 66%,<br>7-way_accuracy_RRUFF_trained_on_souped_large: 86%,<br>230-way_accuracy_RRUFF_trained_on_souped_large: 77%,<br>F1_RRUFF_trained_on_souped_large: 0.753,<br>F1_RRUFF_trained_on_souped_large: 0.568,<br>7-way_accuracy_MP_trained_on_souped_large: 75%,<br>230-way_accuracy_MP_trained_on_souped_large: 45%,<br>7-way_accuracy_MP_trained_on_souped_large: 54%,<br>230-way_accuracy_MP_trained_on_souped_large: 25%,<br>7-way_accuracy_MP_trained_on_souped_large: 67%,<br>230-way_accuracy_MP_trained_on_souped_large: 36%,<br>accuracy_at_80%_size: 88%,<br>accuracy_at_80%_size: 94%,<br>training_accuracy_convergence: ~98% (models converge to ~98% accuracy on synthetic training data),<br>F1_RRUFF_trained_on_souped_large: 0.859 |
| **Application Domains** | Materials science,<br>Crystallography / X-ray powder diffraction analysis,<br>High-throughput experimental XRD (in-situ experiments),<br>Materials characterization and phase identification,<br>Alloying and dynamic compression experiments (time-resolved / extreme conditions),<br>Automated experimental data analysis and materials discovery |

---


### [119. Autonomous chemical research with large language models](https://doi.org/10.1038/s41586-023-06792-0), Nature *(December 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Seven-compound synthesis benchmark (web searcher test set),<br>ECL catalogue samples (Model samples),<br>Perera et al. Suzuki reaction dataset (flow dataset),<br>Doyle Buchwald–Hartwig C–N cross-coupling dataset,<br>Large compound SMILES library (used in computational experiments) |
| **Models** | GPT,<br>Transformer,<br>Attention Mechanism,<br>Multi-Head Attention |
| **Tasks** | Planning,<br>Information Retrieval,<br>Text Generation,<br>Control,<br>Experimental Design,<br>Optimization,<br>Language Modeling |
| **Learning Methods** | Prompt Learning,<br>In-Context Learning,<br>Reinforcement Learning,<br>Fine-Tuning,<br>Embedding Learning,<br>Pre-training |
| **Performance Highlights** | task_score_scale: 1-5 (5=very detailed and chemically accurate; 3=minimum acceptable score),<br>experimental_validation: GC–MS signals matching reference compounds (Suzuki: peak at 9.53 min matching biphenyl; Sonogashira: peak at 12.92 min matching reference),<br>execution_success: Generated code corrected via documentation search and executed successfully on OT-2 and on ECL (HPLC example executed),<br>normalized_advantage: plotted over iterations (improvement over random strategy),<br>normalized_maximum_advantage (NMA): used to compare convergence and maximal performance,<br>iterations: maximum of 20 iterations; corresponds to 5.2% and 6.9% of total search space for first and second datasets respectively,<br>retrieval_success: Documentation sections selected correctly; ECL functions correctly identified; prompt-to-SLL generated code executed at ECL,<br>sample_retrieval: 1,110 Model samples catalog successfully searchable (sample retrieval worked for queries such as 'Acetonitrile') |
| **Application Domains** | Chemistry (organic synthesis and reaction optimization),<br>Laboratory automation / robotics (liquid handling, heater–shaker, plate readers),<br>Cloud laboratory execution (Emerald Cloud Lab),<br>Experimental design and optimization,<br>Scientific information retrieval and documentation grounding |

---


### [118. Scaling deep learning for materials discovery](https://doi.org/10.1038/s41586-023-06735-9), Nature *(December 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project (snapshot March 2021),<br>OQMD (Open Quantum Materials Database) (snapshot June 2021),<br>ICSD (Inorganic Crystal Structure Database) (experimental structures),<br>WBM (Wang, Botti and Marques) dataset snapshot (used for comparison),<br>GNoME-discovered dataset (GNoME generated structures and DFT relaxations / ionic relaxation trajectories),<br>AIRSS-generated random-structure test set,<br>AIMD test sets for MLIP robustness (T=400 K and T=1000 K trajectories),<br>M3GNet dataset (used for comparisons) |
| **Models** | Graph Neural Network,<br>Message Passing Neural Network,<br>Multi-Layer Perceptron,<br>Graph Neural Network,<br>Ensemble (model ensemble) |
| **Tasks** | Regression,<br>Binary Classification,<br>Clustering |
| **Learning Methods** | Active Learning,<br>Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Transfer Learning,<br>Ensemble Learning,<br>Representation Learning,<br>Zero-Shot Learning,<br>Out-of-Distribution Learning |
| **Performance Highlights** | MAE_meV_per_atom_initial_benchmark: 28,<br>MAE_meV_per_atom_improved_network: 21,<br>MAE_meV_per_atom_final_ensemble: 11,<br>MAE_meV_per_atom_compositional_baseline_ref25: 60,<br>MAE_meV_per_atom_compositional_after_AIRSS_filtering: 40,<br>precision_stable_prediction_composition_only_percent: 33,<br>hit_rate_structural_final_percent: greater than 80,<br>hit_rate_compositional_final_percent: 33,<br>initial_hit_rate_structural_percent: less than 6,<br>initial_hit_rate_compositional_percent: less than 3,<br>previous_work_hit_rate_percent: 1,<br>pretrained_MLIP_parameters: 16.24 million,<br>inference_latency_50_atom_system_ms: 14,<br>inference_throughput_ns_per_day_at_2fs: approx. 12,<br>zero_shot_outperforming_state_of_the_art: pretrained GNoME potential outperforms a state-of-the-art NequIP model trained on hundreds of structures (qualitative claim),<br>classification_test_compositions: 623 unseen compositions used for superionic classification experiments,<br>comparative_performance_statement: NequIP trained on M3GNet data performed better than the M3GNet models trained with energies and forces (M3GNet-EF) reported in ref.62 (qualitative/comparative claim) |
| **Application Domains** | Materials discovery,<br>Solid-state chemistry / inorganic crystal discovery,<br>Battery materials (Li-ion conductors, solid electrolytes),<br>Layered materials for electronics and energy storage,<br>Interatomic potential development for molecular dynamics simulations,<br>High-throughput computational materials screening |

---


### [117. An autonomous laboratory for the accelerated synthesis of novel materials](https://doi.org/10.1038/s41586-023-06734-w), Nature *(December 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project (version 2022.10.28),<br>Inorganic Crystal Structure Database (ICSD),<br>SynTERRA / text-mined synthesis literature corpus (24,304 publications),<br>A-Lab experimental dataset (this work),<br>Filtered candidate sets during materials screening |
| **Models** | Convolutional Neural Network,<br>XGBoost,<br>Multi-Layer Perceptron,<br>Feedforward Neural Network,<br>Ensemble Learning |
| **Tasks** | Classification,<br>Regression,<br>Recommendation,<br>Optimization,<br>Experimental Design |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Reinforcement Learning,<br>Representation Learning,<br>Self-Supervised Learning,<br>Ensemble Learning |
| **Performance Highlights** | ensemble_size: 100,<br>dropout_rate: 50%,<br>training_epochs: 50,<br>training_epochs: 50,<br>targets_improved: 9 targets (6 had zero yield initially),<br>example_yield_increase: approximately 70% increase in yield for CaFe2P2O9,<br>search_space_reduction: up to 80% reduction in number of possible experiments for some targets |
| **Application Domains** | Materials science / Solid-state inorganic materials synthesis,<br>Autonomous experimentation / Robotics-integrated laboratory automation,<br>Computational materials (DFT-driven screening and phase stability analysis),<br>Text-mining / NLP for synthesis knowledge extraction,<br>X-ray diffraction analysis and automated crystallographic refinement |

---


### [115. Vision-controlled jetting for composite systems and robots](https://doi.org/10.1038/s41586-023-06684-3), Nature *(November 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | BodyParts3D |
| **Models** | _None_ |
| **Tasks** | Depth Estimation,<br>Control,<br>Experimental Design |
| **Learning Methods** | _None_ |
| **Performance Highlights** | depth_map_resolution: 64 µm × 32 µm × 8 µm,<br>raw_data_rate: 2 GB s^-1,<br>depth_map_size: 54-megapixel depth map,<br>depth_map_processing_time: < 1 s (to convert image data to 54-megapixel depth map),<br>heightmap_pixels_per_map: 2.36 × 10^9 pixels computed into height map within 2.5 s,<br>scanner_camera_rate: each camera captures 6,000 laser line images per second; four cameras used; each camera captures 9,000 images per scan,<br>printing_throughput: up to 33 ml min^-1,<br>speedup_vs_prior_work: 660 times faster than previous work (MultiFab ref. 33),<br>voxel_throughput: 24 × 10^9 voxels h^-1,<br>voxel_size_used_in_examples: 32 µm × 64 µm × 20 µm (example voxel size),<br>total_assigned_voxels_capacity: 6.15 × 10^11 individually assigned voxels,<br>build_volume: 500 mm × 245 mm × 200 mm,<br>print_speed_z_max: up to 16 mm h^-1 (z-direction),<br>print_head_specs: print heads native resolution 400 DPI; eject droplets ≈ 70 pl at 15 kHz; each inkjet unit contains four print heads (Fujifilm Dimatix SG1024-L),<br>heightmap_generation_rate: height maps computed from 2.36 × 10^9 pixels within 2.5 s; geometrically calibrated to 32 µm × 64 µm × 20 µm pixel resolution,<br>walking_robot_speed: approximately 0.01 m s^-1 (approx. 0.1 body length s^-1),<br>walking_robot_turn_speed: (20/15) ° s^-1 (reported turning speed),<br>pneumatic_joint_pressure_capacity: each joint supports actuation pressures up to 35 kPa,<br>pump_flow_rate: up to 2.3 l min^-1 at 90 beats min^-1,<br>robot_actuation_channels: walker uses four pneumatic networks; valves command pressures between 0 kPa to 250 kPa; valve flow per channel up to 380 l min^-1 |
| **Application Domains** | Soft Robotics,<br>Additive Manufacturing / 3D Printing,<br>Materials Science (polymer chemistry),<br>Robotics (bioinspired robot systems),<br>Biomedical / Anatomical modeling (hand geometry),<br>Metamaterials,<br>Industrial manufacturing / prototyping |

---


### [114. AI-driven robotic chemist for autonomous synthesis of organic molecules](https://doi.org/10.1126/sciadv.adj0461), Science Advances *(November 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | ReaxysDB (subset used for training),<br>Synbot experimental dataset for target molecules M1, M2, M3 |
| **Models** | Transformer,<br>Message Passing Neural Network,<br>Graph Neural Network |
| **Tasks** | Sequence-to-Sequence,<br>Optimization,<br>Regression |
| **Learning Methods** | Supervised Learning,<br>Active Learning |
| **Performance Highlights** | top1_prediction_accuracy_improvement: 4.5-7.0%,<br>M1_reference_conversion_on_Synbot: 86.5%,<br>M1_reported_isolation_yield_reference: 37.7%,<br>M1_target_conversion_set: 91.5%,<br>M1_best_found_conversion: 100.0% (found within first trial of searchspace),<br>M2_reference_conversion_on_Synbot: 15.0%,<br>M2_target_conversion_set: 70.0%,<br>M2_best_found_conversion: 100.0% (at 36th and 37th tryouts),<br>M3_modified_reference_conversion_on_Synbot: 50.9%,<br>M3_target_conversion_set: 80.0%,<br>M3_best_found_conversion: target achieved at 42nd trial,<br>system_throughput: 12 reactions per 24 hours (on average),<br>efficiency_gain_vs_human: at least 6x (Synbot vs human performing two experiments/day),<br>reproducibility_dispensing_MAE: ≤0.73 mg,<br>reproducibility_dispensing_CV: ≤2.55%,<br>conversion_yield_CV_overall: <5%,<br>conversion_yield_CV_late_stage: <2.5%,<br>trials_fraction_of_search_space_to_goal: <1% of trials from total search space |
| **Application Domains** | organic chemistry / synthetic chemistry,<br>automated laboratory robotics / robotic chemistry,<br>chemical process optimization,<br>materials discovery (functional organic materials, organic electronics, pharmaceuticals) |

---


### [113. Machine learning-enabled constrained multi-objective design of architected materials](https://doi.org/10.1038/s41467-023-42415-y), Nature Communications *(October 19, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Unlabeled porosity matrix dataset (~18,000 samples),<br>Labeled FEM simulation dataset (initial labeled set: 95; total labeled set size not explicitly stated),<br>Experimental compression test replicates (Ti6Al4V and pure Zn scaffolds),<br>Repository: GAD-MALL code and datasets |
| **Models** | Autoencoder,<br>Convolutional Neural Network,<br>Variational Autoencoder,<br>Gaussian Mixture Model,<br>Finite Element Method |
| **Tasks** | Regression,<br>Multi-objective Optimization,<br>Data Generation,<br>Feature Extraction,<br>Optimization,<br>Image Generation |
| **Learning Methods** | Unsupervised Learning,<br>Supervised Learning,<br>Active Learning,<br>Representation Learning,<br>Ensemble Learning |
| **Performance Highlights** | R2: >0.92 (on test dataset),<br>MAE: low (exact value not specified in main text; referenced in Supplementary Fig. 4),<br>latent_dimension: 8,<br>reconstruction_loss: chosen trade-off (variational variant had higher reconstruction loss and was not used),<br>E2500_Y_increase: >30% (yield strength improvement found at rounds 3 and 5),<br>Macro_implant_load-bearing_increase: ≈20% higher experimental load-bearing capacity (ML-inspired vs uniform),<br>Zn_Y_increase: ~20% increase in Y for ML-designed Zn scaffolds (experimental),<br>simulation-experiment_error: <10% (FEM calibrated to experiments),<br>model_selection_metric: average negative log-likelihood vs number of clusters (supplementary analysis) |
| **Application Domains** | Architected materials / metamaterials,<br>Orthopedic implants / bone scaffold design,<br>3D printing / Additive manufacturing (laser powder bed fusion),<br>Tissue engineering / biomedical implants,<br>Computational materials design (ML + FEM integration) |

---


### [111. Scalable Diffusion for Materials Generation](https://doi.org/), International Conference on Learning Representations *(October 13, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Perov-5,<br>Carbon-24,<br>MP-20,<br>Materials Project (MP) 2021,<br>GNoME (GNoME dataset / structure search database),<br>AIRSS runs / AIRSS-converged compositions,<br>UniMat training set (all experimentally verified stable materials + additional found via search/substitution) |
| **Models** | Denoising Diffusion Probabilistic Model,<br>U-Net,<br>Convolutional Neural Network,<br>Attention Mechanism,<br>Variational Autoencoder,<br>Generative Adversarial Network,<br>Graph Neural Network,<br>PointNet,<br>Transformer |
| **Tasks** | Data Generation,<br>Graph Generation,<br>Synthetic Data Generation |
| **Learning Methods** | Generative Learning,<br>Supervised Learning,<br>Self-Supervised Learning |
| **Performance Highlights** | Proxy_validity_MP-20_%: 97.2,<br>Proxy_coverage_MP-20_%: 89.4,<br>Proxy_recall_MP-20_%: 99.8,<br>Proxy_precision_MP-20_%: 99.7,<br>∆Ef_vs_CDV_AE (eV/atom): -0.216,<br>EfReductionRate_vs_CDV_AE: 0.863,<br>Num_Stable_vs_MP2021: UniMat 414 vs CDV AE 56 (Ed<0 wrt MP 2021),<br>Num_Metastable_vs_MP2021: UniMat 2157 vs CDV AE 90 (Ed<25 meV/atom wrt MP 2021),<br>Num_Stable_vs_GNoME: UniMat 32 vs CDV AE 1 (Ed<0 wrt GNoME convex hull),<br>AIRSS_convergence_rate: 0.55 (AIRSS baseline),<br>UniMat_convergence_rate: 0.81 (overall; computed 0.817 in Appendix C),<br>∆Ef_vs_AIRSS (eV/atom): -0.68,<br>EfReductionRate_vs_AIRSS: 0.80,<br>Ablation_model_size_small_structural_validity_%: 95.7,<br>Ablation_model_size_small_composition_validity_%: 86.0,<br>Ablation_model_size_small_recall_%: 99.8,<br>Ablation_model_size_small_precision_%: 99.3,<br>Ablation_model_size_large_structural_validity_%: 97.2,<br>Ablation_model_size_large_composition_validity_%: 89.4,<br>Ablation_model_size_large_recall_%: 99.8,<br>Ablation_model_size_large_precision_%: 99.7,<br>Proxy_validity_Perov-5_%: 100 (CDV AE Perov-5),<br>Proxy_COV_Perov-5_%: 98.5,<br>∆Ef_vs_MP-20_test (CDV AE, MP-20 test): 0.279,<br>EfReductionRate_vs_MP-20_test: 0.083,<br>Num_Stable_MP2021: 56 stable (Ed<0),<br>Num_Metastable_MP2021: 90 metastable (Ed<25 meV/atom),<br>Proxy_metrics_reference: Included as a baseline in proxy metric comparisons (Table 1) — specific values reported per dataset in Table 1,<br>Example_Perov-5_validity_%: 100 (LM Perov-5 reported 100 in Table 1),<br>MP-20_validity_%: 95.8 (LM on MP-20 in Table 1) |
| **Application Domains** | Materials discovery,<br>Computational materials science,<br>Crystal structure generation,<br>Accelerating materials synthesis and screening (in silico),<br>High-throughput DFT verification workflows |

---


### [110. Universal machine learning for the response of atomistic systems to external fields](https://doi.org/10.1038/s41467-023-42148-y), Nature Communications *(October 12, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | H2O toy dataset (single equilibrium geometry),<br>N-methylacetamide (NMA) dataset,<br>Liquid water dataset (full),<br>Liquid water exemplary dataset for FieldSchNet comparison (special dataset) |
| **Models** | Message Passing Neural Network,<br>Multi-Layer Perceptron,<br>Graph Neural Network |
| **Tasks** | Regression |
| **Learning Methods** | Supervised Learning,<br>Multi-Task Learning,<br>Supervised Learning,<br>Mini-Batch Learning,<br>Backpropagation,<br>Representation Learning,<br>End-to-End Learning |
| **Performance Highlights** | NMA_energy_RMSE_eV: 0.0053,<br>NMA_dipole_RMSE_Debye: 0.028,<br>NMA_polarizability_RMSE_a.u.: 0.51,<br>LiquidWater_FIREANN-wF_forces_RMSE_meV_per_Angstrom: 39.4,<br>200-configs_FIREANN_forces_RMSE_meV_per_Angstrom: 54.5,<br>200-configs_FIREANN_polarizability_RMSE_a.u.: 2.1,<br>200-configs_FieldSchNet_forces_RMSE_meV_per_Angstrom: 245.4,<br>200-configs_FieldSchNet_polarizability_RMSE_a.u.: 165.1,<br>full_dataset_FIREANN_forces_RMSE_meV_per_Angstrom: 45.5,<br>full_dataset_FIREANN_polarizability_RMSE_a.u.: 2.5,<br>full_dataset_FieldSchNet_forces_RMSE_meV_per_Angstrom: 184.7,<br>full_dataset_FieldSchNet_polarizability_RMSE_a.u.: 12.9,<br>training_time_per_epoch_A100_80GB_FIREANN_minutes: 2.4,<br>training_time_per_epoch_A100_80GB_FieldSchNet_minutes: 7.6,<br>H2O_toy_energy_vs_DFT_behavior: qualitatively exact reproduction of rotational and field-intensity dependence (no numeric RMSE reported) |
| **Application Domains** | computational chemistry / quantum chemistry,<br>molecular spectroscopy (IR and Raman),<br>molecular dynamics and path-integral MD (nuclear quantum effects),<br>condensed-phase / liquid water simulations,<br>materials and periodic systems under external electric fields,<br>electrochemistry (potential application),<br>plasmonic chemistry (potential application),<br>tip-induced catalytic reactions (potential application) |

---


### [109. In Pursuit of the Exceptional: Research Directions for Machine Learning in Chemical and Materials Science](https://doi.org/10.1021/jacs.3c04783), Journal of the American Chemical Society *(October 11, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Inorganic Crystal Structure Database (ICSD),<br>Materials Project,<br>AFLOW,<br>OQMD,<br>SuperCon database,<br>Pearson's Crystal Dataset,<br>Experimental hardness measurements dataset,<br>Training set used by Pogue et al. / Meredig/Stanev studies |
| **Models** | Random Forest,<br>Gradient Boosting Tree,<br>Autoencoder,<br>Transformer,<br>Generative Adversarial Network,<br>Convolutional Neural Network |
| **Tasks** | Regression,<br>Classification,<br>Optimization,<br>Multi-objective Optimization,<br>Active Learning,<br>Anomaly Detection,<br>Clustering,<br>Sequence-to-Sequence,<br>Data Generation,<br>Representation Learning |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Active Learning,<br>Transfer Learning,<br>Multi-Task Learning,<br>Ensemble Learning,<br>Evolutionary Learning,<br>Reinforcement Learning,<br>Representation Learning,<br>Feature Selection |
| **Performance Highlights** | percent_above_superhard_threshold_at_0.5N: 0.1%,<br>percent_above_superhard_threshold_at_5N: 0.01%,<br>example_predicted_Hv: Sc2OsB6 ≈ 38 GPa,<br>relative_experiments_to_find_high_Tc: ≈ 1/3 of experiments required by random search |
| **Application Domains** | Materials science (inorganic materials, superconductors, superhard materials, high-entropy alloys),<br>Chemistry (organic synthesis, retrosynthesis, reaction prediction),<br>Experimental automation / self-driving laboratories,<br>Catalysis and nanoparticle synthesis,<br>Photovoltaics and perovskite solar cells,<br>Mechanical property prediction and design (hardness, elastic moduli),<br>Microscopy and spectroscopy (automated characterization and anomaly detection),<br>Drug discovery and biomedical polymers (mentioned as areas ML contributes) |

---


### [108. Finite-difference time-domain methods](https://doi.org/10.1038/s43586-023-00257-4), Nature Reviews Methods Primers *(October 05, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Ground-truth FDTD simulation data (coarse-mesh and dense-mesh results referenced in ref. 99 and other refs),<br>Coarse-mesh FDTD simulation outputs (used as input features) and dense-mesh FDTD results (ground-truth),<br>FDTD outputs used for PINN / physics-based deep learning (examples in refs. 97, 98) |
| **Models** | Convolutional Neural Network,<br>Long Short-Term Memory,<br>Multi-Layer Perceptron |
| **Tasks** | Regression,<br>Time Series Forecasting,<br>Sequence-to-Sequence |
| **Learning Methods** | Supervised Learning,<br>Self-Supervised Learning,<br>Monte Carlo Learning |
| **Performance Highlights** | qualitative: rapidly and accurately computed scattering parameters; achieved results 'in a fraction of the simulated time needed for a dense grid FDTD simulation',<br>qualitative: replicating the FDTD solution with excellent accuracy (qualitative statement; no numeric error metrics reported in text) |
| **Application Domains** | Computational electromagnetics (full-wave Maxwell simulations),<br>Microwave circuits and devices (S-parameter analysis),<br>Photonics and optics (photonic crystals, metasurfaces, metalenses),<br>Plasmonics and nanophotonics,<br>Biomedical imaging and biosensing (OCT, PWS, SERS, MRI coil design, dosimetry),<br>Geophysics and ionospheric propagation (ground-penetrating radar, Earth–ionosphere waveguide),<br>Metamaterials (negative-index lenses, cloaks, leaky-wave antennas),<br>Quantum electromagnetics / quantum photonics (quantum FDTD, Hong–Ou–Mandel modelling),<br>Multiphysics coupling (electromagnetic–thermal, electromagnetic–circuit, quantum–electromagnetic) |

---


### [107. Inverse design of chiral functional films by a robotic AI-guided system](https://doi.org/10.1038/s41467-023-41951-x), Nature Communications *(October 04, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Experimental dataset of 1493 chiral films,<br>Parameter space specification (prescreened combinations) |
| **Models** | Multi-Layer Perceptron,<br>Feedforward Neural Network,<br>Generative Adversarial Network |
| **Tasks** | Regression,<br>Clustering,<br>Optimization,<br>Data Generation,<br>Hyperparameter Optimization,<br>Data Augmentation |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Adversarial Training,<br>Stochastic Gradient Descent,<br>Hyperparameter Optimization |
| **Performance Highlights** | MAE: 0.04,<br>RMSE: 0.06,<br>R2: 0.985,<br>MAE: 0.09,<br>RMSE: 0.15,<br>R2: 0.95,<br>Experimental_g_abs_max: 1.9,<br>Experimental_g_lum_max: 1.9 |
| **Application Domains** | Chiral materials / chiroptical materials,<br>Photonics / optical materials,<br>Materials science (film fabrication and characterization),<br>Display technology (multiplex laser display / color filters),<br>Luminescent materials (circularly polarized luminescence with perovskite quantum dots),<br>Automated experimentation / robotic chemistry |

---


### [106. Retrosynthesis prediction with an interpretable deep-learning framework based on molecular assembly tasks](https://doi.org/10.1038/s41467-023-41698-5), Nature Communications *(October 03, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | USPTO-50K,<br>USPTO-FULL,<br>USPTO-MIT,<br>Nine Tanimoto similarity-based USPTO-50K splits,<br>Leaving Group (LG) vocabulary / database,<br>Multi-step pathway planning cases (provided by authors) |
| **Models** | Transformer,<br>Graph Neural Network,<br>Graph Attention Network,<br>Graph Convolutional Network,<br>Message Passing Neural Network,<br>Multi-Layer Perceptron,<br>Feedforward Neural Network,<br>Long Short-Term Memory,<br>Radial Basis Function Network,<br>Seq2Seq |
| **Tasks** | Sequence-to-Sequence,<br>Graph Generation,<br>Link Prediction,<br>Node Classification,<br>Multi-class Classification,<br>Multi-task Learning,<br>Planning |
| **Learning Methods** | Multi-Task Learning,<br>Contrastive Learning,<br>Pre-training,<br>Self-Supervised Learning,<br>Representation Learning,<br>Dynamic Learning Rate Scheduling (polynomial decay with warm-up) |
| **Performance Highlights** | USPTO-50K (reaction class known) top-1: 66.8%,<br>USPTO-50K (reaction class known) top-3: 88.0%,<br>USPTO-50K (reaction class known) top-5: 92.5%,<br>USPTO-50K (reaction class known) top-10: 95.8%,<br>USPTO-50K (reaction class unknown) top-1: 57.7%,<br>USPTO-50K (reaction class unknown) top-3: 79.2%,<br>USPTO-50K (reaction class unknown) top-5: 84.8%,<br>USPTO-50K (reaction class unknown) top-10: 91.4%,<br>USPTO-FULL top-1: 51.4%,<br>USPTO-FULL top-3: 70.7%,<br>USPTO-FULL top-5: 74.7%,<br>USPTO-FULL top-10: 79.2%,<br>USPTO-MIT top-1: 60.3%,<br>USPTO-MIT top-3: 81.6%,<br>USPTO-MIT top-5: 86.4%,<br>USPTO-MIT top-10: 90.5%,<br>Multi-step pathway planning literature correspondence: 86.9% of single-step reactions (153/176) correspond to reported reactions via SciFinder search,<br>LGM accuracy (reaction-type given vs unknown range reported): increases from 65.4% to 73.2% when reaction type label is provided (text reports improvement range),<br>RCP accuracy (reaction-type given vs unknown range reported): RCP accuracy increased from 81.6% to 91.2% when reaction type label is provided (text reports improvement range),<br>multi-step planning: pathways identified: 101 pathways,<br>single-step matches to literature: 153/176 single steps matched (~86.9%) via SciFinder search,<br>re-ranking improvements: significant improvement in top-1, top-3, top-5, top-10 accuracies when re-ranking predictions from RetroXpert, GLN, and NeuralSym (box-plot based results reported; exact numeric deltas not provided in main text) |
| **Application Domains** | Organic chemistry,<br>Computer-assisted synthetic planning (retrosynthesis),<br>Drug development / medicinal chemistry,<br>Computational chemistry / cheminformatics |

---


### [105. Accelerating science with human-aware artificial intelligence](https://doi.org/10.1038/s41562-023-01648-z), Nature Human Behaviour *(October 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Inorganic materials corpus (Tshitoyan et al. dataset),<br>MEDLINE (biomedical literature),<br>DrugBank candidate drug pool,<br>Comparative Toxicogenomics Database (CTD) curated drug–disease associations,<br>ClinicalTrials.gov (COVID-19 trials),<br>Candidate inorganic compounds pool,<br>Author disambiguation resources |
| **Models** | Graph Neural Network,<br>Graph Convolutional Network,<br>GraphSAGE,<br>Autoencoder |
| **Tasks** | Link Prediction,<br>Ranking,<br>Recommendation,<br>Node Classification,<br>Feature Extraction |
| **Learning Methods** | Unsupervised Learning,<br>Representation Learning,<br>Feature Learning |
| **Performance Highlights** | thermoelectricity_precision_full_graph: 62%,<br>ferroelectricity_precision_full_graph: 58%,<br>photovoltaics_precision_full_graph: 74%,<br>thermoelectricity_precision_authorless_graph: 48%,<br>ferroelectricity_precision_authorless_graph: 50%,<br>photovoltaics_precision_authorless_graph: 58%,<br>discoverer_top50_precision_thermoelectric_and_ferroelectric_1yr: 40%,<br>discoverer_top50_precision_photovoltaics_1yr: 20% |
| **Application Domains** | Materials science (inorganic materials, thermoelectricity, ferroelectricity, photovoltaics),<br>Drug discovery and drug repurposing,<br>Biomedical research (disease–drug associations),<br>COVID-19 therapeutics and vaccines,<br>Science of science / bibliometrics (prediction of scientific discoveries and discoverers) |

---


### [104. A foundation model for generalizable disease detection from retinal images](https://doi.org/10.1038/s41586-023-06555-x), Nature *(October 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | MEH-MIDAS (Moorfields Diabetic imAge dataSet),<br>Kaggle EyePACS,<br>Reference 34 OCT dataset (SPECTRALIS/related),<br>MEH-AlzEye,<br>UK Biobank,<br>Kaggle APTOS-2019,<br>IDRiD,<br>MESSIDOR-2,<br>PAPILA,<br>Glaucoma Fundus,<br>JSIEC,<br>Retina (Kaggle cataractdataset),<br>OCTID |
| **Models** | Vision Transformer,<br>Transformer,<br>Autoencoder,<br>Multi-Layer Perceptron,<br>ResNet,<br>Generalized Linear Model |
| **Tasks** | Image Classification,<br>Multi-class Classification,<br>Binary Classification,<br>Time Series Forecasting,<br>Survival Analysis,<br>Feature Extraction |
| **Learning Methods** | Self-Supervised Learning,<br>Supervised Learning,<br>Transfer Learning,<br>Fine-Tuning,<br>Contrastive Learning,<br>Generative Learning,<br>Pre-training |
| **Performance Highlights** | AUROC_APTOS-2019: 0.943 (95% CI 0.941, 0.944),<br>AUROC_IDRID: 0.822 (95% CI 0.815, 0.829),<br>AUROC_MESSIDOR-2: 0.884 (95% CI 0.88, 0.887),<br>AUROC_CFP: 0.862 (95% CI 0.86, 0.865),<br>AUROC_OCT: 0.799 (95% CI 0.796, 0.802),<br>AUROC_MyocardialInfarction_CFP: 0.737 (95% CI 0.731, 0.743),<br>sensitivity_MyocardialInfarction_CFP: 0.70,<br>specificity_MyocardialInfarction_CFP: 0.67,<br>AUROC_HeartFailure: 0.794 (95% CI 0.792, 0.797),<br>AUROC_IschaemicStroke: 0.754 (95% CI 0.752, 0.756),<br>AUROC_Parkinsons: 0.669 (95% CI 0.65, 0.688),<br>AUROC_wetAMD_DINO: 0.866 (95% CI 0.864, 0.869),<br>AUROC_ischaemicStroke_DINO: 0.728 (95% CI 0.725, 0.731),<br>AUROC_wetAMD_CFP_SL-ImageNet: 0.83 (95% CI 0.825, 0.836),<br>AUROC_age_logistic_regression: 0.63 |
| **Application Domains** | Ophthalmology (retinal imaging),<br>Medical imaging / Clinical diagnosis,<br>Oculomics (using retinal images to predict systemic diseases),<br>Cardiovascular disease risk prediction,<br>Neurodegenerative disease prediction (e.g., Parkinson's),<br>Clinical prognostics (wet-AMD conversion) |

---


### [103. Accelerated discovery of multi-elemental reverse water-gas shift catalysts using extrapolative machine learning approach](https://doi.org/10.1038/s41467-023-41341-3), Nature Communications *(September 21, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Initial catalyst dataset (Iteration = 0),<br>Closed-loop experimental dataset (final),<br>Catalyst candidate composition grid |
| **Models** | Random Forest,<br>Multi-Layer Perceptron |
| **Tasks** | Regression,<br>Optimization,<br>Clustering,<br>Feature Selection,<br>Feature Extraction,<br>Experimental Design,<br>Hyperparameter Optimization |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Representation Learning,<br>Feature Learning,<br>Active Learning,<br>Batch Learning |
| **Performance Highlights** | R^2 (cross-validation, final / best achieved): 0.81,<br>number_of_tested_candidates_experimentally: 300 (255 ML-predicted),<br>number_of_discovered_superior_catalysts: >100,<br>discovery_outcome: identified Pt(3)/Rb(1)-Ba(1)-Mo(0.6)-Nb(0.2)/TiO2 as optimal catalyst; discovered >100 catalysts outperforming prior best,<br>R^2 (explorative model with 8-dim elemental descriptor representation): achieved highest prediction accuracy among the three representation strategies at initial stage; overall best R^2 = 0.81 reported after iterations |
| **Application Domains** | heterogeneous catalysis,<br>catalyst discovery / materials discovery,<br>materials informatics,<br>chemical reaction engineering (reverse water-gas shift, CO2 conversion),<br>experimental design / autonomous discovery workflows |

---


### [102. Accurate proteome-wide missense variant effect prediction with AlphaMissense](https://doi.org/10.1126/science.adg7492), Science *(September 19, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | ClinVar,<br>ProteinGym (MAVE collection),<br>Additional MAVE benchmark (this study),<br>Deciphering Developmental Disorders (DDD) de novo variants,<br>Proteome-wide possible missense variants (all possible single amino acid substitutions),<br>Primate variant population data (and human population frequency data),<br>gnomAD (used implicitly for observed/absent variants and allele frequency strata),<br>UK Biobank / GeneBass summary statistics (complex trait associations) |
| **Models** | Transformer,<br>Autoencoder,<br>Attention Mechanism,<br>Self-Attention Network |
| **Tasks** | Binary Classification,<br>Classification,<br>Regression,<br>Language Modeling,<br>Representation Learning,<br>Ranking |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Weakly Supervised Learning,<br>Knowledge Distillation,<br>Ensemble Learning,<br>Representation Learning,<br>Fine-Tuning |
| **Performance Highlights** | ClinVar_auROC: 0.94,<br>ClinVar_per_gene_auROC_average: 0.95,<br>Proteome_predictions_total: 71 million missense variant predictions (saturating human proteome),<br>Classified_fraction_at_90pct_precison_on_ClinVar: 92.9% resolved vs EVE 67.1%,<br>Proteome_fraction_likely_pathogenic: 32% (22.8 million),<br>Proteome_fraction_likely_benign: 57% (40.9 million),<br>ProteinGym_mean_Spearman: 0.514,<br>Additional_MAVE_mean_Spearman: 0.45,<br>Subset_25_human_proteins_mean_Spearman_all_methods_comparison: 0.474,<br>Example_SHOC2_Spearman: 0.47,<br>Example_GCK_Spearman: 0.53,<br>auROC_underpowered_genes_cell_essentiality: 0.88,<br>LOEUF_auROC_underpowered: 0.81,<br>auROC_rest_of_proteome_alphaMissense: 0.8,<br>auROC_rest_of_proteome_LOEUF: 0.82,<br>ESM1v_MAVE_example_Spearman: 0.459,<br>ESM1v_ClinVar_auROC: reported in comparisons (lower than AlphaMissense; exact number not given in main text for ClinVar) |
| **Application Domains** | Human genetics / clinical variant interpretation,<br>Molecular biology / protein function and mutational scanning,<br>Rare disease diagnostics,<br>Complex trait genetics / population genomics,<br>Computational biology / proteome-scale variant effect prediction,<br>Functional genomics (MAVE integration),<br>Drug target and biomedical research prioritization |

---


### [101. Demonstration of an AI-driven workflow for autonomous high-resolution scanning microscopy](https://doi.org/10.1038/s41467-023-40339-1), Nature Communications *(September 07, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Few-layer WSe2 dark-field experimental dataset (APS live experiment),<br>Pre-acquired dark-field WSe2 image (numerical validation dataset),<br>Cameraman (standard image) - training image,<br>MIT/USC-SIPI/scikit-image images (for simulations) |
| **Models** | Multi-Layer Perceptron,<br>Radial Basis Function Network,<br>Gaussian Process |
| **Tasks** | Image Inpainting,<br>Experimental Design,<br>Optimization,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Pre-training,<br>Batch Learning,<br>Transfer Learning |
| **Performance Highlights** | stable_reconstruction_coverage: 27%,<br>experimental_reconstruction_sufficient_coverage: <25%,<br>experiment_time_saving: ≈65% (~80 minutes saved for the 200x40 demonstration),<br>decision_making_overhead: ≤2% (computation ≈0.15 s to compute new positions per iteration),<br>decision_time_per_iteration: ≈0.15 s (compute new positions) ; ≈42 s to scan batch of 50 positions ; total ≈0.37 s to process diffraction patterns and communicate measurements (per iteration overhead numbers reported),<br>FAST_candidate_calc_time_comparison: FAST: ≈1.5 s (200×40 image on low-power CPU) vs GP: ≈6 s (50×50 image on NVIDIA DGX-2) reported from literature comparison,<br>reconstruction_quality_metrics: NRMSE and SSIM curves show FAST achieves lower NRMSE and higher SSIM at substantially lower sampling percentages compared to raster grid (RG), uniform random (UR), and low-discrepancy (LDR) sampling; exact numeric NRMSE/SSIM values not tabulated in text,<br>final_reconstruction_method: Biharmonic inpainting applied to measured points to generate final images (higher-quality reconstructions shown),<br>sparse_sampling_effectiveness: FAST reproduces flake boundaries and bubbles with high fidelity at much lower measurement percentages than static sampling methods (qualitative and NRMSE/SSIM improvements shown); reconstruction stabilized between 15-20% in experimental demonstration (visual convergence by ≈20%) |
| **Application Domains** | Scanning microscopy,<br>X-ray microscopy (dark-field scanning diffraction microscopy),<br>Synchrotron beamline experiments (Advanced Photon Source),<br>Materials characterization (2D materials, WSe2 thin films),<br>Autonomous / self-driving laboratory experimentation |

---


### [100. A principal odor map unifies diverse tasks in olfactory perception](https://doi.org/10.1126/science.ade4401), Science *(September 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | GS-LF (Good Scents + Leffingwell databases),<br>Prospective validation panel dataset (this study),<br>QC subset for GC-MS/GC-O analyses,<br>Compiled likely-odorants list (~500k),<br>Dravnieks Atlas of Odor Character Profiles,<br>DREAM Olfaction Prediction Consortium dataset (Keller et al.),<br>Abraham et al. detection-threshold dataset,<br>Snitz et al. perceptual similarity dataset |
| **Models** | Graph Neural Network,<br>Random Forest,<br>Support Vector Machine,<br>Linear Model |
| **Tasks** | Multi-label Classification,<br>Regression,<br>Dimensionality Reduction,<br>Feature Extraction,<br>Representation Learning,<br>Out-of-Distribution Learning |
| **Learning Methods** | Supervised Learning,<br>Representation Learning,<br>Transfer Learning,<br>Fine-Tuning,<br>Mini-Batch Learning,<br>Ensemble Learning |
| **Performance Highlights** | cross_validation_AUROC: 0.89,<br>prospective_eval_better_than_median_panelist_percent: 53%,<br>example_per-molecule_correlation_RGNN: 0.63 (example shown),<br>per-label_surpassed_median_panelist: 30/55 labels (55%),<br>statistical_comparison_to_prior_state_of_art: paired two-tailed Student's t-test p = 3.3e-7 (GNN vs previous SOTA on same data),<br>prospective_eval_better_than_median_panelist_percent: 41%,<br>example_per-molecule_correlation_RRF: 0.45 (example shown),<br>triplet_discordance_correct_percent: 19% (baseline model correctly predicts empirical discordance only 19% of time),<br>descriptor_applicability_vs_SVM: POM-linear model outperforms chemoinformatic SVM baseline on Dravnieks, DREAM (Keller), and current data (higher target correlation R),<br>detection_threshold_vs_SVM: POM-linear model outperforms chemoinformatic SVM baseline on Abraham et al. detection-threshold data (higher correlation R),<br>perceptual_similarity_vs_SVM: POM-linear model outperforms chemoinformatic SVM baseline on Snitz et al. perceptual similarity data (higher correlation R),<br>triplet_discordance_prediction_percent: 50% (GNN model correctly predicted counterintuitive structure-odor discordance in 50% of triplets),<br>chemical_class_sulfur_R: R = 0.52 (mean correlation for sulfur-containing molecules) |
| **Application Domains** | olfaction / smell perception,<br>cheminformatics / chemical space mapping,<br>sensory neuroscience,<br>psychophysics,<br>flavor and fragrance industry |

---


### [99. Learning heterogeneous reaction kinetics from X-ray videos pixel by pixel](https://doi.org/10.1038/s41586-023-06393-x), Nature *(September 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | In situ STXM images of carbon-coated LiFePO4 nanoparticles,<br>Auger Electron Microscopy (AEM) carbon-coating intensity maps,<br>Ptychography images (ex situ) |
| **Models** | State Space Model,<br>Gaussian Process |
| **Tasks** | Regression,<br>Structured Prediction,<br>Image-to-Image Translation,<br>Uncertainty Quantification |
| **Learning Methods** | Maximum A Posteriori,<br>Model-Based Learning |
| **Performance Highlights** | training_RMSE: 6.8%,<br>validation_RMSE: 9.6 ± 0.9%,<br>experimental_noise_estimate_sigma_e: ≈0.07 (7%),<br>training_RMSE_at_rho2=0.01: 6.0%,<br>training_RMSE_at_rho2->∞: 10.6%,<br>pixel-to-pixel_correlation_js_vs_AEM: -0.4 (correlation coefficient),<br>training_RMSE: 6.8%,<br>validation_RMSE: 9.6 ± 0.9% |
| **Application Domains** | battery materials / Li-ion battery electrodes,<br>operando microscopy / in situ imaging,<br>energy materials (electrochemistry),<br>materials science (phase-separating solids),<br>non-destructive imaging and characterization |

---


### [98. Material symmetry recognition and property prediction accomplished by crystal capsule representation](https://doi.org/10.1038/s41467-023-40756-2), Nature Communications *(August 25, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project (MP) dataset,<br>MatBench dataset |
| **Models** | Capsule Network,<br>Transformer,<br>Multi-Layer Perceptron,<br>Long Short-Term Memory,<br>DenseNet,<br>Convolutional Neural Network,<br>Graph Neural Network,<br>Graph Convolutional Network,<br>Graph Attention Network,<br>Attention Mechanism |
| **Tasks** | Regression,<br>Feature Extraction,<br>Representation Learning,<br>Dimensionality Reduction,<br>Clustering |
| **Learning Methods** | Supervised Learning,<br>Maximum Likelihood Estimation,<br>Variational Inference,<br>Attention Mechanism,<br>Representation Learning,<br>Feature Extraction,<br>End-to-End Learning |
| **Performance Highlights** | MAE_bandgap_MatBench: 0.181 eV,<br>MAE_formation_energy_MatBench: 0.0161 eV/atom,<br>MAE_bandgap_test_6027: 0.25 eV,<br>MAE_formation_energy_test_6027: 0.0184 eV/atom,<br>MAE_bandgap_test_baseline_MLP: 0.58 eV,<br>MAE_bandgap_test_baseline_DenseNet: 0.55 eV,<br>MAE_bandgap_test_baseline_TFN: 0.49 eV,<br>MAE_bandgap_test_baseline_SE(3)_transformer: 0.86 eV,<br>MAE_reduction_vs_CGCNN_bandgap: 39.1%,<br>MAE_reduction_vs_CGCNN_formation_energy: 52.6%,<br>MAE_reduction_vs_MEgNet_bandgap: 6.2%,<br>MAE_reduction_vs_MEgNet_formation_energy: 36.1%,<br>MAE_reduction_vs_SchNet_bandgap: 23.3%,<br>MAE_reduction_vs_SchNet_formation_energy: 26.1%,<br>MAE_bandgap_test_baseline_EGNN: 0.76 eV |
| **Application Domains** | Materials science / computational materials discovery,<br>Crystal property prediction (bandgap, formation energy),<br>Solid-state physics / electronic structure prediction,<br>Machine learning for scientific interpretation (symmetry recognition and representation learning) |

---


### [97. ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis](https://doi.org/10.1021/jacs.3c05819), Journal of the American Chemical Society *(August 16, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | MOF synthesis dataset (text-mined),<br>Embedding dataset of text segments,<br>Held-out test set for ML prediction |
| **Models** | GPT,<br>Random Forest,<br>Representation Learning |
| **Tasks** | Named Entity Recognition,<br>Text Summarization,<br>Binary Classification,<br>Information Retrieval,<br>Sequence Labeling,<br>Feature Selection,<br>Dimensionality Reduction,<br>Text Generation,<br>Question Answering,<br>Clustering |
| **Learning Methods** | Prompt Learning,<br>Few-Shot Learning,<br>Zero-Shot Learning,<br>Supervised Learning,<br>Representation Learning,<br>Ensemble Learning,<br>Pre-training,<br>Embedding Learning |
| **Performance Highlights** | precision:  >95% (aggregate across processes),<br>recall:  >90% (aggregate across processes),<br>F1:  >92% (aggregate across processes),<br>per-parameter precision/recall/F1: 90-99% reported in abstract for ChemPrompt-guided extraction,<br>accuracy: 87% (average on held-out set),<br>F1: 92% (on held-out set),<br>other_metrics_reported: Precision, Recall, Area Under the Curve were evaluated (values not all explicitly listed; F1/accuracy quoted),<br>embedding_dimensionality: 1536,<br>speed_improvement: 28−37% faster on subsequent readings when storing embeddings locally (reduces processing time by 15−20s per paper described),<br>qualitative_performance: Effective at filtering irrelevant content but can miss synthesis segments that mix characterization or crystallographic data (mitigated by passing mid-relevance to classifier) |
| **Application Domains** | chemistry (metal-organic frameworks synthesis),<br>literature text mining / scientific literature curation,<br>materials science (MOF synthesis outcome prediction),<br>scientific data-to-dialogue systems (chatbot for MOF synthesis Q&A) |

---


### [96. Enhancing corrosion-resistant alloy design through natural language processing and deep learning](https://doi.org/10.1126/sciadv.adg7992), Science Advances *(August 11, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Electrochemical metrics for corrosion-resistant alloys (adapted for pitting potential) |
| **Models** | Recurrent Neural Network,<br>Long Short-Term Memory,<br>Multi-Layer Perceptron,<br>Feedforward Neural Network,<br>Autoencoder |
| **Tasks** | Regression,<br>Feature Extraction,<br>Feature Selection,<br>Optimization,<br>Out-of-Distribution Learning |
| **Learning Methods** | Supervised Learning,<br>Representation Learning,<br>Feature Extraction,<br>Gradient Descent,<br>Backpropagation,<br>Representation Learning |
| **Performance Highlights** | validation_loss (MAE approx): 150 mV,<br>R2 (average over sixfold cross-validation, test predictions): 0.78 ± 0.06,<br>R2 (previous simple DNN for comparison): 0.61 ± 0.04,<br>validation_loss (MAE approx): 168 mV,<br>R2 (average over sixfold cross-validation, test predictions): 0.66,<br>validation_loss (MAE approx): 170 mV,<br>R2 (average over sixfold cross-validation, test predictions): 0.61 ± 0.04,<br>optimization_learning_rate: 0.0001,<br>method: Multidimensional gradient descent using AugNet to compute derivatives,<br>example_word_effect: introduction of the word 'potentiostatic' changed predicted pitting potential from ~177.53 mV to 233.18 mV (error values reported in Table 2) |
| **Application Domains** | Materials science,<br>Corrosion science / Electrochemistry,<br>Alloy design and optimization,<br>Computational metallurgy,<br>Materials informatics / Machine learning for materials |

---


### [95. Applied machine learning as a driver for polymeric biomaterials design](https://doi.org/10.1038/s41467-023-40459-8), Nature Communications *(August 10, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | PolyInfo,<br>Khazana,<br>Polymers: a Property Database,<br>Polymer Property Predictor and Database (PPPDB),<br>MatWeb,<br>Block Copolymer Phase Behavior Database (BCDB),<br>Electron Affinity and Ionization Potential Data,<br>Huan et al. polymer dataset (Sci Data 2016),<br>Community Resource for Innovation in Polymer Technology (CRIPT),<br>Simulated datasets (DFT/molecular dynamics),<br>High-throughput experimental datasets (automated copolymer synthesis examples),<br>Property-specific datasets summarized in Table 3 (collection of studies) |
| **Models** | Random Forest,<br>Gaussian Process,<br>Recurrent Neural Network,<br>Long Short-Term Memory,<br>Multi-Layer Perceptron,<br>Support Vector Machine,<br>Gradient Boosting Tree,<br>LightGBM,<br>Message Passing Neural Network,<br>Variational Autoencoder,<br>Generative Adversarial Network,<br>Linear Model,<br>Graph Neural Network,<br>Gaussian Mixture Model,<br>Random Forest |
| **Tasks** | Regression,<br>Graph Generation,<br>Optimization,<br>Ranking,<br>Clustering,<br>Dimensionality Reduction |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Transfer Learning,<br>Fine-Tuning,<br>Active Learning,<br>Ensemble Learning,<br>Evolutionary Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | Polymeric biomaterials design,<br>Medical devices (catheters, coatings, implants),<br>Drug delivery and polymer excipients,<br>Regenerative medicine (tissue engineering scaffolds),<br>Antifouling coatings & polymers,<br>Biosensors and biologic sensing,<br>3D printing of tissue engineering scaffolds,<br>Protein/ribonucleoprotein delivery (gene editing),<br>Medical imaging (e.g., 19F MRI agents),<br>Polymer electronics / implantable electronics |

---


### [94. Machine Learning Descriptors for Data-Driven Catalysis Study](https://doi.org/10.1002/advs.202301020), Advanced Science *(August 04, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Additive library (Guo et al.),<br>High-throughput catalyst screening dataset (Nguyen et al.),<br>High-throughput OCM catalyst dataset (Ishioka et al.),<br>Permuted three-element catalyst space (Ishioka et al.),<br>Collected OER experimental dataset (Hong et al.),<br>Materials Project perovskite database screening,<br>DFT-computed perovskites for GPR (Li et al. / related),<br>DFT adsorption datasets for *CO and *OH (Li et al.),<br>HEA structures dataset (molecular dynamics + DFT),<br>Spectral adsorption configurations (Wang et al.),<br>Single-atom-alloy screening dataset (SISSO-based high-throughput),<br>Small experimental ORR dataset (Karim et al.),<br>Robotic experimental overpotential dataset |
| **Models** | Decision Tree,<br>Random Forest,<br>XGBoost,<br>Linear Model,<br>Gradient Boosting Tree,<br>Support Vector Machine,<br>Gaussian Process,<br>Multi-Layer Perceptron,<br>Graph Convolutional Network,<br>Random Forest (as used for extra-trees / ETR surrogate) |
| **Tasks** | Regression,<br>Classification,<br>Clustering,<br>Feature Selection,<br>Feature Extraction,<br>Ranking,<br>Hyperparameter Optimization,<br>Dimensionality Reduction |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Transfer Learning,<br>Ensemble Learning,<br>Boosting,<br>Representation Learning |
| **Performance Highlights** | cross_validation_score_range: 0.67-0.84,<br>cross_validation_score_range: 0.67-0.84,<br>Pearson_r_Eads_CO_on_Ag(111): 0.961,<br>Pearson_r_Δe_CO_on_Ag(111): 0.954,<br>RMSE_Eads: 0.015 eV,<br>RMSE_Δe: 0.005 e-,<br>MAE: 0.55 eV,<br>R2: 0.90,<br>MAE: 0.51 eV,<br>R2: 0.84,<br>RMSE_prediction_vs_DFT: ≈0.2 eV,<br>testing_MAE: 0.09 eV,<br>testing_RMSE: 0.12 eV,<br>approximate_scope: GPR used to extend predictions to ≈4000 double perovskites from ~250 DFT calculations,<br>MAE: 0.51 eV,<br>R2: 0.84,<br>Pearson_r_predicted_vs_experimental_overpotentials: 0.878,<br>spectral_ETR_prediction_quality: high (Pearson r typically >0.8; many >0.9 for transfer to diverse systems) |
| **Application Domains** | Heterogeneous catalysis,<br>Electrocatalysis (CO2 reduction reaction, OER, ORR, NRR, HER),<br>Oxidative coupling of methane (OCM),<br>High-throughput experimental catalyst screening,<br>High-entropy alloy (HEA) catalyst discovery,<br>Single-atom alloy catalysts (SAACs),<br>Perovskite oxide catalysts,<br>Metal-zeolite catalysts,<br>Surface–adsorbate interaction prediction (computational spectroscopy),<br>Catalyst design combining DFT and experimental data |

---


### [93. Machine-learning-assisted material discovery of oxygen-rich highly porous carbon active materials for aqueous supercapacitors](https://doi.org/10.1038/s41467-023-40282-1), Nature Communications *(August 01, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Literature-collected dataset for ANN training (Supplementary Table 1),<br>Experimental dataset from this work (hyperporous carbons synthesized and measured in this study) |
| **Models** | Multi-Layer Perceptron |
| **Tasks** | Regression |
| **Learning Methods** | Supervised Learning,<br>Backpropagation |
| **Performance Highlights** | dataset_size: 288 data points,<br>train_RMSE: 25.0,<br>validation_RMSE: 34.5,<br>test_RMSE: 38.5,<br>validation_MSE_at_best_epoch: 1189 (best training performance at epoch 35),<br>train_RMSE_after_adding_experimental_data: 33.0,<br>validation_RMSE_after_adding_experimental_data: 35.6,<br>test_RMSE_after_adding_experimental_data: 46.2,<br>predicted_max_specific_capacitance_initial (in 1 M H2SO4): 611 F/g,<br>predicted_optimal_features_initial: Smicro = 1502 m2/g, Smeso = 687 m2/g, O content = 20 at.%, N content = 0.5 at.%,<br>predicted_max_specific_capacitance_from_prior literature (comparison): 570 F/g (reference prediction for N/O-doped carbons in 6 M KOH from earlier ANN),<br>predicted_optimal_features_after_reinforcement: Smicro = 1710 m2/g, Smeso = 1050 m2/g, N-doping = 2.3 at.%, O-doping = 20 at.% |
| **Application Domains** | Materials discovery,<br>Electrochemical energy storage,<br>Supercapacitors (carbon-based electrodes),<br>Materials informatics / data-driven materials design |

---


### [92. Scientific discovery in the age of artificial intelligence](https://doi.org/10.1038/s41586-023-06221-2), Nature *(August 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | 1.6 million organic-light-emitting-diode material candidates,<br>11 billion synthon-based ligand candidates,<br>ATOM3D,<br>Open Reaction Database,<br>Open Catalyst 2020 (OC20),<br>GuacaMol,<br>Therapeutics Data Commons,<br>250 million protein sequences (protein sequence corpus),<br>LHC Olympics 2020 (anomaly detection challenge / HEP dataset),<br>Pretrained black-hole waveform models (gravitational-wave waveform datasets) |
| **Models** | Autoencoder,<br>Variational Autoencoder,<br>Generative Adversarial Network,<br>Normalizing Flow,<br>Diffusion Model,<br>Convolutional Neural Network,<br>Transformer,<br>Graph Neural Network,<br>Graph Convolutional Network,<br>Graph Attention Network,<br>Message Passing Neural Network,<br>Recurrent Neural Network,<br>Boltzmann Machine,<br>Attention Mechanism,<br>Multi-Layer Perceptron |
| **Tasks** | Anomaly Detection,<br>Image Super-Resolution,<br>Image Denoising,<br>Language Modeling,<br>Structured Prediction,<br>Graph Generation,<br>Regression,<br>Image Generation,<br>Decision Making,<br>Clustering,<br>Sequence-to-Sequence,<br>Ranking |
| **Learning Methods** | Unsupervised Learning,<br>Self-Supervised Learning,<br>Supervised Learning,<br>Reinforcement Learning,<br>Active Learning,<br>Weakly Supervised Learning,<br>Transfer Learning,<br>Fine-Tuning,<br>Contrastive Learning,<br>Pre-training,<br>Adversarial Training |
| **Performance Highlights** | speed_improvement: up to six orders of magnitude faster than traditional methods,<br>protein_structure_accuracy: atomic accuracy (described qualitatively as 'with atomic accuracy, even for proteins whose structure is unlike any of the proteins in the training dataset') |
| **Application Domains** | Physics (high-energy physics, particle collisions, astrophysics, gravitational-wave astronomy),<br>Chemistry (drug discovery, small-molecule design, synthesis planning, catalysis),<br>Materials science (materials discovery, organic light-emitting-diode candidates, catalysts),<br>Biology and bioinformatics (protein folding, protein design, genomics, single-cell analysis),<br>Earth sciences (seismology, Earth system science),<br>Medical imaging and healthcare (pathology slides, chest X-rays, MRI, diagnostic applications),<br>Robotics / experimental automation (self-driving labs, autonomous experimentation),<br>Quantum physics and quantum experiments,<br>Computational fluid dynamics and differential-equation-based modelling |

---


### [91. Fatigue database of complex metallic alloys](https://doi.org/10.1038/s41597-023-02354-1), Scientific Data *(July 12, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | FatigueData-CMA2022 |
| **Models** | ResNet,<br>Convolutional Neural Network,<br>GPT,<br>BERT,<br>Linear Model |
| **Tasks** | Image Classification,<br>Semantic Segmentation,<br>Text Classification,<br>Structured Prediction,<br>Regression,<br>Dimensionality Reduction,<br>Clustering,<br>Information Retrieval |
| **Learning Methods** | Few-Shot Learning,<br>Pre-training,<br>Fine-Tuning,<br>Supervised Learning |
| **Performance Highlights** | precision: 86%,<br>recall: 94%,<br>F1: 90%,<br>precision: 81%,<br>recall: 90%,<br>F1: 86%,<br>F1: 88%,<br>table data extraction precision: 62%,<br>table data extraction recall: 76%,<br>table data extraction F1: 68%,<br>data extraction precision: 82%,<br>data extraction recall: 51%,<br>data extraction F1: 63%,<br>text data extraction precision: 81%,<br>text data extraction recall: 97%,<br>text data extraction F1: 88% |
| **Application Domains** | Materials science,<br>Fatigue of complex metallic alloys,<br>Metallic glasses (MGs),<br>Multi-principal element alloys / High-entropy alloys (MPEAs/HEAs),<br>Data-driven materials design and property analysis,<br>Scientific literature mining / automated data curation |

---


### [90. Encoding physics to learn reaction–diffusion processes](https://doi.org/10.1038/s42256-023-00685-7), Nature Machine Intelligence *(July 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Synthetic 2D Gray–Scott (GS) reaction–diffusion dataset (training measurements),<br>Synthetic 3D Gray–Scott (GS) reaction–diffusion dataset (training measurements),<br>Synthetic 2D / 3D FitzHugh–Nagumo (FN) reaction–diffusion datasets,<br>Synthetic 2D λ–Ω reaction–diffusion dataset,<br>Coefficient identification datasets for 2D Gray–Scott (GS) (S1 and S2),<br>Synthetic Burgers' equation dataset (used for interpretability example),<br>Additional examples referenced (Kolmogorov turbulent flows, etc.) |
| **Models** | Convolutional Neural Network,<br>Recurrent Neural Network,<br>Long Short-Term Memory,<br>Multi-Layer Perceptron,<br>ResNet,<br>Feedforward Neural Network,<br>Graph Neural Network,<br>Transformer |
| **Tasks** | Time Series Forecasting,<br>Image Super-Resolution,<br>Regression,<br>Structured Prediction,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Gradient Descent,<br>Backpropagation,<br>Fine-Tuning,<br>Transfer Learning |
| **Performance Highlights** | accumulative_RMSE: significantly lower than baselines across considered time-marching interval (exact curves shown in Fig. 2 and Fig. 4),<br>mean_absolute_relative_error_coefficients_inverse: 0.6% (noise-free), 1.61% (10% Gaussian noise) for coefficient identification,<br>parameter_efficiency: PeRCNN uses the least number of trainable parameters among compared models (reported in Supplementary Note E.5),<br>accumulative_RMSE: higher than PeRCNN; performs fairly well on 2D cases but deviates considerably for 3D cases (see Fig. 2 and Fig. 4),<br>accumulative_RMSE: higher than PeRCNN; particularly deviates in 3D cases (see Fig. 2),<br>inverse_identification_error: PeRCNN shows superiority to the PINN (details in Supplementary Table 4),<br>reconstruction: recurrent ResNet unable to reconstruct fine-resolution snapshots in 2D GS case under limited noisy training data (see Supplementary Note E.5.1),<br>training_time_per_epoch: For 3D case, elapsed time for training one epoch by PeRCNN is comparable to that of the ResNet (Supplementary Note E.5),<br>discovery_metrics: precision, recall and relative ℓ2 error of the coefficient vector reported in Supplementary Note F (no single scalar in main text); method recovers governing PDEs completely for clean or mildly noisy data and uncovers majority of terms at 10% noise,<br>example_identified_coefficients: ut = 2.001 × 10−5 Δu − 1.003 uv^2 − 0.04008 u + 0.04008; vt = 5.042 × 10−6 Δv + 1.009 uv^2 − 0.1007 v (from Fig. 6 / main text) |
| **Application Domains** | Chemistry (reaction systems),<br>Biology (pattern formation, cell proliferation),<br>Geology,<br>Physics (spatiotemporal PDE systems),<br>Ecology (population dynamics),<br>Fluid dynamics (Burgers', Navier–Stokes references),<br>Epidemiology (mentioned as systems with unknown PDEs),<br>Climate science,<br>Materials science (super-resolution, homogenization),<br>Computational engineering / scientific computing |

---


### [88. Using a physics-informed neural network and fault zone acoustic monitoring to predict lab earthquakes](https://doi.org/10.1038/s41467-023-39377-6), Nature Communications *(June 21, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | p5270,<br>p5271 |
| **Models** | Multi-Layer Perceptron |
| **Tasks** | Regression,<br>Time Series Forecasting |
| **Learning Methods** | Supervised Learning,<br>Backpropagation,<br>Mini-Batch Learning,<br>Transfer Learning,<br>Pre-training,<br>Fine-Tuning |
| **Performance Highlights** | test_R2_shear: test R2 > 0.9 for training data >= 20%,<br>test_R2_slip: R2 between 0.75 and 0.87 (varies with training size),<br>RMSE_table: {'70-10-20': 0.0923, '60-10-20': 0.0964, '50-10-20': 0.0986, '40-10-20': 0.0948, '30-10-20': 0.108, '20-10-20': 0.145, '10-10-20': 0.148, '5-10-20': 0.1835},<br>RMSE_table_PINN#1: {'70-10-20': 0.092, '60-10-20': 0.0943, '50-10-20': 0.0967, '40-10-20': 0.0938, '30-10-20': 0.1031, '20-10-20': 0.1269, '10-10-20': 0.1336, '5-10-20': 0.1654},<br>RMSE_table_PINN#2: {'70-10-20': 0.09, '60-10-20': 0.0863, '50-10-20': 0.088, '40-10-20': 0.0909, '30-10-20': 0.103, '20-10-20': 0.122, '10-10-20': 0.1227, '5-10-20': 0.1487},<br>relative_improvement_low_data: PINN models outperform purely data-driven models by roughly 10-15% when training data are scarce (<=20%),<br>test_R2_behavior: For training data >= 20% PINN test R2 > 0.9 for shear stress; PINNs show improved stability (lower variance) and better slip rate prediction, especially PINN #2,<br>general: All transfer-learned (TL) models outperform standalone (trained from scratch) p5271 models across data splits; TL models converge faster.,<br>specific_observations: TL PINN #1 consistently outperforms other models across splits after tuning; TL PINN #2 and TL data-driven show similar performance except at 10% where TL PINN #2 significantly outperforms TL data-driven; TL PINN models outperform standalone by large margin when training data scarce (10%).,<br>RMSE_reference: Supplementary Table S2 corroborates TL models have smaller RMSE than standalone; TL PINN #1 has smallest errors per split (numerical RMSEs reported in supplement).,<br>learned_constants_PINN#1_error_range: 2% to 14% deviation from known experimental values across varying training dataset sizes (σ, k, v_l),<br>learned_constants_PINN#2_error_range: 1% to 8% deviation from known experimental values across varying training dataset sizes (σ, K, v_l, ρ); A_intact true value not available,<br>contextual_note: Errors generally increase as training set size decreases |
| **Application Domains** | Geophysics / Earthquake physics (laboratory earthquake prediction),<br>Seismic monitoring and prediction,<br>Structural health monitoring / nondestructive evaluation (generalizable context),<br>Geothermal reservoir monitoring, induced seismicity risk assessment, CO2 storage and unconventional reservoir monitoring (application relevance discussed) |

---


### [87. Discovery of senolytics using machine learning](https://doi.org/10.1038/s41467-023-39120-1), Nature Communications *(June 10, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Training dataset (assembled by authors),<br>Screening libraries (assembled by authors for computational screen),<br>Top predicted hits selected for experimental validation,<br>Experimental validation datasets (cellular assays),<br>Panel of known senolytics used as positives (training) |
| **Models** | Random Forest,<br>Support Vector Machine,<br>XGBoost,<br>Naive Bayes,<br>Generalized Linear Model,<br>Decision Tree,<br>Message Passing Neural Network |
| **Tasks** | Binary Classification,<br>Classification,<br>Clustering,<br>Community Detection,<br>Feature Selection,<br>Dimensionality Reduction,<br>Data Augmentation,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Boosting |
| **Performance Highlights** | precision_5-fold_CV: 0.7 ± 0.16,<br>selected_hits_fraction_screened: 21 / 4340 (0.4%),<br>hit_confirmation_rate_experimental: 3 / 21 = 14.28%,<br>prediction_cutoff: selected compounds with P > 44% (prediction probability),<br>qualitative: high precision, low recall (few false positives but many false negatives),<br>feature_selection: used average reduction of Gini index to select 165 features from 200,<br>qualitative: higher recall but lower precision compared to RF (opposite trade-off to RF),<br>qualitative: substantially outperformed by XGBoost on this dataset (no numeric metrics provided),<br>qualitative: worse performance than SVM and RF in comparisons (Supplementary Table 2) |
| **Application Domains** | Drug discovery / early-stage virtual screening,<br>Senolytics discovery (targeting cellular senescence),<br>Cheminformatics / computational chemistry,<br>Phenotypic (target-agnostic) screening,<br>Experimental cell biology validation (in vitro senescence models) |

---


### [86. Precursor recommendation for inorganic synthesis by machine learning materials similarity from scientific literature](https://doi.org/10.1126/sciadv.adg8180), Science Advances *(June 09, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Text-mined inorganic solid-states synthesis recipes (Kononova et al. dataset and derived expansions used in this work) |
| **Models** | Multi-Layer Perceptron,<br>Feedforward Neural Network,<br>Attention Mechanism,<br>Transformer,<br>Rawcomposition,<br>Magpie encoding,<br>FastText,<br>Multi-Layer Perceptron |
| **Tasks** | Recommendation,<br>Multi-label Classification,<br>Regression,<br>Feature Extraction,<br>Representation Learning |
| **Learning Methods** | Self-Supervised Learning,<br>Multi-Task Learning,<br>Representation Learning,<br>Supervised Learning,<br>Backpropagation,<br>Stochastic Learning |
| **Performance Highlights** | success_rate_within_5_attempts: 82%,<br>second_attempt_success_rate: 73%,<br>first_guess_most_common_baseline_success_rate: 36%,<br>example_predicted_probabilities_LaAlO3: see table: e.g., P(use La2O3 | cond La2O3)=0.75, P(use Al2O3 | cond Al2O3)=0.73, P(use Al(NO3)3 | cond Al(NO3)3)=0.65,<br>success_rate_within_5_attempts: 68%,<br>success_rate_within_5_attempts: 56%,<br>num_test_materials_applicable: 1985 (out of 2654) due to vocabulary issues,<br>success_rate_within_5_attempts: 66%,<br>success_rate_within_5_attempts: 58% |
| **Application Domains** | Inorganic materials synthesis,<br>Solid-state synthesis of inorganic materials,<br>Materials science (synthesis planning and recommendation),<br>Autonomous laboratories and recommendation engines for experimental design |

---


### [85. The rise of self-driving labs in chemical and materials sciences](https://doi.org/10.1038/s44160-022-00231-0), Nature Synthesis *(June 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Open Reaction Database (ORD),<br>Chiral metal halide perovskite nanoparticle experiments,<br>Photocatalyst formulation campaign (hydrogen evolution),<br>Quantum dot / semiconductor nanoparticle synthesis datasets,<br>3D-printed geometry experiments for mechanical optimization,<br>General datasets generated by self-driving labs (SDLs) |
| **Models** | Multi-Layer Perceptron,<br>Feedforward Neural Network,<br>Graph Neural Network |
| **Tasks** | Optimization,<br>Experimental Design,<br>Regression,<br>Image Classification,<br>Clustering,<br>Hyperparameter Optimization |
| **Learning Methods** | Active Learning,<br>Evolutionary Learning,<br>Supervised Learning,<br>Online Learning,<br>Transfer Learning,<br>Representation Learning |
| **Performance Highlights** | discovery_speedup: >1,000× faster (referenced for autonomous synthesis–property mapping and on-demand synthesis of semiconductor and metal nanoparticles),<br>notes: specific performance numbers vary by study,<br>photocatalyst_activity: 6× more active than prior art,<br>experiments: 688 experiments in 8-day continuous unattended operation,<br>experiment_count_reduction: 60× fewer experiments than conventional grid search (three-dimensional-printed geometry case),<br>general_benefit: reduced total cost of computation and experimentation when leveraging prior data/models (qualitative),<br>example_reference: transfer learning used in designing lattices for impact protection (ref. 82) |
| **Application Domains** | Chemical synthesis (organic synthesis, retrosynthesis),<br>Materials science (nanomaterials, thin films, perovskites),<br>Clean energy technologies (photocatalysts, solar materials),<br>Pharmaceuticals / active pharmaceutical ingredients (APIs),<br>Additive manufacturing / mechanical design (3D-printed geometries),<br>Catalysis,<br>Device manufacturing and co-design (materials + device integration) |

---


### [84. Combinatorial synthesis for AI-driven materials discovery](https://doi.org/10.1038/s44160-023-00251-4), Nature Synthesis *(June 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Inkjet-printing composition libraries (Gregoire et al. implementation),<br>Microfluidic CsPbX nanocrystal parametric maps (droplet-based microfluidic platform),<br>Autonomous solution-based synthesis platforms datasets (Ada, RAPID, MAOSIC),<br>Combinatorial sputter-deposition thin-film libraries,<br>Web of Science publication counts (combinatorial / autonomous materials) |
| **Models** | None explicitly named from provided model list |
| **Tasks** | Optimization,<br>Experimental Design,<br>Data Generation,<br>Anomaly Detection,<br>Information Retrieval,<br>Structured Prediction,<br>Regression |
| **Learning Methods** | Active Learning,<br>Supervised Learning,<br>Generative Learning,<br>Representation Learning,<br>Ensemble Learning |
| **Performance Highlights** | materials_per_human_intervention: on the order of 10–10^3 materials (for autonomous solution-based platforms),<br>latency: good/low latency for autonomous solution-based workflows,<br>throughput_example: one-to-several unique composition libraries per day for sputter deposition; ~400,000 materials per day achievable for inkjet printing workflows,<br>closed_loop_discovery_examples: demonstrated in refs; qualitative acceleration of discovery reported,<br>latency/automation: good latency and improved decision-making in closed-loop implementations,<br>synthesis_duration: as low as 3–5 seconds per synthesis,<br>in-line_characterization: real-time PL and absorbance used to infer composition/size/distribution,<br>quality_scores: microfluidic nanoparticle synthesis earned 'good' for purity and monitoring in the paper's metric table |
| **Application Domains** | materials discovery,<br>thin-film materials science,<br>electrocatalysis (oxygen evolution catalysts),<br>photoelectrochemical materials / photoanodes,<br>nanoparticle / nanocrystal synthesis (perovskite nanocrystals),<br>autonomous experimental workflows / self-driving laboratories,<br>data science / literature mining for materials synthesis |

---


### [83. Discovering small-molecule senolytics with deep neural networks](https://doi.org/10.1038/s43587-023-00415-z), Nature Aging *(June 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Screened compounds (training set),<br>Broad Institute Drug Repurposing Hub (scored subset),<br>Extended Broad Institute library (scored subset),<br>Predicted chemical space (final prediction set),<br>Curated compounds for experimental validation,<br>Validated hits,<br>Protein structural dataset for molecular docking |
| **Models** | Graph Neural Network,<br>Message Passing Neural Network,<br>Feedforward Neural Network,<br>Random Forest,<br>Ensemble (of Chemprop models) |
| **Tasks** | Binary Classification,<br>Classification,<br>Regression,<br>Dimensionality Reduction,<br>Ranking |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Representation Learning,<br>Bayesian Hyperparameter Optimization,<br>Pre-training |
| **Performance Highlights** | auPRC: 0.243,<br>auPRC_95%_CI: 0.138–0.339,<br>baseline_auPRC: 0.019,<br>prediction_score_range: 2.1e-6 to 0.70 (on 804,959 compounds),<br>working_hit_rate_positive_predictive_value: 11.6% (25 true positives out of 216 high-ranking curated compounds),<br>validated_true_positives_curated: 25/216 (high-ranking),<br>validated_true_negatives_curated: 50/50 (low-ranking negative controls),<br>auPRC_max_observed: 0.15,<br>number_with_PS_gt_0.4_Drug_Repurposing_Hub: 28 (and 284 with PS > 0.1),<br>number_with_PS_gt_0.5_extended_library: 681,<br>number_with_PS_gt_0.4_extended_library: 2,537 |
| **Application Domains** | aging / geroscience,<br>drug discovery / small-molecule screening,<br>cheminformatics / virtual screening,<br>computational biology / molecular docking,<br>pharmacology / senotherapeutics |

---


### [82. A robotic platform for the synthesis of colloidal nanocrystals](https://doi.org/10.1038/s44160-023-00250-5), Nature Synthesis *(June 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Gold nanocrystals experimental database,<br>Double-perovskite (Cs2AgIn1−xBixCl6) experimental database,<br>Literature-mined synthesis-parameter dataset for gold NCs,<br>Solvent screening dataset (double-perovskite),<br>Surfactant screening dataset (double-perovskite),<br>RGB color dataset (gold and double-perovskite NCs) |
| **Models** | SISSO (sure independence screening and sparsifying operator) |
| **Tasks** | Regression,<br>Experimental Design,<br>Feature Extraction,<br>Data Generation,<br>Optimization |
| **Learning Methods** | Supervised Learning |
| **Performance Highlights** | R^2: 0.95,<br>R^2: 0.94,<br>R^2: 0.90,<br>achieved_AR_mean: 4.06,<br>achieved_AR_std: 0.41,<br>achieved_sizes_nm: 78 (nanosized), 749 (microsized) |
| **Application Domains** | Colloidal nanocrystal synthesis,<br>Materials chemistry / inorganic materials,<br>Perovskite nanocrystals,<br>Robotic/automated chemical synthesis,<br>High-throughput experimentation,<br>Materials discovery and inverse design |

---


### [81. Data-driven design of new chiral carboxylic acid for construction of indoles with C-central and C–N axial chirality via cobalt catalysis](https://doi.org/10.1038/s41467-023-38872-0), Nature Communications *(May 31, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | rxn1 dataset (Cp*Co(III)/CCA-catalyzed asymmetric C–H alkylation of indoles with central chirality),<br>rxn2 delta dataset (target atroposelective C–H alkylation with axial chirality),<br>virtual screening candidate CCA set,<br>reaction encoding / descriptor set |
| **Models** | Support Vector Machine,<br>Random Forest,<br>XGBoost |
| **Tasks** | Regression,<br>Ranking,<br>Optimization,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Transfer Learning,<br>Few-Shot Learning |
| **Performance Highlights** | Pearson R (10-fold CV, rxn1): 0.859,<br>MAE (10-fold CV, rxn1): 0.179 kcal/mol,<br>Pearson R (direct application to rxn2, base model without transfer): 0.451,<br>MAE (rxn2 base model before delta correction): 0.210 kcal/mol,<br>MAE (rxn2 after delta learning correction): 0.095 kcal/mol,<br>Predicted enantioselectivity (CCA-3, CCA-4, CCA-5): predicted ~89% (CCA-3, CCA-4), 88% (CCA-5),<br>Experimental enantioselectivity (CCA-4): 94% e.e.,<br>Prediction error (other tested CCAs CCA-6 to CCA-9): maximum error of 14% e.e.,<br>Relative performance: LSVR reported as best in 10-fold CV (Pearson R 0.859, MAE 0.179 kcal/mol); detailed results for RF and XGBoost in Supplementary Table 4 |
| **Application Domains** | Organic chemistry,<br>Catalysis,<br>Asymmetric synthesis,<br>Synthetic chemistry,<br>Molecular catalyst design,<br>Drug discovery / medicinal chemistry (indole motifs and atropisomers relevant to pharma) |

---


### [80. A database of ultrastable MOFs reassembled from stable fragments with machine learning models](https://doi.org/10.1016/j.matt.2023.03.009), Matter *(May 03, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | CoRE MOF (ASR subset),<br>Extended CoRE MOF 2019 dataset (unsanitized structures added),<br>MOFSimplify extracted stability dataset (~3,000 MOFs),<br>hMOF,<br>BW-DB,<br>ToBaCCo,<br>ARC-MOF,<br>Ultrastable MOF database (this work) |
| **Models** | Multi-Layer Perceptron |
| **Tasks** | Regression,<br>Binary Classification,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Feature Extraction,<br>Feature Selection |
| **Performance Highlights** | MAE_Td_Celsius: 44,<br>activation_stability_threshold: 0.5,<br>predicted_ultrastable_fraction_ultrastable_MOF_database: 9524_of_54139_(~18%),<br>predicted_thermally_and_activation_stable_in_new_DB: 25,336_of_54,139_(~47%),<br>predicted_activation_stable_below_average_thermal: 19,342_of_54,139_(~36%),<br>predicted_thermally_stable_not_activation_stable: 4,285_of_54,139_(~8%),<br>ultrastable_counts_hMOF: 416,<br>ultrastable_counts_BW-DB: 767,<br>ultrastable_counts_ToBaCCo: 750,<br>ultrastable_counts_ARC-MOF: 1,564 |
| **Application Domains** | Materials discovery,<br>Metal-organic frameworks (MOFs) design and screening,<br>Gas storage and separation (methane storage/deliverable capacity),<br>Catalysis (high-temperature catalysis relevance),<br>Mechanical stability assessment of porous materials,<br>In silico database construction for porous materials (including potential application to COFs and ZIFs) |

---


### [79. High-throughput printing of combinatorial materials from aerosols](https://doi.org/10.1038/s41586-023-05898-9), Nature *(May 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Printed combinatorial gradient films (various materials: Ag/Bi2Te3, Bi2Te2.7Se0.3 with S doping, polyurethane FGP, GO/rGO gradients, etc.),<br>Printing process parameter measurements (ink flow rates, sheath gas flow rates, nozzle sizes, printing speed) |
| **Models** | _None_ |
| **Tasks** | Optimization,<br>Hyperparameter Optimization,<br>Data Generation,<br>Experimental Design,<br>Feature Extraction |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | Materials science (combinatorial materials discovery),<br>Additive manufacturing / Printed electronics,<br>Thermoelectrics / energy harvesting,<br>Polymer functional grading and mechanics,<br>Nanocomposites and nanomaterials (0D/1D/2D materials),<br>Chemical/materials reaction screening (e.g., GO reduction) |

---


### [78. Generative Models as an Emerging Paradigm in the Chemical Sciences](https://doi.org/10.1021/jacs.2c13467), Journal of the American Chemical Society *(April 26, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | GuacaMol,<br>MOSES (Molecular Sets),<br>Polymer Genome,<br>ANI-2x (ANI2x) |
| **Models** | Variational Autoencoder,<br>Generative Adversarial Network,<br>Normalizing Flow,<br>Diffusion Model,<br>Graph Neural Network,<br>Recurrent Neural Network,<br>Gaussian Process |
| **Tasks** | Data Generation,<br>Graph Generation,<br>Sequence-to-Sequence,<br>Optimization,<br>Regression,<br>Language Modeling |
| **Learning Methods** | Reinforcement Learning,<br>Policy Gradient,<br>Actor-Critic,<br>Deterministic Policy Gradient,<br>Temporal Difference Learning,<br>Adversarial Training,<br>Representation Learning,<br>Active Learning,<br>Supervised Learning |
| **Performance Highlights** | penalized_logP_benchmark: GraphAF outperformed other common generative models at the time in its ability to generate high penalized logP values (no numeric value provided in text),<br>benchmarking_tasks: MolGAN enabled better predictions on a number of benchmarking tasks (no numeric values provided in text) |
| **Application Domains** | Chemical sciences,<br>Molecular discovery / drug discovery,<br>Materials science (including organic crystals and functional materials),<br>Polymeric/macromolecular design,<br>Automated/self-driving laboratories / autonomous experimentation,<br>Computational chemistry and molecular simulation (integration with ML interatomic potentials),<br>Synthetic chemistry / retrosynthetic planning |

---


### [77. A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing](https://doi.org/10.1038/s41524-023-01003-w), npj Computational Materials *(April 05, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | PolymerAbstracts,<br>Materials science corpus (2.4 million abstracts),<br>Polymer-relevant subset of corpus,<br>ChemDNER,<br>Inorganic Synthesis recipes (Materials Science Procedural Text Corpus),<br>Inorganic Abstracts,<br>ChemRxnExtractor,<br>PubMed corpus (referenced),<br>MatBERT pretraining corpus (referenced),<br>ChemBERT pretraining corpus (referenced),<br>Polymer property extraction output (product dataset) |
| **Models** | BERT,<br>Transformer,<br>Linear Model,<br>Bidirectional LSTM |
| **Tasks** | Named Entity Recognition,<br>Sequence Labeling,<br>Named Entity Recognition,<br>Structured Prediction,<br>Regression,<br>Information Retrieval |
| **Learning Methods** | Self-Supervised Learning,<br>Unsupervised Learning,<br>Transfer Learning,<br>Fine-Tuning,<br>Supervised Learning,<br>Weakly Supervised Learning |
| **Performance Highlights** | Precision (%): 62.5,<br>Recall (%): 70.6,<br>F1 (%): 66.4,<br>ChemDNER F1 (%): 69.2,<br>Inorganic Synthesis recipes F1 (%): 68.6,<br>Inorganic Abstracts F1 (%): 86.0,<br>ChemRxnExtractor F1 (%): 71.4,<br>PolymerAbstracts F1 (%): 65.8,<br>PolymerAbstracts F1 (%): 65.2,<br>PolymerAbstracts F1 (%): 62.6,<br>PolymerAbstracts F1 (%): 57.0,<br>PolymerAbstracts F1 (%): 56.2,<br>Extracted property records (count): 300000,<br>Source abstracts processed: 130000,<br>Processing time: 60 hours on a single Quadro 16 GB GPU |
| **Application Domains** | Polymers,<br>Materials Science (general),<br>Polymer Solar Cells (organic photovoltaics),<br>Fuel Cells (polymer electrolyte membranes),<br>Supercapacitors (energy storage),<br>Chemical / Organic Chemistry (NER benchmarking datasets) |

---


### [76. Evolutionary-scale prediction of atomic-level protein structure with a language model](https://doi.org/10.1126/science.ade2574), Science *(March 17, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | UniRef90 / UniRef50 (training clusters),<br>MGnify90 (MGnify database),<br>Protein Data Bank (PDB),<br>AlphaFold2-predicted structures (augmentation),<br>CAMEO test set,<br>CASP14 test set,<br>Held-out UniRef50 clusters for perplexity evaluation |
| **Models** | Transformer,<br>Attention Mechanism,<br>Multi-Head Attention |
| **Tasks** | Language Modeling,<br>Structured Prediction,<br>Representation Learning,<br>Supervised Learning |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Supervised Learning,<br>Transfer Learning,<br>Fine-Tuning,<br>Knowledge Distillation,<br>Representation Learning |
| **Performance Highlights** | perplexity_8M: 10.45,<br>perplexity_15B: 6.37,<br>perplexity_3B_on_CAMEO: 5.7,<br>TM-score_15B_on_CAMEO_projection: 0.72,<br>TM-score_15B_on_CASP14_projection: 0.55,<br>correlation_perplexity_vs_CASP14_TMscore: -0.99,<br>correlation_perplexity_vs_CAMEO_TMscore: -1.00,<br>correlation_contactprecision_vs_CASP14_TMscore: 0.96,<br>correlation_contactprecision_vs_CAMEO_TMscore: 0.99,<br>ESMFold_TM-score_CAMEO_avg: 0.83,<br>ESMFold_TM-score_CASP14_avg: 0.68,<br>AlphaFold2_TM-score_CAMEO_avg: 0.88,<br>AlphaFold2_TM-score_CASP14_avg: 0.85,<br>ESMFold_LDDT_high_confidence_pLDDT_gt_0.7_on_CAMEO: 0.83,<br>AlphaFold2_LDDT_on_CAMEO: 0.85,<br>median_all-atom_RMSD95_CAMEO: 1.91 Å,<br>median_backbone_RMSD95_CAMEO: 1.33 Å,<br>median_all-atom_RMSD95_pLDDT_gt_0.9: 1.42 Å,<br>median_backbone_RMSD95_pLDDT_gt_0.9: 0.94 Å,<br>DockQ_examples: Glucosamine-6-phosphate deaminase (7LQM) DockQ 0.91; L-asparaginase (7QYM) DockQ 0.97,<br>DockQ_multimer_agreement_with_AlphaFoldMultimer: qualitative DockQ categorization same for 53.2% of chain pairs (on 2,978 complexes),<br>speed_384_residues_on_V100: 14.2 s (≈6× faster than single AlphaFold2 model),<br>speedup_on_short_sequences: up to ~60×,<br>metagenomic_scale_predictions: >617 million sequences folded; ~365 million predictions mean pLDDT>0.5 & pTM>0.5; ~225 million predictions mean pLDDT>0.7 & pTM>0.7,<br>calibration_correlation_pLDDT_vs_AlphaFold_LDDT_on_~4000_MGnify_subset: Pearson r = 0.79 |
| **Application Domains** | Structural biology,<br>Proteomics,<br>Metagenomics,<br>Bioinformatics / Computational biology,<br>Protein design,<br>Biotechnology / drug discovery (potential downstream applications) |

---


### [75. AlphaFlow: autonomous discovery and optimization of multi-step chemistry using a self-driven fluidic lab guided by reinforcement learning](https://doi.org/10.1038/s41467-023-37139-y), Nature Communications *(March 14, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | AlphaFlow cALD sequence selection campaign data,<br>AlphaFlow reagent volume and reaction time optimization data (volume-time campaign),<br>AlphaFlow digital twin training dataset (480 nm QD campaign),<br>AlphaFlow repository (source data & code) |
| **Models** | Multi-Layer Perceptron,<br>Gradient Boosting Tree,<br>Multi-Layer Perceptron,<br>Gradient Boosting Tree |
| **Tasks** | Optimization,<br>Experimental Design,<br>Decision Making,<br>Planning,<br>Control,<br>Binary Classification,<br>Experimental Design |
| **Learning Methods** | Reinforcement Learning,<br>Model-Based Learning,<br>Supervised Learning,<br>Ensemble Learning |
| **Performance Highlights** | lambda_AP_shift_vs_conventional_after_6_cycles_nm: 26,<br>photoluminescence_intensity_vs_conventional_percent: 450,<br>RPV_increase_by_4th_cycle_percent: 40,<br>RL_viable_set_found_after_experiments: 4 (87% of known optimum),<br>RL_final_reward_after_100_experiments_percent_of_optimum: 94,<br>ENN_BO_viable_set_found_after_100_experiments: 0 (failed to identify 20-consecutive viable injections),<br>basin_hopping_simulations_needed_for_optimum_function_evals: >50,000 simulated experiments,<br>RL_simulations_experiments_to_reach_94_percent_optimum: 100 real experiments (digital-twin RL campaigns) |
| **Application Domains** | Chemistry,<br>Materials Science,<br>Nanoscience,<br>Colloidal quantum dot synthesis (CdSe/CdS core-shell QDs),<br>Automated experimentation / Self-driving laboratories,<br>Flow chemistry / microfluidic reaction systems |

---


### [73. A multi-modal pre-training transformer for universal transfer learning in metal–organic frameworks](https://doi.org/10.1038/s42256-023-00628-2), Nature Machine Intelligence *(March 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | 1 million hypothetical MOFs (hMOFs),<br>20,000 hypothetical MOFs (hMOFs) fine-tuning set,<br>QMOF database (version 13),<br>CoRE MOF (CoREMOF) datasets,<br>Text-mined stability datasets,<br>Pre-training splits (internal) |
| **Models** | Transformer,<br>BERT,<br>Vision Transformer,<br>Graph Convolutional Network,<br>Graph Neural Network,<br>Multi-Layer Perceptron |
| **Tasks** | Pre-training tasks (group),<br>Multi-class Classification,<br>Regression,<br>Binary Classification,<br>Regression,<br>Classification,<br>Node Classification |
| **Learning Methods** | Pre-training,<br>Fine-Tuning,<br>Transfer Learning,<br>Supervised Learning,<br>Multi-Task / Multi-Modal Learning,<br>Attention Mechanism,<br>End-to-End Learning |
| **Performance Highlights** | accuracy: 0.97,<br>MAE: 0.01,<br>accuracy: 0.98,<br>R2: 0.78,<br>R2: 0.83,<br>R2: 0.77,<br>R2: 0.78,<br>MAE: 0.3,<br>accuracy: 0.76,<br>R2: 0.44,<br>MAE (temperature): 45 °C,<br>relative_performance: outperforms baseline models (energy histogram, descriptor-based ML, CGCNN) across dataset sizes (5k–20k) as shown in Fig. 3c |
| **Application Domains** | Metal–organic frameworks (MOFs) / porous materials,<br>Materials discovery and design,<br>Gas adsorption and storage (e.g., H2, N2, O2, CO2),<br>Molecular diffusion in porous media,<br>Electronic property prediction (band gap, DFT properties),<br>Materials stability prediction (solvent removal stability, thermal stability),<br>Text-mined experimental property prediction |

---


### [72. Accelerating the design of compositionally complex materials via physics-informed artificial intelligence](https://doi.org/10.1038/s43588-023-00412-7), Nature Computational Science *(March 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | six million texts (literature corpus mined by Pei et al.),<br>NOMAD laboratory / shared materials data,<br>thermodynamic and kinetic databases (CALPHAD and related databases),<br>combinatorial high-throughput experimental materials libraries,<br>ab initio simulation datasets (DFT calculations),<br>Materials literature (heterogeneous corpus) |
| **Models** | Multi-Layer Perceptron,<br>Graph Neural Network,<br>Random Forest,<br>Attention Mechanism |
| **Tasks** | Regression,<br>Classification,<br>Image Classification,<br>Semantic Segmentation,<br>Surrogate modeling (mapped to Regression / Model reduction),<br>Information Retrieval,<br>Outlier Detection,<br>Clustering,<br>Dimensionality Reduction,<br>Active Learning (task framed as data acquisition for model improvement) |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Reinforcement Learning,<br>Semi-Supervised Learning,<br>Unsupervised Learning,<br>Transfer Learning |
| **Performance Highlights** | discovered_candidates_count: 70 |
| **Application Domains** | Computational materials science,<br>Alloy design (high-entropy alloys, Invar alloys, superalloys),<br>Ceramics and high-entropy ceramics,<br>Corrosion and surface protection,<br>Battery materials and catalysis (multi-physics problems),<br>Microstructure evolution and mechanical behavior (phase-field, crystal plasticity),<br>Atomistic/molecular dynamics and interatomic potential development,<br>Materials literature mining and knowledge extraction,<br>Sustainable materials design and life-cycle assessment |

---


### [71. Biological research and self-driving labs in deep space supported by artificial intelligence](https://doi.org/10.1038/s42256-023-00618-4), Nature Machine Intelligence *(March 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | NASA GeneLab,<br>NASA Open Science Data Repository (includes GeneLab and other spaceflight-relevant data),<br>BioSentinel dataset (yeast deep-space CubeSat experiment),<br>Nanopore sequencing data generated aboard the ISS,<br>SPOKE knowledge network embeddings (used with transcriptomic spaceflown mouse data),<br>ECG data from astronaut wearable device (used to train ECG Generator model),<br>Various spaceflight image datasets (retinal/OCT, microscopy, behavioural video) |
| **Models** | Generative Adversarial Network,<br>Variational Autoencoder,<br>Vision Transformer,<br>Transformer,<br>Convolutional Neural Network,<br>Variational Autoencoder,<br>Generative Adversarial Network |
| **Tasks** | Instance Segmentation,<br>Image Classification,<br>Pose Estimation,<br>Synthetic Data Generation,<br>Data Augmentation,<br>Feature Extraction,<br>Clustering,<br>Dimensionality Reduction,<br>Time Series Forecasting,<br>Anomaly Detection,<br>Language Modeling,<br>Image Generation |
| **Learning Methods** | Supervised Learning,<br>Transfer Learning,<br>One-Shot Learning,<br>Few-Shot Learning,<br>Pre-training,<br>Fine-Tuning,<br>Generative Learning,<br>Federated Learning,<br>Continual Learning,<br>Contrastive Learning |
| **Performance Highlights** | performance_level: human-level performance |
| **Application Domains** | Space biology,<br>Precision astronaut health / space medicine,<br>Multi-omics and genomics (spaceflight sequencing and analysis),<br>Imaging (ocular/retinal imaging, microscopy, behavioural video),<br>Synthetic biology and automated lab automation (self-driving labs),<br>Robotics and microfluidics for automated experiments,<br>Knowledge graph / biomedical knowledge integration,<br>Edge/onboard computing for space systems |

---


### [70. A Materials Acceleration Platform for Organic Laser Discovery](https://doi.org/10.1002/adma.202207070), Advanced Materials *(February 09, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | Organic laser dataset (this work) / organic-laser-data (GitHub) |
| **Models** | _None_ |
| **Tasks** | Data Generation,<br>Experimental Design,<br>Optimization,<br>Feature Extraction |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | Organic semiconductor lasers (OSLs),<br>Accelerated materials discovery / Materials acceleration platforms,<br>Automated synthesis and autonomous laboratories,<br>Optoelectronics (thin-film devices, ASE/lasing characterization),<br>Computational quantum chemistry for structure–property relationships |

---


### [69. Machine Learning-Assisted Synthesis of Two-Dimensional Materials](https://doi.org/10.1021/acsami.2c18167), ACS Applied Materials & Interfaces *(January 11, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | CVD-grown MoS2 dataset (constructed from historical literature) |
| **Models** | XGBoost,<br>Support Vector Machine,<br>Naive Bayes,<br>Multi-Layer Perceptron |
| **Tasks** | Binary Classification,<br>Classification,<br>Feature Selection |
| **Learning Methods** | Supervised Learning,<br>Boosting,<br>Backpropagation,<br>End-to-End Learning |
| **Performance Highlights** | accuracy: average prediction accuracy of over 88%,<br>AUROC: 0.91,<br>predicted_probability_example_experiment: 86.8913%,<br>validation_probabilities_table2_P(%): [87.6066, 86.2643, 89.8598, 56.0011, 76.2967, 57.0913],<br>recall: 94.1% (SVM recall reached 94.1%, 2% higher than XGBoost for recall) |
| **Application Domains** | Materials synthesis (chemical vapor deposition),<br>Two-dimensional materials (MoS2),<br>Materials science for electronics and optoelectronics,<br>Accelerated materials discovery and experimental optimization |

---


### [68. Toward the design of ultrahigh-entropy alloys via mining six million texts](https://doi.org/10.1038/s41467-022-35766-5), Nature Communications *(January 04, 2023)*

| Category | Items |
|----------|-------|
| **Datasets** | 6.4 million abstracts (training corpora),<br>2.6 million candidate alloys (combinatorial search space),<br>Shortlist of 494 HEA candidates,<br>Alloy DOI list used to generate training corpora |
| **Models** | Multi-Layer Perceptron,<br>Feedforward Neural Network,<br>Principal Component Analysis (PCA) |
| **Tasks** | Language Modeling,<br>Embedding Learning,<br>Feature Extraction,<br>Dimensionality Reduction,<br>Ranking,<br>Information Retrieval,<br>Named Entity Recognition,<br>Recommendation |
| **Learning Methods** | Unsupervised Learning,<br>Transfer Learning,<br>Embedding Learning,<br>Representation Learning,<br>Pre-training |
| **Performance Highlights** | vector_dimension: 200,<br>window_size: 8,<br>training_epochs: 30,<br>training_corpus_size: 6.4M abstracts,<br>selected_candidates_from_search_space: 494 out of 2,600,000,<br>Senkov_alloy_ranking: continually ranked among the top three for 5-component BCC candidates across yearly models,<br>Cantor_alloy_ranking: ranked as the second most promising solid-solution HEA by our method already before 2004,<br>correlation_with_thermodynamic_gamma: positive linear correlation (numeric coefficient not provided),<br>visual_grouping: elements with similar chemical features are grouped together in PCA projection,<br>alloy_name_standardization_effect: all permutations of element order yield the same alloy node in alloyKG (alphabetized), enabling reliable retrieval (qualitative),<br>recommended_candidates: top-ranked alloys such as TiCrFeCoNi (six-component) and TiCrFeCoNiCuZn (seven-component); 494 total shortlisted |
| **Application Domains** | Materials Science,<br>Metallurgy,<br>High-Entropy Alloys (HEAs) design,<br>Integrated Computational Materials Engineering (ICME),<br>Scientific literature mining / Text mining,<br>Knowledge graph-based literature retrieval |

---


### [67. On scientific understanding with artificial intelligence](https://doi.org/10.1038/s42254-022-00518-3), Nature Reviews Physics *(December 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | 1.6 million molecule search space (Gómez-Bombarelli et al.),<br>SARS-CoV-2 molecular dynamics simulation data,<br>Glycoblocks molecular dynamics data,<br>Large Hadron Collider (LHC) experimental data (ATLAS, CMS etc.),<br>Materials science literature corpus (used for unsupervised word embeddings),<br>Solar System observational data (last ~30 years),<br>Protein structure data / human proteome (AlphaFold outputs and training data),<br>DrugBank chemical space (used in VR exploration),<br>General scientific literature corpora / semantic knowledge networks |
| **Models** | Graph Neural Network,<br>Transformer,<br>BERT,<br>GPT,<br>Siamese Network,<br>Gradient Boosting Tree,<br>Decision Tree,<br>Random Forest,<br>Recurrent Neural Network,<br>Variational Autoencoder,<br>Feedforward Neural Network,<br>Ensemble Learning,<br>Graph Attention / Graph-based neural approaches (implicit) |
| **Tasks** | Anomaly Detection,<br>Outlier Detection,<br>Regression,<br>Time Series Forecasting,<br>Representation Learning,<br>Feature Extraction,<br>Information Retrieval,<br>Experimental Design,<br>Clustering,<br>Node Classification,<br>Language Modeling |
| **Learning Methods** | Unsupervised Learning,<br>Supervised Learning,<br>Weakly Supervised Learning,<br>Reinforcement Learning,<br>Representation Learning,<br>Boosting,<br>Pre-training,<br>Ensemble Learning |
| **Performance Highlights** | qualitative: high-quality prediction of object's motion; simultaneously predicts masses correctly,<br>qualitative: BERT/GPT-3 cited as able to help extract scientific knowledge and enable advanced queries in natural-language interaction |
| **Application Domains** | Physics (theoretical and experimental),<br>Chemistry (molecular design, materials, quantum chemistry),<br>Biology / Structural Biology (protein folding, SARS-CoV-2 spike protein),<br>Quantum Optics / Quantum Computing,<br>High-Energy / Particle Physics (LHC anomaly detection),<br>Astronomy / Astrophysics,<br>Materials Science (literature mining, materials discovery),<br>Mathematics (conjecture generation, theorem guidance),<br>Laboratory Automation and Robotics (automated experiments and discovery) |

---


### [64. Into the Unknown: How Computation Can Help Explore Uncharted Material Space](https://doi.org/10.1021/jacs.2c06833), Journal of the American Chemical Society *(October 19, 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project,<br>NOMAD,<br>MPDS (Materials Platform for Data Science),<br>NREL MatDB (Computational Science Center - Materials Database),<br>HOIP combinatorial dataset (Lu et al.),<br>Zeolite synthesis dataset (Jensen et al.),<br>Generated reaction dataset (Tempke & Musho / VAE example),<br>Robot photocatalyst experiment dataset (Burger et al.),<br>Dataset of porous/rigid amorphous materials |
| **Models** | Variational Autoencoder,<br>Generative Adversarial Network,<br>Recurrent Neural Network,<br>Feedforward Neural Network,<br>Machine Learning ForceFields |
| **Tasks** | Regression,<br>Sequence-to-Sequence,<br>Synthetic Data Generation,<br>Graph Generation,<br>Optimization,<br>Experimental Design,<br>Data Generation |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Transfer Learning,<br>Reinforcement Learning,<br>Active Learning |
| **Performance Highlights** | generated_samples_count: 7,000,000,<br>robot_experiments: 688 experiments,<br>improvement_factor: 6x more active formulations |
| **Application Domains** | materials discovery,<br>inorganic materials (including perovskites, cathode materials),<br>organic materials and porous organic cages,<br>zeolites / porous crystalline materials,<br>polymeric / amorphous materials (membranes),<br>photocatalysis (hydrogen production),<br>high-throughput experimental robotics / autonomous experimentation,<br>synthesis route prediction / retrosynthesis |

---


### [63. The endless search for better alloys](https://doi.org/10.1126/science.ade5503), Science *(October 07, 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | published data of ~700 alloys,<br>DFT-calculated physical properties,<br>thermodynamic databases,<br>experimental feedback dataset (measured properties from synthesized candidates),<br>generated candidate compositions (top 1000 candidates),<br>search space of compositions |
| **Models** | Multi-Layer Perceptron |
| **Tasks** | Regression,<br>Optimization,<br>Experimental Design,<br>Feature Selection |
| **Learning Methods** | Supervised Learning,<br>Active Learning |
| **Performance Highlights** | training_dataset_size: ~700 alloys,<br>discovered_invar_heas_count: 17,<br>lowest_thermal_expansion_coefficient_found: ~2.3e-6 K^-1,<br>prior_HEA_record_thermal_expansion_coefficient: ~1e-5 K^-1,<br>search_space_size: millions of compositions,<br>candidate_pool_size_for_selection: 1000 (generated candidates),<br>iteration_count: 6 cycles,<br>synthesized_candidates_per_cycle: top 3 candidates synthesized,<br>final_discoveries: 17 Invar HEAs discovered |
| **Application Domains** | Materials Science,<br>Alloy design / High-Entropy Alloys (HEAs),<br>Computational materials discovery,<br>Experimental materials synthesis and characterization |

---


### [62. An artificial intelligence enabled chemical synthesis robot for exploration and optimization of nanomaterials](https://doi.org/10.1126/sciadv.abo2626), Science Advances *(October 07, 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | Simulated chemical space (extinction spectrum simulation),<br>Experimental UV-Vis spectral datasets from three hierarchically linked chemical spaces,<br>Experimental TEM image dataset (secondary characterization),<br>Multistep synthesis experimental repeats dataset (directed graph multistep runs) |
| **Models** | _None_ |
| **Tasks** | Optimization,<br>Experimental Design,<br>Data Generation,<br>Hyperparameter Optimization,<br>Search / Discovery (Exploration) |
| **Learning Methods** | Evolutionary Learning |
| **Performance Highlights** | discovery_efficiency_vs_random_search: exploration algorithm found samples belonging to all classes after 78 steps while random search did not find all classes after 200 steps (16 repeats),<br>average_fitness_reached: average fitness of the highest performance samples from different classes eventually reaches 98% of the estimated maximum (simulated benchmark),<br>nanorod_yield_before_optimization: ca. 57%,<br>nanorod_yield_after_optimization: ca. 95%,<br>optimization_runs: 5 steps (115 reactions) per optimization campaign,<br>multisolution_finding: multiple synthetic conditions found corresponding to different morphologies with high spectral similarity (e.g., octahedral, concave octahedral, smooth polyhedral),<br>target_generation: used to create target spectra from 3D nanostructures derived from electron micrographs; enabled optimization toward targets not directly found in exploration,<br>simulation_implementation: GPU-accelerated discrete-dipole approximation implemented in PyDScat-GPU (TensorFlow 2) |
| **Application Domains** | Nanomaterials discovery,<br>Chemical synthesis automation,<br>Materials science,<br>Spectroscopic characterization (UV-Vis),<br>Robotics for laboratory automation,<br>Computational simulation of optical spectra (discrete-dipole approximation) |

---


### [61. Machine learning–enabled high-entropy alloy discovery](https://doi.org/10.1126/science.abo4940), Science *(October 07, 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | Invar database (benchmark dataset of Invar alloys),<br>Experimental dataset produced in this work |
| **Models** | Autoencoder,<br>Gaussian Mixture Model,<br>Gaussian Mixture Model,<br>Multi-Layer Perceptron,<br>Gradient Boosting Tree,<br>Ensemble Learning,<br>Gaussian Process |
| **Tasks** | Data Generation,<br>Regression,<br>Experimental Design,<br>Optimization,<br>Representation Learning,<br>Feature Extraction |
| **Learning Methods** | Active Learning,<br>Unsupervised Learning,<br>Supervised Learning,<br>Representation Learning,<br>Ensemble Learning,<br>Feature Extraction |
| **Performance Highlights** | testing_error_without_physics: 0.19,<br>testing_error_with_physics: 0.14,<br>MAPE_initial_to_three_iterations_FeNiCoCr: 1.5 -> 0.2,<br>discovery_rate_vs_trial_and_error: 5x (fivefold higher discovery rate than trial-and-error, table S2),<br>experimental_alloys_validated: 17 new alloys measured; 2 alloys discovered with TEC ~2 × 10^-6/K at 300 K,<br>representative_experimental_TEC_values: A3 experimental TEC = 1.41 × 10^-6/K; A9 = 2.02 × 10^-6/K; B2 = 4.38 × 10^-6/K; B4 = 4.94 × 10^-6/K |
| **Application Domains** | metallurgy,<br>materials science,<br>high-entropy alloy discovery,<br>alloy design (Invar and Kovar alloys),<br>computational materials discovery / AI for materials |

---


### [60. Autonomous optimization of non-aqueous Li-ion battery electrolytes via robotic experimentation and machine learning coupling](https://doi.org/10.1038/s41467-022-32938-1), Nature Communications *(September 27, 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | Clio experimental dataset (autonomous electrolyte measurements),<br>AEM-derived simulated dataset / design-space grid,<br>Baseline electrolyte measurements |
| **Models** | Gaussian Process |
| **Tasks** | Optimization,<br>Regression,<br>Experimental Design |
| **Learning Methods** | Supervised Learning,<br>Maximum Likelihood Estimation |
| **Performance Highlights** | conductivity_optimum_mS_per_cm: 13.7,<br>conductivity_optimum_conditions: EC:DMC 40:60 by mass, 0.9 m LiPF6 (measured at 26–28 °C),<br>number_of_experiments: 42 (live autonomous campaign),<br>mean_repeatability_error_conductivity_percent: ±1.3%,<br>95%_CI_repeatability_percent: ±3.8%,<br>enhancement_factor_EF_upper_bound_percent: 5,<br>enhancement_factor_EF_trend_over_40_samples_percent: 2.5,<br>acceleration_factor_AF_at_optimum_average: 10x,<br>acceleration_factor_AF_for_98.5%_of_maximum_average: 6x,<br>acceleration_factor_AF_range_at_98.5%: 4.5x–11.5x,<br>overall_time_acceleration_compared_to_random_search: 6x (reported overall acceleration for the work flow),<br>electrolytes_discovered: 6 high-performing electrolytes identified in two work-days,<br>pouch_cell_discharge_capacity_improvement_worst_percent: 5,<br>pouch_cell_discharge_capacity_improvement_best_percent: 13 |
| **Application Domains** | Battery research / Li-ion electrolytes,<br>Electrochemistry,<br>Materials discovery,<br>Automated experimentation / self-driving laboratory,<br>Energy storage / fast-charging battery development |

---


### [59. Data-Driven Materials Innovation and Applications](https://doi.org/10.1002/adma.202104113), Advanced Materials *(September 08, 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | Open Quantum Materials Database (OQMD),<br>Materials Project (MP),<br>Automatic-FLOW (AFLOW) / AFLOWLIB,<br>NOMAD (Novel Materials Discovery Laboratory),<br>Computational Materials Repository (CMR) / C2DB,<br>Inorganic Crystal Structure Database (ICSD),<br>Cambridge Structural Database (CSD),<br>Crystallography Open Database (COD),<br>OQMD / MP combined datasets used in case studies,<br>SuperCon database,<br>JARVIS-DFT / JARVIS database,<br>Custom experimental datasets (literature-collected) used across application studies |
| **Models** | Linear Model,<br>Support Vector Machine,<br>Decision Tree,<br>Random Forest,<br>Gradient Boosting Tree,<br>K-Nearest Neighbor,<br>Gaussian Process,<br>Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>Recurrent Neural Network,<br>Graph Neural Network,<br>Generative Adversarial Network,<br>Variational Autoencoder,<br>Restricted Boltzmann Machine,<br>Ensemble Methods (Bagging/Boosting/AdaBoost),<br>Genetic Algorithm |
| **Tasks** | Regression,<br>Classification,<br>Binary Classification,<br>Multi-class Classification,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Extraction,<br>Image Classification,<br>Object Detection,<br>Synthetic Data Generation,<br>Graph Generation,<br>Hyperparameter Optimization,<br>Ranking,<br>Data Generation |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Deep Learning,<br>Active Learning,<br>Transfer Learning,<br>Bayesian Optimization,<br>Reinforcement Learning,<br>Evolutionary Learning,<br>Ensemble Learning,<br>Active Learning |
| **Performance Highlights** | RMSE: 0.09 eV,<br>Efrozen_RMSE_train: 0.06 eV,<br>Efrozen_RMSE_test: 0.11 eV,<br>Erelax_train_RMSE: 0.05 eV,<br>Erelax_test_RMSE: 0.10 eV,<br>Pearson_coefficient: 0.79,<br>RMSE_PCE: 1.07% (PCE units),<br>Coverage_calculation_MAE: 0.07 eV,<br>Coverage_calculation_RMSE: 0.10 eV,<br>Coverage_calculation_R2: 0.93,<br>OER_MAE: 0.13 eV,<br>OER_RMSE: 0.18 eV,<br>OER_R2: 0.8,<br>RMSE_bandgap: 0.283 eV,<br>R2: 0.957,<br>R2_bandgap: 0.97,<br>RMSE_bandgap: 0.086 eV,<br>MAE_ΔECO: 0.1 eV,<br>MAD_ΔECO: 0.1 eV,<br>MAE_ΔE_CO_HEA: 0.046 eV,<br>MAE_ΔE_H_HEA: 0.048 eV,<br>RMSE_ΔG_OH: 0.036 eV,<br>R2: 0.993,<br>cn*_MAE: 0.07%,<br>NRR_ΔG_MAE: 0.57 eV,<br>Voltage_MAE: 0.44 V,<br>R2_voltage: 0.86,<br>Elastic_constants_R2_C11: 0.60,<br>R2_C12: 0.79,<br>R2_C44: 0.60,<br>Thermoelectric_S_classification_AUC: 0.96,<br>Thermoelectric_powerfactor_AUC: 0.82,<br>Supercon_classifier_accuracy: 92%,<br>Capacitance_R2: 0.91,<br>Optoelectronics_MAE: 0.086,<br>R2: 0.835,<br>Glass_forming_ability_accuracy: 89%,<br>Dmax_R2: 0.8,<br>Dmax_MAE: 0.21 nm,<br>DeltaTx_MAE: 8.8 K,<br>2D_PV_classifier_accuracy: 1.0,<br>2D_PV_precision: 1.0,<br>2D_PV_recall: 1.0,<br>2D_PV_AUC: 1.0 |
| **Application Domains** | Energy conversion (water splitting: HER/OER/OWS),<br>Photovoltaics (perovskite, organic, metal-oxide, 2D PVs),<br>Carbon dioxide reduction reaction (CRR) catalysis,<br>Oxygen reduction reaction (ORR) / fuel cells & metal-air batteries,<br>Nitrogen reduction reaction (NRR) electrocatalysis,<br>Thermoelectric materials discovery,<br>Piezoelectric materials / electrostrains,<br>Rechargeable alkali-ion batteries (electrolytes and electrodes),<br>Supercapacitors,<br>Environmental decontamination (advanced oxidation / PEC processes),<br>Flexible electronics (composite/process optimization),<br>Optoelectronics (2D octahedral oxyhalides),<br>Superconductors (T_c prediction & screening),<br>Metallic glasses (glass-forming ability, Dmax, ΔT_x),<br>Magnetic materials (Curie temperature prediction, permanent/soft magnets),<br>Materials thermodynamic stability and phase prediction,<br>High-throughput computational materials science / databases / workflow automation |

---


### [58. Imaging and computing with disorder](https://doi.org/10.1038/s41567-022-01681-1), Nature Physics *(September 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | DiffuserCam compressive 3D imaging (1.3 megapixel sensor → 100 million voxels),<br>Transmission of natural scene images through a multimode fibre (experimental dataset),<br>Large-scale optical reservoir computing dataset for spatiotemporal chaotic systems prediction,<br>Classification of time-domain waveforms using a speckle-based optical reservoir computer (experimental waveform dataset),<br>Multimode-fibre non-linear classification experiments (dataset used to demonstrate improved classification with nonlinearity),<br>Speckle-based spectrometer spectral-response datasets |
| **Models** | Perceptron,<br>Convolutional Neural Network,<br>Support Vector Machine,<br>Linear Model,<br>Recurrent Neural Network,<br>Feedforward Neural Network,<br>Graph Neural Network |
| **Tasks** | Image Classification,<br>Regression,<br>Time Series Forecasting,<br>Object Localization,<br>Image Super-Resolution,<br>Image-to-Image Translation,<br>Classification |
| **Learning Methods** | Supervised Learning,<br>Backpropagation,<br>End-to-End Learning,<br>Representation Learning |
| **Performance Highlights** | accuracy_comparison: nonlinear multimode fibre classification accuracy > single-layer neural network accuracy |
| **Application Domains** | Optics / Photonics / Computational imaging,<br>Acoustics,<br>Radiofrequency communications / Wireless,<br>Seismic waves,<br>Ultrasound imaging,<br>Neuroscience / Functional brain imaging,<br>Quantum information,<br>Neuromorphic computing / hardware |

---


### [57. Deep-learning seismology](https://doi.org/10.1126/science.abm4470), Science *(August 12, 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | STanford EArthquake Dataset (STEAD),<br>Marmousi2,<br>BP2004,<br>INSTANCE (Italian seismic dataset for machine learning),<br>Authors' meta-analysis database of published papers,<br>Synthetic and semisynthetic seismic datasets (generated by simulations or GANs),<br>Competition and benchmarking datasets (richterx, AETA, other community benchmarks) |
| **Models** | Convolutional Neural Network,<br>Recurrent Neural Network,<br>Long Short-Term Memory,<br>Transformer,<br>Attention Mechanism,<br>Autoencoder,<br>Generative Adversarial Network,<br>Capsule Network,<br>U-Net,<br>Graph Neural Network,<br>Physics-Informed Neural Networks,<br>Variational Autoencoder,<br>Multi-Layer Perceptron |
| **Tasks** | Classification,<br>Binary Classification,<br>Multi-class Classification,<br>Sequence Labeling,<br>Time Series Forecasting,<br>Clustering,<br>Image Denoising,<br>Image Inpainting,<br>Image Super-Resolution,<br>Semantic Segmentation,<br>Object Detection,<br>Regression,<br>Clustering,<br>Node Classification,<br>Optimization,<br>Synthetic Data Generation |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Semi-Supervised Learning,<br>Transfer Learning,<br>Reinforcement Learning,<br>Fine-Tuning,<br>Pre-training,<br>Ensemble Learning,<br>Federated Learning,<br>Variational Inference |
| **Performance Highlights** | speedup: more than 100x,<br>computation_time_reduction: at least an order of magnitude |
| **Application Domains** | Seismology (earthquake monitoring and seismic imaging),<br>Subsurface characterization (velocity inversion, reservoir properties),<br>Earthquake early warning (EEW) and ground motion prediction,<br>Seismic data denoising and processing (active and passive seismic),<br>Exploratory data analysis of continuous seismic records (e.g., landslides, Mars seismicity, urban noise),<br>Unconventional sensing domains (distributed acoustic sensing, fiber-optic sensing, MEMS accelerometers),<br>Gravitational-wave detection and denoising (LIGO applications),<br>Monitoring moving objects and biosensing (human footsteps, vehicle classification, wildlife detection),<br>Benchmarking and community datasets / AI-for-science infrastructure |

---


### [56. Machine learning in the search for new fundamental physics](https://doi.org/10.1038/s42254-022-00455-1), Nature Reviews Physics *(June 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | ATLAS/CMS LHC collision data (example: 139 fb^-1),<br>LHC trigger data stream (raw collisions),<br>LHCb high-rate data (30 MHz software trigger studies),<br>MicroBooNE data / LArTPC image data,<br>DUNE (simulation and prototyping data),<br>PILArNet,<br>IceCube event data (irregular geometry),<br>LUX dataset (dark matter direct detection),<br>XENON1T dataset,<br>EXO-200 dataset,<br>DarkSide-50 / DarkSide-20k data / prototypes,<br>NEXT / nEXO / PandaX-III  (simulated and experimental TPC data),<br>LHC Olympics and Dark Machines challenge datasets,<br>ATLAS/CMS simulated Monte Carlo datasets |
| **Models** | Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>U-Net,<br>Recurrent Neural Network,<br>Long Short-Term Memory,<br>Graph Neural Network,<br>Autoencoder,<br>Variational Autoencoder,<br>Generative Adversarial Network,<br>Normalizing Flow,<br>Gradient Boosting Tree,<br>XGBoost,<br>Support Vector Machine,<br>Decision Tree,<br>Feedforward Neural Network,<br>Bayesian Network |
| **Tasks** | Classification,<br>Binary Classification,<br>Multi-class Classification,<br>Regression,<br>Image Classification,<br>Semantic Segmentation,<br>Instance Segmentation,<br>Anomaly Detection,<br>Density Estimation,<br>Data Generation,<br>Clustering,<br>Feature Extraction,<br>Time Series Forecasting,<br>Graph Classification,<br>Node Classification,<br>Image Generation |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Semi-Supervised Learning,<br>Weakly Supervised Learning,<br>Self-Supervised Learning,<br>Domain Adaptation,<br>Adversarial Training,<br>Contrastive Learning,<br>Transfer Learning,<br>End-to-End Learning,<br>Representation Learning,<br>Ensemble Learning,<br>Inference-aware Learning / Likelihood-aware Optimization |
| **Performance Highlights** | light-flavour false-positive rate: 1/390,<br>true positive rate (b-jet efficiency): 70% |
| **Application Domains** | High-energy particle physics (collider experiments, e.g., LHC: ATLAS, CMS, LHCb),<br>Neutrino experiments (LArTPCs: DUNE, MicroBooNE, SBND, NOvA, KamLAND-Zen, MINERvA, IceCube, KM3NeT/ORCA),<br>Rare event searches (dark matter direct detection, neutrinoless double beta decay: LUX, XENON1T, EXO-200, DarkSide, NEXT, nEXO, PandaX-III, PICO, CRESST),<br>Astrophysics / Cosmology (mentioned as related domains: Gaia, LIGO, Vera C. Rubin Observatory, Square Kilometer Array),<br>Hardware and systems for ML acceleration (FPGAs, GPUs) in scientific triggers and online processing |

---


### [55. Enhancing computational fluid dynamics with machine learning](https://doi.org/10.1038/s43588-022-00264-7), Nature Computational Science *(June 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | two-dimensional Kolmogorov flow,<br>plume configuration,<br>turbulent channel flow at Re_tau = 180,<br>filtered DNS of decaying homogeneous isotropic turbulence,<br>Kraichnan turbulence (2D decaying turbulence),<br>Taylor–Green vortex,<br>reacting and non-reacting finite-volume CFD high-resolution data,<br>particle simulations / PN junction (Poisson solver examples cited),<br>real urban geometry (LES case),<br>flow past a cylinder (canonical ROM example) |
| **Models** | Convolutional Neural Network,<br>Long Short-Term Memory,<br>Multi-Layer Perceptron,<br>Autoencoder,<br>U-Net,<br>Generative Adversarial Network,<br>Feedforward Neural Network,<br>Random Forest,<br>Gaussian Process,<br>Recurrent Neural Network |
| **Tasks** | Image Super-Resolution,<br>Regression,<br>Time Series Forecasting,<br>Dimensionality Reduction,<br>Clustering,<br>Optimization,<br>Policy Learning,<br>Feature Extraction,<br>Image-to-Image Translation |
| **Learning Methods** | Supervised Learning,<br>Reinforcement Learning,<br>Multi-Agent Learning,<br>Transfer Learning,<br>Bayesian Optimization,<br>Unsupervised Learning,<br>Physics-Informed (methodology mentioned: PINNs / physics-informed networks),<br>Representation Learning |
| **Performance Highlights** | accuracy_vs_Jacobi_solver: outperformed Jacobi solver at low Richardson numbers (Ri),<br>degradation_high_Ri: accuracy degrades at higher Ri,<br>mesh_coarsening_factor: 8-10x coarser in each dimension,<br>agreement: excellent agreement with reference simulations,<br>error: below 10% (for decomposition approach),<br>contextual_note: used as first guess in iterative algorithms,<br>turbulence_statistics: maintained turbulence for an interval long enough to obtain converged turbulence statistics,<br>computational_cost: ≈0.1x (one-tenth) the computational cost compared to reference high-resolution simulation,<br>accuracy_trend: very good agreement with reference high-resolution data initially; errors increase with time,<br>relative_performance: outperforms traditional RANS models based on linear and nonlinear eddy-viscosity models,<br>relative_performance: obtained better results than classical algebraic models,<br>convergence_speed_or_accuracy: GEP outperformed standard LES models in a LES of a Taylor–Green vortex (as reported),<br>generalization: favorable generalization properties across grid sizes and flow conditions (reported qualitatively),<br>accuracy: achieving accurate results when imposing concrete pressure-gradient conditions on a turbulent boundary layer,<br>convergence_speedup: 1.9 to 7.4 times faster convergence than CFD solver alone |
| **Application Domains** | computational fluid dynamics (CFD),<br>turbulence modeling (RANS and LES),<br>direct numerical simulation (DNS) acceleration,<br>reduced-order modeling (ROM),<br>aerodynamics / aerodynamic optimization,<br>atmospheric boundary layers / weather and climate,<br>combustion (premixed turbulent combustion),<br>multiphase flows (bubble/gas-liquid flows),<br>astrophysics (simulation acceleration use cases),<br>biomedical simulations (PINN applications),<br>high-performance computing (HPC) acceleration |

---


### [54. Dataset of solution-based inorganic materials synthesis procedures extracted from the scientific literature](https://doi.org/10.1038/s41597-022-01317-2), Scientific Data *(May 25, 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | Dataset of solution-based inorganic materials synthesis procedures,<br>Full-text paragraph corpus for BERT pretraining,<br>Paragraph-level labeled dataset for paragraph classification,<br>Annotated materials entity dataset (MER),<br>Solution-based synthesis paragraph corpus (for extraction pipeline) |
| **Models** | BERT,<br>Bidirectional LSTM,<br>Conditional Random Field,<br>Seq2Seq,<br>Recurrent Neural Network |
| **Tasks** | Text Classification,<br>Named Entity Recognition,<br>Sequence Labeling,<br>Structured Prediction,<br>Information Retrieval |
| **Learning Methods** | Self-Supervised Learning,<br>Pre-training,<br>Fine-Tuning,<br>Supervised Learning,<br>Sequence-to-Sequence |
| **Performance Highlights** | F1: 99.5%,<br>precursors_precision: 0.98,<br>precursors_recall: 0.99,<br>precursors_F1: 0.98,<br>targets_precision: 0.97,<br>Operations_precision: 0.96,<br>Operations_recall: 0.85,<br>Operations_F1: 0.90,<br>Balanced_reactions_precision: 0.94,<br>temperature_F1: 0.94,<br>time_F1: 0.93,<br>atmosphere_F1: 0.94,<br>Quantities_precision: 0.90,<br>Quantities_recall: 0.85,<br>Quantities_F1: 0.87,<br>paragraph_classification_F1: 99.5%,<br>overall_extraction_yield: ~15% (28,749 reactions from 189,553 solution-based paragraphs) |
| **Application Domains** | Materials science,<br>Inorganic materials synthesis,<br>Scientific text mining / Natural language processing for materials literature,<br>Automated extraction for dataset creation and data-driven synthesis planning |

---


### [53. High-entropy nanoparticles: Synthesis-structure-property relationships and data-driven discovery](https://doi.org/10.1126/science.abn3103), Science *(April 08, 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Data Bank (3D atomic coordinates and chemical species),<br>Combinatorial thin-film materials libraries (~342 compositions per batch),<br>Combinatorial HEA nanoparticle microelectrode library (64 cavities),<br>High-throughput computational screening datasets (empirical/CALPHAD/DFT-based),<br>Simulation-generated datasets for ML (first-principles data integrated ML) |
| **Models** | Gaussian Process,<br>Multi-Layer Perceptron,<br>Graph Neural Network |
| **Tasks** | Regression,<br>Classification,<br>Optimization,<br>Distribution Estimation,<br>Ranking |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Reinforcement Learning |
| **Performance Highlights** | iterations: ~150 iterations |
| **Application Domains** | Catalysis (thermocatalysis and electrocatalysis),<br>Energy technologies (hydrogen evolution, ammonia synthesis, CO2 reduction, fuel cells),<br>Materials discovery and design (high-entropy alloys, oxides, sulfides, carbides, MXenes),<br>Nanomaterials synthesis and characterization,<br>High-throughput experimentation and closed-loop optimization |

---


### [52. Distributed representations of atoms and materials for machine learning](https://doi.org/10.1038/s41524-022-00729-3), npj Computational Materials *(March 18, 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project structures (used to train SkipAtom),<br>Elpasolite formation energy dataset,<br>OQMD (Open Quantum Materials Database) Formation Energy dataset,<br>Matbench test-suite datasets (benchmark tasks),<br>Mat2Vec corpus (materials science literature),<br>Atom2Vec embeddings / dataset (co-occurrence matrix),<br>Processed data & scripts for this study (repository) |
| **Models** | Feedforward Neural Network,<br>Graph Neural Network,<br>Multi-Layer Perceptron |
| **Tasks** | Regression,<br>Binary Classification,<br>Feature Extraction |
| **Learning Methods** | Unsupervised Learning,<br>Supervised Learning,<br>Maximum Likelihood Estimation,<br>Stochastic Gradient Descent,<br>Mini-Batch Learning,<br>Representation Learning,<br>Pre-training |
| **Performance Highlights** | MAE (eV/atom) - Elpasolite (SkipAtom 30 dim): 0.1183 ± 0.0050,<br>MAE (eV/atom) - Elpasolite (SkipAtom 86 dim): 0.1126 ± 0.0078,<br>MAE (eV/atom) - Elpasolite (SkipAtom 200 dim): 0.1089 ± 0.0061,<br>MAE (eV/atom) - OQMD Formation Energy (Bag-of-Atoms one-hot, sum pooled, 86 dim): 0.0388 ± 0.0002,<br>MAE (eV/atom) - OQMD Formation Energy (Atom2Vec 86, sum): 0.0396 ± 0.0004,<br>MAE (eV/atom) - OQMD Formation Energy (SkipAtom 86, sum): 0.0420 ± 0.0005,<br>MAE (eV/atom) - OQMD Formation Energy (Mat2Vec 200, sum): 0.0401 ± 0.0004,<br>Benchmark summary (qualitative): Pooled Mat2Vec achieved best results in 4 of 8 benchmark tasks; pooled SkipAtom best in 2 of 8. 200-dim representations generally outperform 86-dim. Sum- and mean-pooling outperform max-pooling.,<br>Qualitative improvement over existing benchmarks: Authors report outperforming existing benchmarks on tasks where only composition is available (Experimental Band Gap, Bulk Metallic Glass Formation, Experimental Metallicity) — see Fig. 5 for comparisons. |
| **Application Domains** | Materials science / materials informatics,<br>Computational materials / inorganic crystals,<br>DFT property prediction (formation energy, band gap),<br>High-throughput materials screening,<br>Chemical composition-based property prediction |

---


### [50. A self-driving laboratory advances the Pareto front for material properties](https://doi.org/10.1038/s41467-022-28580-6), Nature Communications *(February 22, 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | Combustion synthesis experiments (Ada self-driving laboratory),<br>Initialization experiments,<br>Spray-coated validation samples,<br>Sputtered palladium reference samples (calibration),<br>XRF hyperspectral maps,<br>Simulated response surface (for benchmarking) |
| **Models** | Gaussian Process |
| **Tasks** | Optimization,<br>Regression,<br>Experimental Design,<br>Regression |
| **Learning Methods** | Supervised Learning,<br>Active Learning |
| **Performance Highlights** | LOOCV residuals_vs_experimental_uncertainty: comparable (no numeric exact values provided in main text),<br>assigned_uncertainty_for_conductivity_points: 20% of point value (used in heteroskedastic GP),<br>samples_to_outperform_random_search_in_noise_free_scenario: qEHVI required <100 samples to outperform 10,000 random samples,<br>comparison_metrics_used: hypervolume, acceleration factor (AF), enhancement factor (EF),<br>total_physical_experiments_run: 253 experiments,<br>Pareto_front_discovery: four replicate campaigns each generated clear Pareto fronts; specific Pareto-optimal synthesis conditions identified (see main text and Fig. 3) |
| **Application Domains** | Materials science,<br>Thin-film synthesis and characterization (palladium films),<br>Self-driving laboratories / autonomous experimentation,<br>Materials discovery and optimization (multi-objective),<br>Scalable deposition / spray combustion synthesis,<br>Experimental design and automation |

---


### [49. Density of states prediction for materials discovery via contrastive learning from probabilistic embeddings](https://doi.org/10.1038/s41467-022-28543-x), Nature Communications *(February 17, 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | phDOS dataset (from ref.34, replicated from ref.14),<br>Materials Project eDOS dataset (version 2021-03-22),<br>Materials Project structures without eDOS (candidates),<br>DFT-computed validation set for candidate VB-gap materials |
| **Models** | Graph Neural Network,<br>Graph Attention Network,<br>Multi-Layer Perceptron,<br>Attention Mechanism,<br>Multi-Head Attention,<br>Variational Autoencoder |
| **Tasks** | Regression,<br>Distribution Estimation,<br>Binary Classification |
| **Learning Methods** | Supervised Learning,<br>Contrastive Learning,<br>Variational Inference,<br>Representation Learning,<br>Fine-Tuning,<br>End-to-End Learning |
| **Performance Highlights** | R2: 0.62,<br>MAE: 0.078,<br>MSE: 0.023,<br>WD: 24,<br>phDOS_CV_MAE: 1.96,<br>phDOS_CV_MSE: 11,<br>phDOS_avgPhonon_MAE: 17.1,<br>phDOS_avgPhonon_MSE: 625,<br>R2: 0.57,<br>MAE: 0.085,<br>MSE: 0.026,<br>WD: 21,<br>phDOS_CV_MAE: 1.32,<br>phDOS_CV_MSE: 10,<br>phDOS_avgPhonon_MAE: 10.6,<br>phDOS_avgPhonon_MSE: 348,<br>R2: 0.63,<br>MAE: 0.086,<br>MSE: 0.029,<br>WD: 33,<br>phDOS_CV_MAE: 3.3,<br>phDOS_CV_MSE: 49,<br>phDOS_avgPhonon_MAE: 26.2,<br>phDOS_avgPhonon_MSE: 2284,<br>R2: 0.57,<br>MAE: 3.8,<br>MSE: 74.5,<br>WD: 0.21,<br>VB_gap_identification_F1: 0.397,<br>VB_gap_identification_Precision: 0.698,<br>VB_gap_identification_Recall: 0.278,<br>VB_gap_discovery_Precision_on_100: 0.47,<br>VB_gap_discovery_Recall_on_100: 1.0,<br>R2: 0.53,<br>MAE: 3.64,<br>MSE: 80.4,<br>WD: 0.27,<br>VB_gap_identification_F1: 0.352,<br>VB_gap_identification_Precision: 0.509,<br>VB_gap_identification_Recall: 0.269,<br>VB_gap_discovery_Precision_on_100: 0.67,<br>VB_gap_discovery_Recall_on_100: 0.27,<br>R2: 0.45,<br>MAE: 0.105,<br>MSE: 0.042,<br>WD: 44,<br>phDOS_CV_MAE: 4.66,<br>phDOS_CV_MSE: 80,<br>phDOS_avgPhonon_MAE: 35.1,<br>phDOS_avgPhonon_MSE: 3392,<br>R2: 0.3,<br>MAE: 4.89,<br>MSE: 120.9,<br>WD: 0.42,<br>VB_gap_identification_F1: 0.182,<br>VB_gap_identification_Precision: 0.263,<br>VB_gap_identification_Recall: 0.139,<br>VB_gap_discovery_Precision_on_100: 0.0,<br>VB_gap_discovery_Recall_on_100: 0.0 |
| **Application Domains** | Materials science,<br>Computational materials discovery,<br>Spectral property prediction (phonon density of states, electronic density of states),<br>Thermoelectric materials screening,<br>Transparent conductor discovery,<br>High-throughput materials screening |

---


### [48. Data-driven modeling and prediction of non-linearizable dynamics via spectral submanifolds](https://doi.org/10.1038/s41467-022-28518-y), Nature Communications *(February 15, 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | von Kármán beam finite-element simulation,<br>flow past a cylinder (vortex shedding) simulation dataset,<br>sloshing in a water tank (experimental) |
| **Models** | Linear Model,<br>Polynomial Model |
| **Tasks** | Dimensionality Reduction,<br>Feature Extraction,<br>Regression,<br>Time Series Forecasting |
| **Learning Methods** | Supervised Learning,<br>Representation Learning,<br>Feature Learning |
| **Performance Highlights** | NMTE: 0.027,<br>NMTE_initial_cubic: 1.17 (117%),<br>NMTE_high_order: 0.0386,<br>NMTE: 0.0188,<br>DMD_long_term_behavior: divergent / fails to capture limit cycle |
| **Application Domains** | structural dynamics / vibration analysis,<br>computational fluid dynamics / flow control,<br>experimental fluid mechanics (sloshing),<br>mechanical engineering (beam models, MEMS),<br>model reduction and dynamical systems analysis |

---


### [47. Innovative Materials Science via Machine Learning](https://doi.org/10.1002/adfm.202108044), Advanced Functional Materials *(February 03, 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | open quantum material database,<br>open inorganic material database,<br>crystallographic open database,<br>thermoelectric open data resource,<br>two-dimensional material database,<br>novel material discovery database,<br>high-throughput combination database of electronic band structure for inorganic scintillator materials,<br>inorganic amorphous database,<br>inorganic crystal structure database (used for XRD patterns),<br>DFT-computational datasets (large DFT-computational data sets and smaller DFT-computed data sets),<br>hypothetical zeolites dataset,<br>small stainless steels dataset (defects and solidification cracking susceptibility) |
| **Models** | Convolutional Neural Network,<br>Graph Neural Network,<br>Radial Basis Function Network,<br>Multi-Layer Perceptron,<br>Decision Tree,<br>Support Vector Machine,<br>K-Nearest Neighbors,<br>AdaBoost,<br>Naive Bayes,<br>Recurrent Neural Network,<br>Feedforward Neural Network,<br>Graph Convolutional Network,<br>Generative Adversarial Network |
| **Tasks** | Regression,<br>Classification,<br>Multi-class Classification,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Selection,<br>Optimization,<br>Anomaly Detection,<br>Image Classification,<br>Feature Extraction |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Reinforcement Learning,<br>Adversarial Training,<br>Transfer Learning,<br>Pre-training,<br>Ensemble Learning,<br>Few-Shot Learning,<br>Zero-Shot Learning,<br>One-Shot Learning,<br>Transfer Learning |
| **Performance Highlights** | RMSE_reduction: over 9% |
| **Application Domains** | materials science,<br>materials discovery,<br>energy materials (e.g., thermoelectrics, battery materials),<br>solid-state materials,<br>catalysis and electrocatalysis,<br>photocatalysis,<br>alloys and metallurgy (stainless steels),<br>polymer materials,<br>semiconductor materials,<br>biocompatible / bio-related materials |

---


### [46. An invertible crystallographic representation for general inverse design of inorganic crystals with targeted properties](https://doi.org/10.1016/j.matt.2021.11.032), Matter *(January 05, 2022)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project (queried Nov 2019),<br>Ab initio electronic transport database (Ricci et al., reference 1; used via BoltzTraP calculations),<br>Inorganic Crystal Structure Database (ICSD) cross-reference |
| **Models** | Variational Autoencoder,<br>Convolutional Neural Network,<br>Feedforward Neural Network,<br>Graph Convolutional Network |
| **Tasks** | Regression,<br>Data Generation,<br>Synthetic Data Generation,<br>Representation Learning,<br>Optimization |
| **Learning Methods** | Semi-Supervised Learning,<br>Supervised Learning,<br>Representation Learning,<br>Variational Inference,<br>Backpropagation |
| **Performance Highlights** | case1_Ef=-0.5_validity_rate: 77.8% (14/18),<br>case1_Ef=-0.5_success_rate: 38.9% (7/18),<br>case1_random_success_rate: 10.5% (2,781/26,402),<br>case1_improvement_over_random: 270%,<br>case2_validity_rate: 84.2% (16/19),<br>case2_success_rate: 36.8% (7/19),<br>case2_random_success_rate: 5.5% (3,035/54,925),<br>case2_improvement_over_random: 560%,<br>case3_validity_rate: 42.9% (12/28),<br>case3_success_rate: 7.1% (2/28),<br>case3_random_success_rate: not calculated (lack of complete power factor labels) |
| **Application Domains** | Inverse design of inorganic crystalline materials,<br>Materials discovery for photovoltaics and optoelectronics (bandgap-targeted design),<br>Thermoelectrics (design for high TE power factor),<br>General materials property-driven design and high-throughput computational materials science |

---


### [45. Applied Machine Learning for Developing Next-Generation Functional Materials](https://doi.org/10.1002/adfm.202104195), Advanced Functional Materials *(December 16, 2021)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project,<br>OQMD,<br>AFLOW,<br>ICSD (Inorganic Crystal Structure Database) / experimentally known materials,<br>PbS quantum dots experimental dataset (digitized lab notebooks),<br>Homma ternary electrolyte experimental search,<br>Harada co-doped NASICON-type dataset,<br>Automated perovskite robotics (RAPID) dataset,<br>Organic photovoltaic device dataset (David et al.),<br>Battery cycle dataset (Severson et al.),<br>Electrochemical impedance spectroscopy (EIS) dataset (Dahn et al.),<br>Clustering dataset for solid-state Li-ion conductors (Zhang et al.),<br>Multi-fidelity bandgap datasets (Chen et al.),<br>Semiconducting materials in public databases,<br>Perovskite composition optimization images (well plate images),<br>DFT datasets for catalytic surfaces (Ulissi et al.),<br>MOF generation dataset used with variational autoencoder (Aspuru-Guzik et al.) |
| **Models** | Graph Convolutional Network,<br>Random Forest,<br>Mask R-CNN,<br>Convolutional Neural Network,<br>Variational Autoencoder,<br>Recurrent Neural Network,<br>Feedforward Neural Network,<br>Gaussian Process,<br>Gaussian Mixture Model,<br>Variational Autoencoder,<br>Generative Adversarial Network |
| **Tasks** | Regression,<br>Classification,<br>Clustering,<br>Image Classification,<br>Instance Segmentation,<br>Optimization,<br>Clustering,<br>Image Classification,<br>Regression,<br>Anomaly Detection,<br>Data Generation |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Reinforcement Learning,<br>Transfer Learning,<br>Few-Shot Learning,<br>Active Learning,<br>Representation Learning |
| **Performance Highlights** | formation_energy_MAE: 0.039 eV per atom,<br>bandgap_MAE: 0.388 eV,<br>bulk_modulus_MAE: 0.054 log(GPa),<br>EIS_failure_rate: <1% on ~100,000 spectra,<br>DFT_optimizations_needed: as few as 30 DFT optimizations to find most stable polymorphs (per cited study),<br>coverage_RMSE: 0.10 eV,<br>overpotential_RMSE: 0.18 eV,<br>MOF_CO2_capacity: 7.55 mol kg^-1,<br>selectivity_CO2/CH4: 16 (for top generated MOF),<br>initial_dataset_size: ≈3000 materials,<br>reduced_candidates: 82 materials,<br>predicted_high_conductors: 16 with room-temperature conductivity > 1e-4 S cm^-1 (3 with > 1e-2 S cm^-1),<br>candidates_found: 10 candidate double perovskites with OER performance better than LaCoO3 (as reported) |
| **Application Domains** | Batteries / solid-state electrolytes,<br>Electrocatalysis (OER, CO2 reduction, hydrogen evolution, nitrogen reduction),<br>Optoelectronics (photovoltaics, LEDs, light emitters),<br>Device fabrication and optimization (OLEDs, solar cells),<br>Materials synthesis (nanocrystals, perovskites, MOFs, COFs),<br>High-throughput experimentation and robotics-augmented discovery,<br>Spectroscopy and experimental data analysis (XRD, EIS, NMR, FTIR),<br>Quality control and manufacturing (defect detection in displays),<br>Generative materials design (MOFs, molecules) |

---


### [42. Automating crystal-structure phase mapping by combining deep learning with constraint reasoning](https://doi.org/10.1038/s42256-021-00384-1), Nature Machine Intelligence *(September 2021)*

| Category | Items |
|----------|-------|
| **Datasets** | Multi-MNIST-Sudoku (generated from MNIST & EMNIST),<br>MNIST (single-digit training set),<br>EMNIST (letters A–I used for second Sudoku),<br>Al–Li–Fe oxide synthetic benchmark (phase-mapping dataset with ground truth),<br>Bi–Cu–V oxide experimental dataset,<br>ICDD stick patterns (prototype XRD patterns) |
| **Models** | ResNet,<br>Multi-Layer Perceptron,<br>Conditional GAN,<br>Gaussian Mixture Model,<br>Gaussian Mixture Model,<br>Capsule Network,<br>ResNet,<br>Non-negative Matrix Factorization |
| **Tasks** | Image-to-Image Translation,<br>Image Generation,<br>Clustering,<br>Multi-label Classification,<br>Representation Learning,<br>Clustering |
| **Learning Methods** | Unsupervised Learning,<br>Self-Supervised Learning,<br>Adversarial Training,<br>Pre-training,<br>End-to-End Learning,<br>Stochastic Gradient Descent,<br>Representation Learning,<br>Supervised Learning |
| **Performance Highlights** | Sudoku accuracy (%): 99 (with 100 unlabelled 9x9 Multi-MNIST-Sudoku instances),<br>digit accuracy: not given numerically in main text but reported to be superior to supervised baselines in Extended Data,<br>reconstruction quality (qualitative): high (DRNets reconstruct mixed images closely; demonstrated in Fig. 1i),<br>Activation accuracy (%) (Al–Li–Fe): 100,<br>Reconstruction loss L1 (Al–Li–Fe): 0.038,<br>Reconstruction loss L2 (Al–Li–Fe): <0.001,<br>Gibbs (%) (Al–Li–Fe): 100,<br>Gibbs-alloy (%) (Al–Li–Fe): 100,<br>Phase field connectivity (%) (Al–Li–Fe): 100,<br>Fidelity loss (Al–Li–Fe): <0.001,<br>Reconstruction loss L1 (Bi–Cu–V): 3.916,<br>Reconstruction loss L2 (Bi–Cu–V): 0.268,<br>Gibbs (%) (Bi–Cu–V): 100,<br>Gibbs-alloy (%) (Bi–Cu–V): 100,<br>Phase field connectivity (%) (Bi–Cu–V): 100,<br>Fidelity loss (Bi–Cu–V): 0.482,<br>Activation accuracy (%) (Al–Li–Fe, k=6): 63.1,<br>Reconstruction loss L1 (Al–Li–Fe, k=6): 29.805,<br>Reconstruction loss L2 (Al–Li–Fe, k=6): 7.169,<br>Gibbs (%) (Al–Li–Fe, k=6): 93.9,<br>Gibbs-alloy (%) (Al–Li–Fe, k=6): 87.0,<br>Phase field connectivity (%) (Al–Li–Fe, k=6): 71.0,<br>Fidelity loss (Al–Li–Fe, k=6): 46.156,<br>relative performance: DRNets substantially outperform supervised CapsuleNet demixing baselines on Multi-MNIST-Sudoku (exact numbers in Extended Data Fig. 4),<br>Sudoku/digit accuracy (qualitative): DRNets better than CapsuleNet supervised baseline |
| **Application Domains** | materials science (crystal-structure phase mapping, XRD analysis),<br>computer vision / image reasoning (Multi-MNIST-Sudoku demixing),<br>solar fuels / photoelectrocatalysis (experimental follow-up and materials discovery),<br>scientific discovery workflows (integration of prior scientific knowledge and ML) |

---


### [41. Accurate prediction of protein structures and interactions using a three-track neural network](https://doi.org/10.1126/science.abj8754), Science *(August 20, 2021)*

| Category | Items |
|----------|-------|
| **Datasets** | Protein Data Bank (PDB),<br>CASP14 targets,<br>CAMEO medium and hard targets,<br>Curated set of 693 human protein domains,<br>GPCR benchmark (human GPCRs of currently unknown structure and GPCR sequences with determined structures),<br>Escherichia coli protein complexes (known structures),<br>Cryo-EM map EMD-21645 (IL-12R–IL-12 complex) |
| **Models** | Transformer,<br>Attention Mechanism,<br>Self-Attention Network,<br>Cross-Attention,<br>Multi-Head Attention,<br>Ensemble Learning |
| **Tasks** | Sequence-to-Sequence,<br>Regression,<br>Sequence-to-Sequence |
| **Learning Methods** | End-to-End Learning,<br>Backpropagation,<br>Supervised Learning,<br>Cross-Attention,<br>Stochastic Learning,<br>Ensemble Learning |
| **Performance Highlights** | qualitative: structure predictions with accuracies approaching those of DeepMind (AlphaFold2) on CASP14 targets,<br>runtime_end_to_end: ~10 min on an RTX2080 GPU for proteins with fewer than 400 residues (after sequence and template search),<br>lDDT_fraction: >33% of 693 modeled human domains have predicted lDDT > 0.8,<br>lDDT_to_Ca-RMSD: predicted lDDT > 0.8 corresponded to an average Cα-RMSD of 2.6 Å on CASP14 targets,<br>TM-score_complexes: many cases with TM-score > 0.8 for two- and three-chain complexes,<br>Cα-RMSD_examples: p101 GBD predicted vs final refined structure: Cα-RMSD = 3.0 Å over the beta-sheets,<br>improved_accuracy: Ensembles and using multiple discontinuous crops generated higher-accuracy models (qualitative improvement reported),<br>CAMEO_benchmark: RoseTTAFold outperformed all other servers on 69 CAMEO medium and hard targets (TM-score values used for ranking),<br>molecular_replacement_success: RoseTTAFold models enabled successful molecular replacement for four challenging crystallographic datasets that had previously eluded MR with PDB models,<br>example_Ca-RMSD_SLP: 95 Cα atoms superimposed within 3 Å yielding a Cα-RMSD of 0.98 Å for SLP C-terminal domain |
| **Application Domains** | Structural biology,<br>Protein structure prediction,<br>X-ray crystallography (molecular replacement),<br>Cryo-electron microscopy model building/fitting,<br>Protein-protein complex modeling,<br>Functional annotation of proteins / interpretation of disease mutations,<br>Protein design and small-molecule / binder design (computational discovery) |

---


### [40. Highly accurate protein structure prediction with AlphaFold](https://doi.org/10.1038/s41586-021-03819-2), Nature *(August 2021)*

| Category | Items |
|----------|-------|
| **Datasets** | Protein Data Bank (PDB) (training snapshot 28 Aug 2019),<br>CASP14 (Critical Assessment of protein Structure Prediction, CASP14),<br>Recent PDB test set (post-training cutoff),<br>Uniclust30 (v.2018_08),<br>Big Fantastic Database (BFD),<br>UniRef90 (v.2020_01),<br>MGnify clusters (v.2018_12),<br>Distillation predicted-structure dataset (from Uniclust predictions),<br>PDB subsets used in ablations / filtered analyses |
| **Models** | Transformer,<br>Attention Mechanism,<br>BERT,<br>Graph Neural Network,<br>Multi-Head Attention |
| **Tasks** | Regression,<br>Feature Extraction,<br>Representation Learning,<br>Classification |
| **Learning Methods** | Supervised Learning,<br>Self-Supervised Learning,<br>Knowledge Distillation,<br>Self-Training,<br>Fine-Tuning,<br>End-to-End Learning,<br>Ensemble Learning,<br>Representation Learning |
| **Performance Highlights** | median_backbone_r.m.s.d.95_on_CASP14_domains_(Cα_r.m.s.d.95): 0.96 Å (95% CI = 0.85–1.16 Å),<br>next_best_method_median_backbone_r.m.s.d.95: 2.8 Å (95% CI = 2.7–4.0 Å),<br>all-atom_r.m.s.d.95_AlphaFold: 1.5 Å (95% CI = 1.2–1.6 Å),<br>all-atom_r.m.s.d.95_best_alternative: 3.5 Å (95% CI = 3.1–4.2 Å),<br>median_backbone_r.m.s.d._recent_PDB_chains: 1.46 Å (95% CI = 1.40–1.56 Å),<br>qualitative_improvement: self-distillation considerably improves accuracy (as shown in ablations and Fig. 4a),<br>distillation_dataset_size: predicted structures for ~355,993 Uniclust sequences used for distillation,<br>ablation_effect_masked_MSA_head: ablations show removing auxiliary masked MSA head reduces performance (see Fig. 4a; exact numeric differences in supplementary material),<br>pLDDT_lDDT-Cα_correlation: least-squares fit lDDT-Cα = 0.997 × pLDDT − 1.17 (Pearson r = 0.76), n = 10,795 chains,<br>domain_GDT_improvements: AlphaFold outperforms alternatives on CASP14 domains (median metrics above); iterative recycling and Evoformer depth contribute to accuracy (see Fig. 4b),<br>Ablation_losses: ablations removing triangle updates/IPA/recycling reduce GDT and lDDT-Cα (Fig. 4a; numeric deltas in Supplementary Methods),<br>inference_time_with_ensembling_256_residues: 4.8 min (single model V100) in CASP14 config; ensembling adds overhead; without ensembling representative timings: 0.6 min for 256 residues,<br>ensemble_vs_single_accuracy: accuracy without ensembling is very close or equal to with ensembling (authors note turning off ensembling for speed),<br>pTM_TM-score_correlation: Least-squares linear fit TM-score = 0.98 × pTM + 0.07 (Pearson's r = 0.85), n = 10,795 chains |
| **Application Domains** | structural biology / protein structure prediction,<br>structural bioinformatics,<br>computational biology / proteomics (proteome-scale prediction),<br>structural interpretation for experimental methods (molecular replacement, cryo-EM map interpretation) |

---


### [39. Nanoparticle synthesis assisted by machine learning](https://doi.org/10.1038/s41578-021-00337-5), Nature Reviews Materials *(August 2021)*

| Category | Items |
|----------|-------|
| **Datasets** | CdSe combinatorial synthesis dataset (ref.110),<br>Autonomous microfluidic CdSe quantum dots optimization dataset (SNOBFIT study, ref.55),<br>CsPbBr3 SNOBFIT autonomous platform dataset (ref.56),<br>Lead halide perovskite nanocrystals automated segmented-flow reactor dataset (ref.118),<br>Ensemble neural-network Bayesian optimization dataset for halide exchange (ref.119,120),<br>Silver triangular nanoprism synthesis dataset (ref.137),<br>Literature-assembled metal nanoparticle dataset (illustrative Box 1),<br>Carbon-dot combinatorial dataset (tapioca-flour precursor study),<br>Gold nanocluster literature + new experiments dataset (hybrid NN study),<br>Carbon nanotube growth experimental dataset (autonomous system, ref.158),<br>Web of Science corpus for 'CdSe nanoparticle synthesis' (literature survey) |
| **Models** | Linear Model,<br>Decision Tree,<br>Random Forest,<br>Gradient Boosting Tree,<br>Gaussian Process,<br>Support Vector Machine,<br>Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>Recurrent Neural Network,<br>Feedforward Neural Network |
| **Tasks** | Regression,<br>Classification,<br>Optimization,<br>Multi-objective Optimization,<br>Active Learning,<br>Reinforcement Learning,<br>Clustering,<br>Dimensionality Reduction |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Reinforcement Learning,<br>Ensemble Learning,<br>Pre-training,<br>Unsupervised Learning |
| **Performance Highlights** | Pearson_correlation (wavelength, quantum yield, reaction yield): > 0.9,<br>Pearson_correlation (FWHM): > 0.7,<br>RMSE (predicted thickness of CsPbBr3 nanoplatelets): 0.7556 layers,<br>RMSE (linear model comparison): 1.0266 layers,<br>RMSE (quadratic model comparison): 0.8835 layers,<br>Illustrative_R2 (Box 1 example): 0.75,<br>Scalability_guideline: most suited for datasets < 1,000 samples (training cost scales ~K^3),<br>Optimization_progress (qualitative): Consistent decrease in objective Z with increasing number of experiments; robust model with low error and variance achieved within 20 iterations in some perovskite syntheses,<br>Loss (qualitative): Deep neural network produced significantly lower minimum loss than Bayesian optimization by run 8 (stated as significantly lower; no numeric values provided),<br>RMSE: 0.7556 layers,<br>Pearson_correlation (SPR position prediction): > 0.96 |
| **Application Domains** | Nanoparticle synthesis,<br>Inorganic semiconductor nanoparticles (quantum dots, perovskite nanocrystals),<br>Metal nanoparticles (gold, silver, palladium, copper, aluminium, nanoclusters),<br>Carbon-based nanoparticles (carbon dots, carbon nanotubes, graphene-based materials),<br>Polymeric nanoparticles (drug delivery, coatings),<br>Materials discovery and optimization,<br>Catalysis,<br>Photovoltaics / optoelectronics,<br>Biomedical imaging / diagnostics,<br>Autonomous laboratories / robotics / microfluidics integration |

---


### [38. Physics-informed machine learning](https://doi.org/10.1038/s42254-021-00314-5), Nature Reviews Physics *(June 2021)*

| Category | Items |
|----------|-------|
| **Datasets** | Harvard Clean Energy Project (HCEP) data set,<br>Tomo-BOS imaging data (espresso cup experiment),<br>4D-flow MRI (porcine descending aorta) data,<br>Synthetic 3D plasma measurements (electron density and temperature time series),<br>Instrumented indentation data + low-fidelity finite-element simulations,<br>Ab initio molecular simulation data used to train DeePMD,<br>UCI Machine Learning Repository subsets (examples) |
| **Models** | Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>ResNet,<br>Graph Neural Network,<br>Generative Adversarial Network,<br>Variational Autoencoder,<br>Restricted Boltzmann Machine,<br>Transformer,<br>Gaussian Process |
| **Tasks** | Regression,<br>Time Series Forecasting,<br>Image Denoising,<br>Image-to-Image Translation,<br>Distribution Estimation,<br>Dimensionality Reduction,<br>Clustering |
| **Learning Methods** | Supervised Learning,<br>Semi-Supervised Learning,<br>Transfer Learning,<br>Multi-Task Learning,<br>Adversarial Training,<br>Variational Inference,<br>Markov chain Monte Carlo,<br>Active Learning,<br>Gradient Descent,<br>Stochastic Gradient Descent,<br>Variational Inference |
| **Performance Highlights** | yield_stress_inference_error: reduced from >100% to <5% (reported in ref.64) |
| **Application Domains** | Fluid dynamics,<br>Biophysics / Biomedical imaging (4D-flow MRI),<br>Plasma physics / Magnetic confinement fusion,<br>Materials science (mechanical property inference, fracture/crack detection),<br>Quantum chemistry / Electronic structure (FermiNet, ab initio),<br>Molecular dynamics (DeePMD; large-scale MD),<br>Geophysics / Seismic inversion,<br>Climate and Earth system science,<br>Turbulence modelling,<br>Computational mechanics / Subsurface flow,<br>Image-based flow inference (experimental flow visualization) |

---


### [37. Democratising deep learning for microscopy with ZeroCostDL4Mic](https://doi.org/10.1038/s41467-021-22518-0), Nature Communications *(April 15, 2021)*

| Category | Items |
|----------|-------|
| **Datasets** | ISBI 2012 Neuronal Segmentation Dataset,<br>EPFL mitochondrial EM dataset,<br>DCIS.COM cells — SiR-DNA nuclear marker (StarDist dataset),<br>YOLOv2 bright-field MDA-MB-231 cell migration dataset,<br>Noise2Void 2D (U-251 paxillin-GFP) dataset,<br>Noise2Void 3D (A2780 lifeact-RFP on cell-derived matrices) dataset,<br>CARE (2D & 3D) actin datasets (LifeAct-RFP and Phalloidin),<br>Deep-STORM raw SMLM datasets (phalloidin-Alexa647 glial cell; DNA-PAINT tubulin U2OS),<br>label-free prediction (fnet) dataset — HeLa TOM20,<br>pix2pix paired dataset (DCIS.COM lifeact-RFP -> nucleus SiR-DNA),<br>CycleGAN datasets (SDC, SRRF, SIM microtubule images),<br>StarDist training dataset (DCIS.COM nuclei masks) |
| **Models** | U-Net,<br>YOLO,<br>pix2pix,<br>CycleGAN,<br>Generative Adversarial Network |
| **Tasks** | Image Segmentation,<br>Object Detection,<br>Image Denoising,<br>Image Super-Resolution,<br>Image-to-Image Translation |
| **Learning Methods** | Supervised Learning,<br>Self-Supervised Learning,<br>Transfer Learning,<br>Pre-training,<br>Fine-Tuning,<br>Adversarial Training |
| **Performance Highlights** | IoU: 0.90,<br>mAP: 0.60,<br>mSSIM: 0.74,<br>PSNR: 20.4,<br>NRMSE: 0.16,<br>mSSIM: 0.74,<br>PSNR: 24.8,<br>NRMSE: 0.19 |
| **Application Domains** | Bioimaging / Microscopy,<br>Fluorescence live-cell imaging,<br>Structured Illumination Microscopy (SIM),<br>Spinning-Disk Confocal (SDC),<br>Single-Molecule Localization Microscopy (dSTORM, DNA-PAINT),<br>Electron Microscopy (EM),<br>Cell migration and tracking,<br>Digital pathology / medical imaging (general context mentioned) |

---


### [36. Crystallography companion agent for high-throughput materials discovery](https://doi.org/10.1038/s43588-021-00059-2), Nature Computational Science *(April 2021)*

| Category | Items |
|----------|-------|
| **Datasets** | Synthetic XRD dataset (per phase),<br>BaTiO3 temperature-dependent experimental XRD dataset,<br>ADTA experimental XRD dataset (adamantane-1,3,5,7-tetracarboxylic acid),<br>Ni–Co–Al experimental thin-film library (EDX + XRD),<br>ICSD-derived phase collection for Ni–Co–Al (testing set),<br>CSP-derived ADTA low-energy phases (training/test inputs) |
| **Models** | Convolutional Neural Network,<br>Feedforward Neural Network |
| **Tasks** | Classification,<br>Multi-class Classification,<br>Multi-label Classification,<br>Phase Mapping (structured prediction / mapping across composition or temperature),<br>Search / Retrieval (searching predicted phases in CSP database) |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Stochastic Learning |
| **Performance Highlights** | accuracy: 0.952,<br>cosine_similarity: 0.941,<br>F1-score: 0.946,<br>matched_classifications: 56/60 (≈0.933),<br>cosine_similarity: 0.735,<br>accuracy: 0.763,<br>F1-score: 0.788,<br>top-3_includes_correct_phase: >90% of classifications contain the correct phase in the top three probabilities,<br>various: Figure 5 compares cosine proximity, accuracy and F1-score across combinations of XCA dataset vs AutoXRD dataset and ensemble vs single-learner models (numerical values reported in figure and text for Ni–Co–Al and ADTA benchmarks) |
| **Application Domains** | Materials discovery,<br>Crystallography / phase identification from X-ray diffraction (XRD),<br>High-throughput combinatorial materials characterization,<br>Organic polymorph screening,<br>Inorganic alloy phase mapping (Ni–Co–Al),<br>Autonomous experimentation / self-driving laboratories,<br>Potential extension to other 1D characterization modalities (spectroscopy, pair distribution function, XANES, photoelectron spectra, NMR, mass spectra, etc.) |

---


### [35. Materials design by synthetic biology](https://doi.org/10.1038/s41578-020-00265-w), Nature Reviews Materials *(April 2021)*

| Category | Items |
|----------|-------|
| **Datasets** | _None_ |
| **Models** | _None_ |
| **Tasks** | Optimization |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | Materials synthetic biology,<br>Synthetic biology,<br>Materials science and engineering,<br>Biomedicine,<br>Agriculture,<br>Civil and environmental engineering,<br>Architecture,<br>Product design,<br>Sensing / biosensing,<br>Therapeutics (living therapeutics),<br>Electronics (living electronics / bioelectronics),<br>Energy conversion (microbial fuel cells, biophotovoltaics, artificial photosynthesis),<br>Living building materials / construction,<br>Bioremediation,<br>Biomanufacturing / bioproduction,<br>Biofabrication (3D printing of living materials),<br>Stem- cell differentiation interfaces |

---


### [33. Bayesian reaction optimization as a tool for chemical synthesis](https://doi.org/10.1038/s41586-021-03213-y), Nature *(February 2021)*

| Category | Items |
|----------|-------|
| **Datasets** | Suzuki–Miyaura dataset (reaction 1),<br>Buchwald–Hartwig datasets (reactions 2a–2e),<br>Direct arylation / Pd-catalysed C–H functionalization dataset (reaction 3),<br>Mitsunobu reaction dataset (reaction 4),<br>Deoxyfluorination reaction dataset (reaction 5),<br>Quantum mechanical computation outputs for reactions 1–5 (auto-qchem) |
| **Models** | Gaussian Process,<br>Random Forest,<br>Polynomial Model,<br>Linear Model |
| **Tasks** | Optimization,<br>Regression,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Selection,<br>Feature Extraction,<br>Active Learning |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Stochastic Gradient Descent,<br>Maximum Likelihood Estimation,<br>Batch Learning |
| **Performance Highlights** | worst_case_loss_over_reactions_1_and_2a-e: ≤5% yield (with expected improvement and DFT encodings),<br>standard_deviation_in_outcome_Bayesian_optimization: ≤1.9 (as reported vs DOE methods),<br>statistical_superiority_vs_DOE_mean: p < 0.05 (Bayesian optimization vs DOE designs for mean outcome),<br>reaction_3_success_rate: >99% yield 100% of the time within the experimental budget (for Bayesian optimization runs on reaction 3),<br>human_vs_machine_cross_over: optimizer surpassed human average within 3 batches of five experiments; statistically better after 5 batches (p < 0.05),<br>top_yield_found: 99% yield (identified three distinct conditions giving 99% in 4 rounds of 10 experiments),<br>standard_conditions_benchmark: 60% average yield (replicates: 59% and 60%),<br>top_yield_found: 69% yield (identified by Bayesian optimization in ten rounds of 5 experiments),<br>standard_conditions_benchmark: 36% average yield (replicates: 35% and 36%),<br>relative_performance_vs_GP: Inferior in mean loss, outcome variance and worst-case loss (see Extended Data Table 1),<br>DOE_standard_deviation_range: GSD std ≤6.9; D-optimal std ≤3.3; Bayesian optimization std ≤1.9,<br>DOE_worst_case_loss: GSD worst-case loss ≤16; D-optimal worst-case loss ≤15; Bayesian optimization worst-case loss ≤5 |
| **Application Domains** | Synthetic organic chemistry,<br>Medicinal chemistry / pharmaceutical development,<br>Computational chemistry (DFT-based feature generation),<br>High-throughput experimentation (HTE) workflows,<br>Automated and human-in-the-loop laboratory optimization |

---


### [32. On-the-fly closed-loop materials discovery via Bayesian active learning](https://doi.org/10.1038/s41467-020-19597-w), Nature Communications *(November 24, 2020)*

| Category | Items |
|----------|-------|
| **Datasets** | Ge–Sb–Te composition spread (177 samples),<br>Fe–Ga–Pd composition spread (benchmark),<br>Raw ellipsometry spectral data (Ge–Sb–Te spread, crystalline & amorphous states),<br>AFLOW.org density functional theory (DFT) computed ternary energy hull (external database),<br>ICSD - Inorganic Crystal Structure Database (external) |
| **Models** | Markov Random Field,<br>Gaussian Process |
| **Tasks** | Clustering,<br>Regression,<br>Optimization,<br>Experimental Design,<br>Hyperparameter Optimization |
| **Learning Methods** | Active Learning,<br>Semi-Supervised Learning |
| **Performance Highlights** | Fowlkes-Mallows Index (FMI): convergence threshold defined as FMI >= 80%,<br>Iterations to discover optimum (live CAMEO run): 19 iterations (GST467 discovered),<br>Total samples: CAMEO: ~19 iterations to optimum vs full set 177; run time: ~10 h vs ~90 h for full sweep,<br>Iteration lead over GP-UCB (average): approx. 35-iteration lead (CAMEO over GP-UCB) in post-analysis of 100 runs,<br>Ellipsometry prior contribution to lead: 25-iteration lead out of the 35 attributed to ellipsometry prior,<br>Within 1% of optimal in first 20 runs: CAMEO: 31% of runs (over 100) vs GP-UCB: 10% of runs,<br>Not explicitly numeric: GPR implemented via 'fitrgp' for propagating functional property predictions; improved predictive accuracy reported qualitatively |
| **Application Domains** | Materials science,<br>Solid-state materials / phase-change memory (PCM),<br>Photonic switching devices,<br>Autonomous experimentation / robotics for scientific discovery,<br>Synchrotron X-ray diffraction-based characterization,<br>Combinatorial materials discovery |

---


### [29. A mobile robotic chemist](https://doi.org/10.1038/s41586-020-2442-2), Nature *(July 2020)*

| Category | Items |
|----------|-------|
| **Datasets** | Autonomous robotic search dataset (this work),<br>Initial hole-scavenger screening dataset,<br>Historical photocatalysis dataset (cross-validation),<br>In silico virtual search dataset |
| **Models** | Gaussian Process,<br>Regression |
| **Tasks** | Regression,<br>Optimization,<br>Experimental Design,<br>Hyperparameter Optimization |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Batch Learning,<br>Cross-Validation |
| **Performance Highlights** | baseline_HER: 3.36 ± 0.30 µmol h⁻¹,<br>best_HER_found_by_search: 21.05 µmol h⁻¹,<br>improvement_factor: ≈6× (21.05 / 3.36),<br>experiments_run: 688 real experiments (43 batches),<br>days_running: 8 days autonomous,<br>best_HER: 21.05 µmol h⁻¹,<br>virtual_searches: 100 virtual searches,<br>avg_virtual_experiments_to_95pct: ≈160 virtual experiments to reach 95% of global maximum HER |
| **Application Domains** | photocatalysis (hydrogen evolution from water),<br>materials discovery / materials chemistry,<br>autonomous laboratory automation / mobile robotics in chemistry,<br>experimental design and optimization,<br>high-throughput autonomous experimentation |

---


### [28. Deep-Learning-Enabled Fast Optical Identification and Characterization of 2D Materials](https://doi.org/10.1002/adma.202000953), Advanced Materials *(June 09, 2020)*

| Category | Items |
|----------|-------|
| **Datasets** | Optical microscopy (OM) images of 13 2D materials (training/test split derived from 917 original OM images),<br>CVD graphene OM dataset (for transfer learning experiments),<br>Exfoliated Td-WTe2 OM dataset (for transfer learning experiments),<br>Prediction set: OM images of 17 additional (untrained) 2D materials |
| **Models** | Convolutional Neural Network,<br>Encoder-Decoder,<br>VGG,<br>U-Net |
| **Tasks** | Semantic Segmentation,<br>Multi-class Classification,<br>Instance Segmentation,<br>Image Classification,<br>Feature Extraction,<br>Classification |
| **Learning Methods** | Supervised Learning,<br>Stochastic Gradient Descent,<br>Data Parallel / Ensemble Learning,<br>Transfer Learning,<br>Fine-Tuning |
| **Performance Highlights** | Global accuracy (by pixel): 0.9689,<br>Mean accuracy (by class): 0.7978,<br>Mean IoU (by pixel): 0.5878,<br>Training time: 30 h 56 min 23 s (on NVIDIA GeForce GTX 1080 Ti, 11 GB),<br>Frames per second (test) CPU: 2.5 fps (224×224 test images),<br>Frames per second (test) GPU: 22.0 fps (224×224 test images),<br>Mean class prediction accuracy (pixel-level, reported elsewhere): 79.78%,<br>Global accuracy (pre-training with 60 images): 0.67 (67%),<br>Comparison (random initialization): Requires at least 240 images to reach comparable accuracy,<br>Mean class accuracy: 0.91 (91%),<br>Quantitative metrics: Not provided as single scalar accuracy in main text; results presented as histograms of projected values and standard deviations showing clear correlations,<br>Description: Ensemble projected values and standard deviations plotted for training set (13 materials) and prediction set (17 materials); qualitative correlation to true bandgap and crystal structure observed |
| **Application Domains** | 2D materials characterization,<br>Nanomaterials optical imaging,<br>Automated optical microscopy / material searching,<br>Materials property prediction (bandgap, crystal structure) from images,<br>High-throughput experimental screening,<br>Laboratory automation and real-time imaging analysis |

---


### [27. Artificial Chemist: An Autonomous Quantum Dot Synthesis Bot](https://doi.org/10.1002/adma.202001626), Advanced Materials *(June 04, 2020)*

| Category | Items |
|----------|-------|
| **Datasets** | In-house QD synthesis dataset (flow-synthesized perovskite QD reactions),<br>Pre-training dataset from NNE-UCB optimizations,<br>150-sample subset used for model comparison |
| **Models** | Multi-Layer Perceptron,<br>Gaussian Process |
| **Tasks** | Regression,<br>Optimization,<br>Multi-task Learning,<br>Hyperparameter Optimization |
| **Learning Methods** | Supervised Learning,<br>Reinforcement Learning,<br>Ensemble Learning,<br>Boosting,<br>Pre-training,<br>Transfer Learning,<br>Multi-Task Learning,<br>Evolutionary Learning |
| **Performance Highlights** | within_10pct_of_lowest_Z_after_25_experiments: true,<br>pretrained_average_Ep_error_after_25_runs_meV: 1,<br>uninformed_NNE-UCB_average_Ep_error_after_25_runs_meV: 3,<br>pretrained_surpassed_other_methods_for_target_count: 9_of_11,<br>relative_performance_compared_to_NNE: underperformed,<br>within_10pct_of_lowest_Z_after_25_experiments: true |
| **Application Domains** | Colloidal quantum dot synthesis (metal halide perovskite QDs),<br>Autonomous flow chemistry / microfluidics,<br>Materials discovery and optimization (nanomaterials),<br>Optoelectronic device materials (solar cells, LEDs),<br>Continuous manufacturing / on-demand nanoparticle production |

---


### [26. Coevolutionary search for optimal materials in the space of all possible compounds](https://doi.org/10.1038/s41524-020-0322-9), npj Computational Materials *(May 14, 2020)*

| Category | Items |
|----------|-------|
| **Datasets** | Chemical space of unary and binary compounds constructed from 74 elements (all elements excluding noble gases, rare earth elements, and elements heavier than Pu),<br>Sets of candidate crystal structures per composition (structures generated/optimized with USPEX/VASP) |
| **Models** | None of the standard ML architectures from the provided list (search uses custom evolutionary / coevolutionary algorithms and physics-based models) |
| **Tasks** | Optimization,<br>Ranking,<br>Clustering |
| **Learning Methods** | Evolutionary Learning,<br>Stochastic Learning |
| **Performance Highlights** | sampled_systems: 600 systems computed in 20 MendS generations (hardness/stability search),<br>search_space_total_binary_systems: 2775 possible binary systems (from 74 elements),<br>best_detected_hardness_diamond_Hv_GPa: 92.7,<br>lonsdaleite_Hv_GPa: 93.6,<br>SiC_Hv_GPa: 33.3,<br>BP_Hv_GPa: 37.2,<br>example_MoB2_Hv_GPa: 28.5,<br>example_MnB4_Hv_GPa (Pnnm ferromagnetic): 40.7,<br>sampled_systems: 450 binary systems over 15 MendS generations (magnetization search),<br>result_top_material: bcc-Fe identified as having the highest zero-temperature magnetization among all possible compounds |
| **Application Domains** | Computational materials discovery,<br>Theoretical crystallography / crystal structure prediction,<br>Materials design for mechanical properties (hardness, fracture toughness),<br>Magnetic materials discovery (magnetization at zero Kelvin),<br>High-throughput ab initio materials screening |

---


### [25. Self-driving laboratory for accelerated discovery of thin-film materials](https://doi.org/10.1126/sciadv.aaz8867), Science Advances *(May 13, 2020)*

| Category | Items |
|----------|-------|
| **Datasets** | Raw robotic platform data from two optimization runs (Ada thin-film pseudomobility dataset) |
| **Models** | Phoenics (global Bayesian optimization algorithm),<br>Surrogate model (unspecified) |
| **Tasks** | Optimization,<br>Experimental Design,<br>Regression,<br>Image Classification |
| **Learning Methods** | Active Learning,<br>Model-Based Learning,<br>End-to-End Learning |
| **Performance Highlights** | experiments_per_campaign: 35,<br>campaign_duration: under 30 hours (including restocking consumables) per 35-sample campaign,<br>throughput: one sample synthesized and characterized approximately every 20 min,<br>failure_rate: 1 failed sample per 35-sample campaign,<br>converged_parameters_doping_ratio: ~0.4 equivalents,<br>converged_parameters_annealing_time: ~75 s,<br>image_positions_per_sample: 3 overlapping dark-field images per sample (4000 x 3000 pixels) captured at three locations; spectroscopy and conductance taken at 7 positions |
| **Application Domains** | Materials Science,<br>Thin-film organic semiconductors (spiro-OMeTAD),<br>Perovskite solar cells,<br>Optoelectronics,<br>Autonomous experimentation / Self-driving laboratories,<br>Clean energy materials discovery |

---


### [24. Accelerated discovery of CO2 electrocatalysts using active machine learning](https://doi.org/10.1038/s41586-020-2242-8), Nature *(May 2020)*

| Category | Items |
|----------|-------|
| **Datasets** | Materials Project-derived copper-containing intermetallics (enumerated surfaces and adsorption sites),<br>Experimental de-alloyed Cu–Al catalyst samples |
| **Models** | _None_ |
| **Tasks** | Regression,<br>Dimensionality Reduction,<br>Clustering,<br>Ranking,<br>Optimization |
| **Learning Methods** | Active Learning,<br>Supervised Learning,<br>Unsupervised Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | electrocatalysis,<br>computational materials discovery / materials science,<br>CO2 electroreduction (CO2-to-C2H4 conversion),<br>high-throughput DFT-driven catalyst screening,<br>experimental electrochemical catalyst testing / chemical energy conversion |

---


### [23. Improved protein structure prediction using potentials from deep learning](https://doi.org/10.1038/s41586-019-1923-7), Nature *(January 2020)*

| Category | Items |
|----------|-------|
| **Datasets** | Protein Data Bank (PDB),<br>CATH (non-redundant domain set),<br>Uniclust30 (2017-10),<br>PSI-BLAST nr dataset (as of 15 December 2017),<br>CASP13 targets / CASP13 dataset |
| **Models** | Convolutional Neural Network,<br>ResNet,<br>Ensemble (model averaging),<br>Rosetta scoring (Vscore2_smooth and Rosetta relax) |
| **Tasks** | Distribution Estimation,<br>Binary Classification,<br>Multi-class Classification,<br>Regression,<br>Structured Prediction,<br>Feature Extraction,<br>Data Augmentation |
| **Learning Methods** | Supervised Learning,<br>Stochastic Gradient Descent,<br>Gradient Descent,<br>Ensemble Learning,<br>Multi-Task Learning,<br>Representation Learning |
| **Performance Highlights** | distogram–realized-structure correlation (Pearson r): test r = 0.72; CASP13 r = 0.78,<br>DLDDT12 vs lDDT12 correlation: Pearson r = 0.92 (CASP13),<br>effect of downsampling distogram (no distogram): TM score = 0.266 when distance potential removed,<br>contact precision (long-range L,L/2,L/5): AlphaFold contact predictions exceed state-of-the-art (compared to top CASP13 contact methods 498 and 032); exact numbers shown in Fig.1c,<br>CASP13 FM high-accuracy domain counts (TM>=0.7): AlphaFold: 24 out of 43 FM domains; next best: 14 out of 43,<br>Q3 accuracy: 84%,<br>improvement via torsion initialization and torsion potential: Removing torsion potential degrades accuracy slightly (numeric TM drop shown in Fig.4b but small),<br>noisy restarts average TM on test set: 0.641 (noisy restarts) vs 0.636 (sampling from predicted torsions),<br>CASP13 assessor summed z-scores (FM best-of-five): AlphaFold: 52.8; next closest group: 36.6 (group 322),<br>CASP13 combined FM and TBM/FM summed z-score (best-of-five): AlphaFold: 68.3; next closest: 48.2,<br>TM score examples & improvements: AlphaFold often achieves high TM scores; gradient-descent based submissions performed better than fragment-assembly (Extended Data Fig.5b); final Rosetta relax adds +0.007 TM on average,<br>effect of Neff on accuracy (correlation): DLDDT12 vs Neff (normalized by length) Pearson r = 0.634,<br>improved prediction for deeper MSAs: distogram accuracy correlates with Neff (Extended Data Fig.3b) |
| **Application Domains** | Protein structure prediction / structural biology,<br>Protein fold recognition and homology detection,<br>Protein–protein interface prediction / docking candidates,<br>Ligand binding pocket prediction (structure-guided drug discovery),<br>X-ray crystallography molecular replacement (phasing assistance),<br>General computational biology using MSA-based inference |

---


### [21. Data-Driven Materials Science: Status, Challenges, and Perspectives](https://doi.org/10.1002/advs.201900808), Advanced Science *(September 01, 2019)*

| Category | Items |
|----------|-------|
| **Datasets** | AFLOW,<br>Materials Project,<br>Open Quantum Materials Database (OQMD),<br>NOMAD (Repository / CoE),<br>HTEM,<br>Organic Materials Database (OMDB),<br>Materials Data Facility (MDF),<br>Crystallography Open Database (COD),<br>Computational Materials Repository (CMR),<br>Materials Cloud / MARVEL NCCR data,<br>SUNCAT / Catalysis Hub,<br>Citrine Informatics (Citrination),<br>Exabyte.io,<br>SpringerMaterials,<br>QCArchive |
| **Models** | Feedforward Neural Network,<br>Decision Tree,<br>Support Vector Machine,<br>Gaussian Process,<br>Convolutional Neural Network,<br>Autoencoder,<br>Variational Autoencoder,<br>Generative Adversarial Network,<br>Message Passing Neural Network,<br>Graph Neural Network,<br>Gaussian Process |
| **Tasks** | Regression,<br>Dimensionality Reduction,<br>Clustering,<br>Sequence-to-Sequence,<br>Feature Extraction,<br>Clustering,<br>Optimization,<br>Data Generation,<br>Image Classification |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Reinforcement Learning,<br>Evolutionary Learning,<br>Representation Learning,<br>Transfer Learning,<br>Feature Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | Materials science (computational & experimental),<br>Catalysis,<br>Energy materials (photovoltaics, photoelectrochemical water splitting),<br>Organic electronics (OLEDs),<br>Polymers and dielectrics,<br>Alloys and high-entropy alloys,<br>Topological materials / electronic materials,<br>Nanoclusters and surface science,<br>Chemical reaction prediction / cheminformatics |

---


### [20. Unsupervised word embeddings capture latent knowledge from materials science literature](https://doi.org/10.1038/s41586-019-1335-8), Nature *(July 2019)*

| Category | Items |
|----------|-------|
| **Datasets** | Scientific abstracts corpus (collected),<br>DFT thermoelectric power factor dataset (ab initio electronic transport database),<br>Experimental thermoelectric dataset (literature-derived),<br>Elpasolite formation energy dataset,<br>Analogy evaluation sets (created for hyperparameter tuning),<br>English Wikipedia corpus (comparison) |
| **Models** | Feedforward Neural Network,<br>GloVe,<br>BERT,<br>Multi-Layer Perceptron,<br>Generalized Linear Model |
| **Tasks** | Unsupervised Learning,<br>Recommendation,<br>Ranking,<br>Binary Classification,<br>Regression,<br>Dimensionality Reduction,<br>Hyperparameter Optimization |
| **Learning Methods** | Unsupervised Learning,<br>Supervised Learning,<br>Pre-training,<br>Representation Learning,<br>Hyperparameter Optimization |
| **Performance Highlights** | Spearman_rank_correlation_with_experimental_maximum_power_factor: 59%,<br>Spearman_rank_correlation_with_experimental_maximum_zT: 52%,<br>Average_computed_power_factor_top_10_predictions: 40.8 μW K−2 cm−1,<br>Average_computed_power_factor_candidates: 11.5 μW K−2 cm−1,<br>Average_computed_power_factor_known_thermoelectrics: 17.0 μW K−2 cm−1,<br>Top_10_average_factor_multiple_over_candidates: 3.6x,<br>Top_10_average_factor_multiple_over_known: 2.4x,<br>Percentile_positions_of_top_three_predictions_among_knowns: 99.6th, 96.5th, 95.3rd,<br>DFT_dataset_Spearman_correlation_with_experiment: 31%,<br>MAE_formation_energy_elpasolites: 0.056 eV per atom,<br>accuracy_f1_score_cross_validation: 89% (fivefold cross-validation) |
| **Application Domains** | Materials science (inorganic materials),<br>Thermoelectrics,<br>Photovoltaics,<br>Topological insulators,<br>Ferroelectrics,<br>Materials discovery / recommendation,<br>Computational materials science (DFT-based property computations) |

---


### [19. 2DMatPedia, an open computational database of two-dimensional materials from top-down and bottom-up approaches](https://doi.org/10.1038/s41597-019-0097-3), Scientific Data *(June 12, 2019)*

| Category | Items |
|----------|-------|
| **Datasets** | 2DMatPedia,<br>Materials Project (MP) subset (input database),<br>JARVIS DFT (used for validation comparison),<br>MPContribs landing page / contributed MP entries |
| **Models** | _None_ |
| **Tasks** | Regression,<br>Binary Classification,<br>Clustering,<br>Feature Extraction |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | materials discovery,<br>two-dimensional (2D) materials science,<br>optoelectronics (wide-band-gap 2D materials),<br>sensing and catalysis (applications benefiting from high surface-to-volume ratio),<br>spintronics / magnetism (magnetic 2D materials),<br>data-driven materials screening, data mining and machine learning applications |

---


### [18. Structure prediction drives materials discovery](https://doi.org/10.1038/s41578-019-0101-8), Nature Reviews Materials *(May 2019)*

| Category | Items |
|----------|-------|
| **Datasets** | Inorganic Crystal Structure Database (ICSD),<br>Pauling File,<br>Computationally identified layered materials (topology-scaling screening),<br>Ideal nets / topology database (most common ideal nets),<br>High-throughput CSP screening outputs (examples reported) |
| **Models** | Multi-Layer Perceptron,<br>Feedforward Neural Network,<br>Gaussian Process |
| **Tasks** | Regression,<br>Optimization,<br>Clustering,<br>Dimensionality Reduction,<br>Feature Extraction,<br>Data Generation |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Evolutionary Learning,<br>Representation Learning,<br>Multi-Task Learning |
| **Performance Highlights** | speed_up: 2-4 orders of magnitude (compared with DFT),<br>accuracy: claimed to 'deliver the same accuracy as first-principles methods' when trained on sufficient DFT data (qualitative),<br>qualitative: ‘The accuracy of quantum mechanics, without the electrons’ (GAP claim); used to achieve near-DFT accuracy,<br>practical_outcome: Demonstrated acceleration of CSP when combining ML potentials with active-learning loops (reference to Podryabinkin et al.),<br>H3S_Tc_predicted: Tc = 203 K (predicted and experimentally verified),<br>LaH10_experimental_Tc: 250–260 K (experimental reports after CSP prediction),<br>examples_energy_gains: e.g., evolutionary searches found lower-energy structures than data mining for several compounds (differences reported in meV atom−1: 24.7, 5.1, 0.2, 33.3 meV atom−1),<br>organic_semiconductor_mobility: predicted / discovered material with hole mobility 12.3–16.0 cm2 V−1 s−1 (compared with typical <10 cm2 V−1 s−1 for parent molecules) |
| **Application Domains** | Computational materials discovery,<br>Crystallography / crystal structure prediction (CSP),<br>Superhard materials discovery and mechanical property prediction,<br>Superconductivity (high-Tc conventional superconductors, metal hydrides),<br>Organic semiconductors and organic materials (polymorph prediction, charge mobility),<br>2D materials and nanoclusters,<br>Surfaces, interfaces and grain boundaries,<br>Battery materials (anode/cathode phases),<br>Photovoltaic materials (bandgap engineering),<br>Catalysis and electride materials (ammonia synthesis, CO2 splitting) |

---


### [17. Capturing chemical intuition in synthesis of metal-organic frameworks](https://doi.org/10.1038/s41467-019-08483-9), Nature Communications *(February 01, 2019)*

| Category | Items |
|----------|-------|
| **Datasets** | HKUST-1 synthesis experiments (robotic platform; reconstructed failed and partially successful experiments) |
| **Models** | Random Forest,<br>Decision Tree |
| **Tasks** | Regression,<br>Feature Selection,<br>Optimization,<br>Experimental Design,<br>Dimensionality Reduction |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Evolutionary Learning |
| **Performance Highlights** | MAE_cross-validation_percent: <9%,<br>MAE_unseen_data_percent: <14%,<br>variable_importance_example: Temperature has ~3x the impact of reactant ratio (relative importance normalized to 1 for max),<br>best_experimental_BET_m2_per_g: 2045,<br>sampling_efficiency_estimate: 20 intuition-based samples vs ~4-5 thousand samples required without intuition to maintain same sampling accuracy |
| **Application Domains** | Metal-Organic Framework (MOF) synthesis,<br>Materials science,<br>Chemical synthesis / synthetic inorganic chemistry,<br>Crystallography / crystal growth optimization,<br>Adsorption / surface area optimization,<br>High-throughput robotic experimentation |

---


### [16. Active learning for accelerated design of layered materials](https://doi.org/10.1038/s41524-018-0129-0), npj Computational Materials *(December 10, 2018)*

| Category | Items |
|----------|-------|
| **Datasets** | Three-layer TMDC hetero-structures (H3) — 126 unique structures,<br>Four-layer TMDC hetero-structures (partial set used in BO tests),<br>Adsorption energies dataset (reference dataset used for BO validation) |
| **Models** | Gaussian Process |
| **Tasks** | Regression,<br>Optimization,<br>Feature Extraction,<br>Feature Selection |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Maximum Likelihood Estimation |
| **Performance Highlights** | training_split_threshold: training sets with fewer than 60% of structures did not produce reliable predictions; >60% showed no additional improvement,<br>evaluation_runs: 100 independent GPR models (randomly selected training sets) used to collect statistics and average out effects from initial training data selection,<br>band_gap_model_training_fraction_used_for_figure: 60% of structures randomly selected for training in shown example,<br>predicted_vs_ground_truth: Figures demonstrate predicted vs ground truth band gap, dispersion curves, and EFF curves with 95% confidence intervals (no single numeric MSE in main text),<br>BO_runs: 500 independent BO runs (different random initial training seeds),<br>max_band_gap_success_rate: 79% of BO runs correctly found the structure with the maximum band gap (1.7 eV); 15% found second-best (1.5 eV); 5% found third-best (1.3 eV),<br>desired_band_gap_1.1eV_success_rate: For searching band gap closest to 1.1 eV, MoSe2-WSe2-WSe2 (band gap 1.05 eV) was returned in 91% of 500 runs,<br>EFF_top_found_rate: In band gap (EFF) optimization, one of the top four (five) optimal structures is found within 30 BO iterations in over 95% of the 500 runs,<br>adsorption_dataset_result: On the adsorption energies dataset, after evaluating only 20% of the dataset, 82% of 500 independent BO runs successfully identified the pair with minimum adsorption energy |
| **Application Domains** | Materials design and discovery,<br>Two-dimensional materials (transition metal dichalcogenide heterostructures),<br>Optoelectronics (band gap engineering for solar cells; Shockley–Queisser limit relevance),<br>Thermoelectrics (electronic transport component and thermoelectric Electronic Fitness Function),<br>Catalysis / surface science (validation on adsorption energy dataset) |

---


### [15. Molecular Dynamics Simulation for All](https://doi.org/10.1016/j.neuron.2018.08.011), Neuron *(September 19, 2018)*

| Category | Items |
|----------|-------|
| **Datasets** | Experimental structural datasets (X-ray crystallography, cryo-EM, NMR, EPR, FRET; implied Protein Data Bank structures),<br>Web of Science publication set (top 250 journals) for the term 'molecular dynamics',<br>MD simulation trajectories (all-atom and coarse-grained trajectories generated in cited studies and by reviewed work) |
| **Models** | _None_ |
| **Tasks** | Regression,<br>Ranking |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | Molecular dynamics / Computational biophysics,<br>Structural biology,<br>Molecular and cellular physiology,<br>Neuroscience (proteins relevant to neuronal signaling, ion channels, neurotransmitter transporters, GPCRs),<br>Drug discovery and medicinal chemistry (lead optimization, virtual screening, allosteric modulator design, biased ligand design),<br>Protein folding and aggregation studies |

---


### [14. Deep neural networks for accurate predictions of crystal stability](https://doi.org/10.1038/s41467-018-06322-x), Nature Communications *(September 18, 2018)*

| Category | Items |
|----------|-------|
| **Datasets** | Unmixed garnets dataset,<br>Mixed garnets dataset (computed orderings),<br>Unmixed perovskites dataset,<br>Mixed perovskites dataset (A- and B-site mixing),<br>Extended generated candidate set (garnets) |
| **Models** | Feedforward Neural Network,<br>Multi-Layer Perceptron,<br>Deep Neural Network |
| **Tasks** | Regression,<br>Binary Classification |
| **Learning Methods** | Supervised Learning,<br>Backpropagation,<br>Stochastic Gradient Descent |
| **Performance Highlights** | RMSE: 12 meV atom−1,<br>MAE_training: 7 meV atom−1,<br>MAE_validation: 10 meV atom−1,<br>MAE_test: 9 meV atom−1,<br>MAE_training: 22 meV atom−1,<br>MAE_validation: 26 meV atom−1,<br>MAE_test: 26 meV atom−1,<br>MAE_training: ≈11–12 meV atom−1,<br>MAE_validation: ≈11–12 meV atom−1,<br>MAE_test: ≈11–12 meV atom−1,<br>std_predicted_Ef_across_orderings: 2.8 meV atom−1,<br>MAE_training: 21 meV atom−1,<br>MAE_validation: 34 meV atom−1,<br>MAE_test: 30 meV atom−1,<br>MAE_range: 22–39 meV atom−1,<br>Garnet_accuracy_at_Ehull_0_meV/atom: >90% (unmixed; C-mixed DNN also >90% for mixed),<br>Perovskite_accuracy_at_Ehull_0_meV/atom: >80%,<br>Perovskite_accuracy_at_Ehull_30_meV/atom: >70% |
| **Application Domains** | Materials science,<br>Inorganic crystal stability prediction,<br>Garnet materials (C3A2D3O12),<br>Perovskite materials (ABO3),<br>Computational materials discovery / high-throughput screening |

---


### [13. Accelerated discovery of stable lead-free hybrid organic-inorganic perovskites via machine learning](https://doi.org/10.1038/s41467-018-05761-w), Nature Communications *(August 24, 2018)*

| Category | Items |
|----------|-------|
| **Datasets** | 212 selected HOIPs (training/test),<br>346 HOIPs (initial collected dataset),<br>5158 unexplored HOIPs (prediction set),<br>5504 possible HOIPs (combinatorial space) |
| **Models** | Gradient Boosting Tree,<br>Support Vector Machine,<br>Gaussian Process,<br>Decision Tree,<br>Multi-Layer Perceptron |
| **Tasks** | Regression,<br>Feature Selection,<br>Feature Extraction,<br>Hyperparameter Optimization |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Boosting |
| **Performance Highlights** | R2: 0.970,<br>Pearson_r: 0.985,<br>MSE: 0.086,<br>DFT_agreement: ΔEg ≤ 0.1 eV (for six selected HOIPs) |
| **Application Domains** | Materials science,<br>Computational materials design,<br>Hybrid organic-inorganic perovskites (HOIPs),<br>Photovoltaics / solar cell materials screening,<br>High-throughput materials screening (ML + DFT combined workflows) |

---


### [12. Inverse molecular design using machine learning: Generative models for matter engineering](https://doi.org/10.1126/science.aat2663), Science *(July 27, 2018)*

| Category | Items |
|----------|-------|
| **Datasets** | chemical space project,<br>Materials Project,<br>MoleculeNet |
| **Models** | Variational Autoencoder,<br>Autoencoder,<br>Recurrent Neural Network,<br>Long Short-Term Memory,<br>Attention Mechanism,<br>Generative Adversarial Network,<br>Graph Neural Network,<br>Message Passing Neural Network,<br>Gaussian Process,<br>Convolutional Neural Network |
| **Tasks** | Regression,<br>Data Generation,<br>Sequence-to-Sequence,<br>Text Generation,<br>Graph Generation,<br>Optimization,<br>Representation Learning |
| **Learning Methods** | Supervised Learning,<br>Semi-Supervised Learning,<br>Reinforcement Learning,<br>Adversarial Training,<br>Evolutionary Learning,<br>Policy Gradient,<br>Q-Learning,<br>Representation Learning,<br>Backpropagation,<br>Active Learning,<br>Adversarial Training |
| **Performance Highlights** | _None_ |
| **Application Domains** | Drug discovery / pharmaceuticals,<br>Organic photovoltaics,<br>Organic redox flow batteries,<br>Organic light-emitting diodes,<br>Catalysis and reaction discovery,<br>Inorganic materials (dielectric and optical materials, photoanodes, battery electrolytes),<br>Automated materials discovery / closed-loop experimental laboratories,<br>Quantum chemistry / property prediction |

---


### [11. Insightful classification of crystal structures using deep learning](https://doi.org/10.1038/s41467-018-05169-6), Nature Communications *(July 17, 2018)*

| Category | Items |
|----------|-------|
| **Datasets** | AFLOWLIB elemental solid database (pristine subset),<br>Defective dataset (generated from pristine AFLOWLIB subset),<br>AFLOW Library of Crystallographic Prototypes (used to generate transition-path structures),<br>Processed two-dimensional diffraction fingerprint images (DF) derived from structures |
| **Models** | Convolutional Neural Network |
| **Tasks** | Image Classification,<br>Multi-class Classification,<br>Feature Extraction,<br>Representation Learning |
| **Learning Methods** | Supervised Learning,<br>Mini-Batch Learning,<br>Backpropagation,<br>Gradient Descent,<br>Representation Learning,<br>Feature Extraction |
| **Performance Highlights** | accuracy_pristine_train: 100.0%,<br>accuracy_pristine_test: 100.0%,<br>robustness_random_displacement_up_to_sigma_0.06A: 100.0%,<br>robustness_vacancies_up_to_40%: 100.0%,<br>robustness_vacancies_at_60%: >97% (reported),<br>prediction_time_per_image_CPU: ≈70 ms (including reading time) on quad-core Intel i7-3540M,<br>training_time_CPU: ≈80 minutes on quad-core Intel i7-3540M,<br>interpretable_filters: yes (attentive response maps show that learned filters correspond to diffraction peak arrangements / class templates) |
| **Application Domains** | materials science,<br>computational materials science,<br>crystallography / solid-state physics,<br>high-throughput materials discovery,<br>atom probe tomography (local microstructure determination) |

---


### [10. Machine learning for molecular and materials science](https://doi.org/10.1038/s41586-018-0337-2), Nature *(July 2018)*

| Category | Items |
|----------|-------|
| **Datasets** | Inorganic Crystal Structure Database (ICSD),<br>Elpasolite dataset (ABC2D6),<br>20,000+ crystalline and non-crystalline compounds training set,<br>AFLOWLIB,<br>Computational Materials Repository (CMR),<br>GDB (databases of hypothetical small organic molecules),<br>Harvard Clean Energy Project dataset,<br>Materials Project,<br>NOMAD,<br>Open Quantum Materials Database (OQMD),<br>NREL Materials Database,<br>TEDesignLab,<br>ZINC,<br>ChEMBL,<br>ChemSpider,<br>Citrination,<br>Crystallography Open Database,<br>CSD (Cambridge Structural Database),<br>MatNavi,<br>MatWeb,<br>NIST Chemistry WebBook,<br>NIST Materials Data Repository,<br>PubChem |
| **Models** | Naive Bayes,<br>Decision Tree,<br>Random Forest,<br>Support Vector Machine,<br>Multi-Layer Perceptron,<br>Convolutional Neural Network,<br>Generative Adversarial Network,<br>Recurrent Neural Network,<br>Graph Neural Network,<br>Boltzmann Machine,<br>Bayesian Network,<br>Neural Turing Machine,<br>Gaussian Process,<br>Perceptron,<br>Message Passing Neural Network |
| **Tasks** | Supervised Learning,<br>Unsupervised Learning,<br>Semi-Supervised Learning,<br>Classification,<br>Regression,<br>Sequence-to-Sequence,<br>Image Classification,<br>Data Generation,<br>Active Learning,<br>Hyperparameter Optimization,<br>Representation Learning |
| **Learning Methods** | Supervised Learning,<br>Unsupervised Learning,<br>Semi-Supervised Learning,<br>Reinforcement Learning,<br>Active Learning,<br>Meta-Learning,<br>One-Shot Learning,<br>Adversarial Training,<br>Representation Learning,<br>Imitation Learning |
| **Performance Highlights** | new_compounds_verified: 12,<br>accuracy: around 80%,<br>success_rate: 89%,<br>coverage_increase: 6x |
| **Application Domains** | Molecular chemistry,<br>Materials science,<br>Computational chemistry / electronic structure,<br>Drug discovery / medicinal chemistry,<br>Crystallography / crystal engineering,<br>Surface science and microscopy,<br>High-throughput virtual screening,<br>Experimental design and autonomous experimentation,<br>Text mining / literature extraction |

---


### [9. ChemOS: Orchestrating autonomous experimentation](https://doi.org/10.1126/scirobotics.aat5559), Science Robotics *(June 20, 2018)*

| Category | Items |
|----------|-------|
| **Datasets** | 1100 experiments designed by ChemOS (direct-inject HPLC calibration meta-lab),<br>in-house robot color and cocktail spaces dataset,<br>direct-inject sampling dataset for real-time reaction monitoring,<br>autocalibration / tequila sunrise mixing experiment dataset |
| **Models** | Gaussian Process,<br>Multi-Layer Perceptron,<br>Perceptron,<br>Other (PHOENICS) |
| **Tasks** | Experimental Design,<br>Optimization,<br>Online Learning,<br>Experimental Design,<br>Active Learning |
| **Learning Methods** | Online Learning,<br>Active Learning,<br>Reinforcement Learning |
| **Performance Highlights** | _None_ |
| **Application Domains** | Chemistry,<br>Materials Science,<br>Organic Synthesis,<br>Automated / Autonomous Laboratories,<br>Robotics,<br>Analytical Chemistry (HPLC, NMR, reaction monitoring),<br>High-throughput Experimentation,<br>Human-AI Collaborative Experimentation |

---


### [8. Accelerated discovery of metallic glasses through iteration of machine learning and high-throughput experiments](https://doi.org/10.1126/sciadv.aaq1566), Science Advances *(April 13, 2018)*

| Category | Items |
|----------|-------|
| **Datasets** | Landolt-Börnstein (LB) melt-spinning dataset,<br>Landolt-Börnstein (LB) sputtering dataset,<br>HiTp combinatorial sputter co-deposition — Co-V-Zr (this work),<br>HiTp combinatorial sputter co-deposition — Co-Ti-Zr, Co-Fe-Zr, Fe-Ti-Nb (this work),<br>Combinatorial candidate search space (screened computationally) |
| **Models** | Random Forest |
| **Tasks** | Binary Classification,<br>Regression,<br>Ranking,<br>Feature Selection,<br>Hyperparameter Optimization |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning,<br>Stacking,<br>Active Learning |
| **Performance Highlights** | AUC_ROC: 0.88,<br>Grouping-test_accuracy_before_PCT: 75.9%,<br>Grouping-test_accuracy_after_PCT: 76.8%,<br>Log-loss_before_vs_after_PCT_for_Co-V-Zr_map: 3.56 versus 1.75,<br>Log-loss_Co-V-Zr: reduced from 1.75 to 0.28,<br>AUC_ROC_over_sputtered_dataset_generations: improvement from 0.66 (first-gen) to 0.80 (second-gen/third-gen combined reference in Fig.5C),<br>Log-loss_Co-Ti-Zr: reduced from 1.58 to 0.39 (first-gen to second-gen),<br>Log-loss_Co-Fe-Zr: reduced from 1.70 to 0.49 (first-gen to second-gen),<br>Log-loss_Fe-Ti-Nb: reduced from 2.37 to 1.48 (first-gen to second-gen),<br>AUC_ROC_progression_first_to_third_generation: AUC increases from 0.66 (first-gen) to 0.80 (third-gen aggregated comparison),<br>Stacked_model_effect: stacked approach shows most improvement for sputtered-synthesis prediction (qualitative ROC improvement described) |
| **Application Domains** | Materials science,<br>Metallurgy / Metallic glasses,<br>High-throughput experimentation (combinatorial materials synthesis),<br>Materials discovery and design,<br>Computational materials science / data-driven materials screening |

---


### [7. Two-dimensional materials from high-throughput computational exfoliation of experimentally known compounds](https://doi.org/10.1038/s41565-017-0035-5), Nature Nanotechnology *(March 2018)*

| Category | Items |
|----------|-------|
| **Datasets** | Inorganic Crystal Structure Database (ICSD),<br>Crystallographic Open Database (COD),<br>Derived 2D materials database (this work),<br>Materials Project (mentioned) |
| **Models** | _None_ |
| **Tasks** | Data Generation,<br>Feature Extraction,<br>Pattern Recognition,<br>Clustering |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | Two-dimensional materials discovery,<br>Computational materials science,<br>Electronic and optoelectronic materials,<br>Spintronics and data storage (magnetic 2D materials),<br>Topological materials (quantum spin Hall insulators),<br>High-throughput materials screening and database generation |

---


### [6. Accelerated Discovery of Large Electrostrains in BaTiO3-Based Piezoelectrics Using Active Learning](https://doi.org/10.1002/adma.201702884), Advanced Materials *(January 08, 2018)*

| Category | Items |
|----------|-------|
| **Datasets** | Initial training set of synthesized BTO-based compounds,<br>Unexplored composition search space,<br>Newly synthesized compounds (iterative active learning outputs) |
| **Models** | Support Vector Machine,<br>Gradient Boosting Tree,<br>Ensemble Learning |
| **Tasks** | Regression,<br>Feature Selection,<br>Feature Extraction,<br>Experimental Design,<br>Ranking,<br>Optimization |
| **Learning Methods** | Supervised Learning,<br>Active Learning,<br>Ensemble Learning,<br>Boosting |
| **Performance Highlights** | best_model_selection_criterion: least cross-validation error (qualitative),<br>bootstrap_samples_for_uncertainty: 1000,<br>guided_discovery_outcome_bipolar_strain: 0.23% (bipolar electrostrain at 20 kV cm^-1 for discovered compound),<br>guided_discovery_outcome_unipolar_strain: 0.19% (maximum unipolar electrostrain at 20 kV cm^-1 for discovered compound),<br>features_initial: 71,<br>features_after_correlation_pruning: 18,<br>important_features_identified: direction of dependence of C–T (NCT) and T–O (NTO) transition temperatures on dopants (example),<br>bootstrap_samples: 1000,<br>design_strategies_compared: exploitation, exploration, trade-off (efficient global optimization), random,<br>iterations: 5 iterative rounds; 5 compounds predicted and synthesized per strategy (20 compounds total),<br>successful_improvements: 9 of 20 synthesized compounds had larger electrostrains than the best in training set,<br>statistical_significance: Fisher p-value < 0.001,<br>best_discovered_composition_strain: 0.23% bipolar at 20 kV cm^-1 (Ba0.84Ca0.16)(Ti0.90Zr0.07Sn0.03)O3,<br>electrostrictive_coefficient_Q33_for_best: 0.106 m^4 C^-2,<br>design_strategy_best: trade-off between exploration and exploitation (referred to as efficient global optimization) "performs in a superior manner to the others" |
| **Application Domains** | Accelerated materials discovery,<br>Piezoelectric / electrostrictive materials,<br>Experimental materials synthesis and characterization,<br>Computational materials modeling (DFT, Landau theory, phase-field simulations),<br>Optimal experimental design / active learning in materials science |

---


### [4. Design of efficient molecular organic light-emitting diodes by a high-throughput virtual screening and experimental approach](https://doi.org/10.1038/nmat4717), Nature Materials *(October 2016)*

| Category | Items |
|----------|-------|
| **Datasets** | Virtual chemical library (enumerated candidates),<br>TD-DFT screened subset,<br>TD-DFT-derived training/validation data for ML (empirical model),<br>Calibration dataset (experiment vs theory),<br>Experimental device dataset (synthesized leads) |
| **Models** | Multi-Layer Perceptron,<br>Linear Model |
| **Tasks** | Regression,<br>Ranking,<br>Feature Extraction,<br>Hyperparameter Optimization |
| **Learning Methods** | Supervised Learning,<br>Bayesian Optimization,<br>Backpropagation |
| **Performance Highlights** | R^2 (vs TD-DFT labels): 0.94,<br>R^2 (vs TD-DFT labels): 0.80,<br>Top-5% hit fraction: varies with training size (plotted in Fig.3a),<br>ΔEST prediction R^2 (Arrhenius activation energy comparison): 0.84,<br>ΔEST RMSE: 0.08 eV,<br>Mean unsigned error (emission wavelength): 7 nm,<br>Mean unsigned error (ΔEST): 0.1 eV,<br>Mean unsigned error (f): 0.05,<br>Mean unsigned error (kTADF): 0.1 μs^-1 |
| **Application Domains** | Organic electronics,<br>Organic light-emitting diodes (OLEDs),<br>Materials discovery / computational materials design,<br>Cheminformatics / virtual screening,<br>Optoelectronics |

---


### [3. Machine-learning-assisted materials discovery using failed experiments](https://doi.org/10.1038/nature17439), Nature *(May 2016)*

| Category | Items |
|----------|-------|
| **Datasets** | Archived laboratory notebook reactions (dark reactions) - curated dataset,<br>Sampled commercially available diamines (experimental validation set),<br>Candidate diamine pool (eMolecules),<br>Cambridge Structural Database (CSD) check |
| **Models** | Support Vector Machine,<br>Decision Tree,<br>Random Forest,<br>Generalized Linear Model |
| **Tasks** | Classification,<br>Binary Classification,<br>Feature Extraction,<br>Feature Selection |
| **Learning Methods** | Supervised Learning,<br>Ensemble Learning |
| **Performance Highlights** | test-set_accuracy_all_reaction_types: 78%,<br>test-set_accuracy_vanadium-selenite_only: 79%,<br>average_accuracy_over_15_splits: 74%,<br>experimental_success_rate_for_model_recommendations: 89%,<br>human_intuition_experimental_success_rate: 78%,<br>accuracy_with_only_six_selected_features: 70.7% |
| **Application Domains** | Materials discovery,<br>Inorganic–organic hybrid materials synthesis,<br>Hydrothermal/solvothermal synthesis,<br>Crystallization prediction / crystal formation,<br>Cheminformatics-driven descriptor generation,<br>Experimental planning / reaction recommendation |

---


### [2. Accelerated search for materials with targeted properties by adaptive design](https://doi.org/10.1038/ncomms11241), Nature Communications *(April 15, 2016)*

| Category | Items |
|----------|-------|
| **Datasets** | Initial training set of 22 alloys,<br>Search space of potential alloys,<br>Synthesized predicted alloys (design loop outputs),<br>Feature descriptors (per-alloy feature vectors) |
| **Models** | Gaussian Process,<br>Support Vector Machine |
| **Tasks** | Regression,<br>Optimization,<br>Experimental Design,<br>Feature Selection |
| **Learning Methods** | Supervised Learning,<br>Active Learning |
| **Performance Highlights** | best_discovered_ΔT_K: 1.84,<br>synthesized_candidates: 36,<br>improved_alloys_ΔT<3.15K: 14,<br>training_set_size: 22,<br>search_space_size: 797,504,<br>bootstrap_samples_for_uncertainty: 1000,<br>Mann-Whitney_U: 172,<br>Mann-Whitney_z(sd): 3.6,<br>Mann-Whitney_p: <0.001,<br>probability_random_occurrence: 3.7e-4,<br>relative_performance_samples_4_to_8: nearly identical to SVR rbf:KG,<br>relative_performance_samples_2_and_3: best among combinations for sample sizes 2 and 3 |
| **Application Domains** | Materials discovery,<br>Materials science (shape memory alloys, NiTi-based SMAs),<br>Adaptive experimental design / closed-loop experimentation,<br>Computational materials design / surrogate modeling |

---


### [1. The Open Quantum Materials Database (OQMD): assessing the accuracy of DFT formation energies](https://doi.org/10.1038/npjcompumats.2015.10), npj Computational Materials *(December 11, 2015)*

| Category | Items |
|----------|-------|
| **Datasets** | Open Quantum Materials Database (OQMD),<br>Inorganic Crystal Structure Database (ICSD) (structures used as inputs),<br>SGTE Solid SUBstance (SSUB) database,<br>Thermodynamic database at the Thermal Processing Technology Center (IIT),<br>Combined experimental formation-energy comparisons (deduplicated),<br>Actinide thermodynamics review (actinide oxides),<br>Materials Project (comparison set) |
| **Models** | _None_ |
| **Tasks** | Regression,<br>Binary Classification,<br>Synthetic Data Generation,<br>Optimization |
| **Learning Methods** | _None_ |
| **Performance Highlights** | _None_ |
| **Application Domains** | Computational materials science,<br>Thermochemistry and phase stability analysis,<br>Crystallography / crystal-structure databases,<br>Materials discovery (prediction of new compounds),<br>Battery and energy materials applications (examples/use cases referenced) |

---

